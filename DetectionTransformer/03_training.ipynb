{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58ef11be",
   "metadata": {},
   "source": [
    "# Notebook to perform the ML model training\n",
    "### Included are the changes to be made to the original model as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2d59301-757e-4a58-b2cf-841eee3fa9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seisbench.models as sbm\n",
    "import seisbench.generate as sbg\n",
    "import seisbench.data as sbd\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from seisbench.util import worker_seeding\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seisbench\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6016a16-0d9d-423f-a1b6-298a2ddb55bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Parameters\n",
    "'''\n",
    "sampling_rate = 100 # Sampling rate in Hz\n",
    "classes = 1 # Number of classes (P, S and Detection)\n",
    "in_channels = 1 # Input channels (e.g. HHZ)\n",
    "phases = 'P' # Phase of interest\n",
    "benchmark_dataset = 'stead' # Benchmark dataset to initialise the pretrained weights --> You could also use OBS or STEAD\n",
    "component_order = 'Z' # Order of the components\n",
    "in_samples = 2601 # Input samples for the model (fixed for all samples)\n",
    "\n",
    "base_path_data = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2edc94ee-a3ed-493d-9802-7877eb50a3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-06 13:50:43,776 | seisbench | WARNING | Setting remote root to: https://seisbench.gfz-potsdam.de/mirror/\n",
      "Please note that this can affect your download speed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 NAME OF THE LAYER,          SHAPE OF SOURCE MODEL,          SHAPE OF TARGET MODEL,           MATCH BETWEEN SHAPES\n",
      "                            encoder.convs.0.weight,         torch.Size([8, 3, 11]),         torch.Size([8, 1, 11]),                          False\n",
      "                              encoder.convs.0.bias,                torch.Size([8]),                torch.Size([8]),                           True\n",
      "                            encoder.convs.1.weight,         torch.Size([16, 8, 9]),         torch.Size([16, 8, 9]),                           True\n",
      "                              encoder.convs.1.bias,               torch.Size([16]),               torch.Size([16]),                           True\n",
      "                            encoder.convs.2.weight,        torch.Size([16, 16, 7]),        torch.Size([16, 16, 7]),                           True\n",
      "                              encoder.convs.2.bias,               torch.Size([16]),               torch.Size([16]),                           True\n",
      "                            encoder.convs.3.weight,        torch.Size([32, 16, 7]),        torch.Size([32, 16, 7]),                           True\n",
      "                              encoder.convs.3.bias,               torch.Size([32]),               torch.Size([32]),                           True\n",
      "                            encoder.convs.4.weight,        torch.Size([32, 32, 5]),        torch.Size([32, 32, 5]),                           True\n",
      "                              encoder.convs.4.bias,               torch.Size([32]),               torch.Size([32]),                           True\n",
      "                            encoder.convs.5.weight,        torch.Size([64, 32, 5]),        torch.Size([64, 32, 5]),                           True\n",
      "                              encoder.convs.5.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "                            encoder.convs.6.weight,        torch.Size([64, 64, 3]),        torch.Size([64, 64, 3]),                           True\n",
      "                              encoder.convs.6.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "              res_cnn_stack.members.0.norm1.weight,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "                res_cnn_stack.members.0.norm1.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "        res_cnn_stack.members.0.norm1.running_mean,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "         res_cnn_stack.members.0.norm1.running_var,               torch.Size([64]),               torch.Size([64]),                           True\n",
      " res_cnn_stack.members.0.norm1.num_batches_tracked,                 torch.Size([]),                 torch.Size([]),                           True\n",
      "              res_cnn_stack.members.0.conv1.weight,        torch.Size([64, 64, 3]),        torch.Size([64, 64, 3]),                           True\n",
      "                res_cnn_stack.members.0.conv1.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "              res_cnn_stack.members.0.norm2.weight,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "                res_cnn_stack.members.0.norm2.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "        res_cnn_stack.members.0.norm2.running_mean,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "         res_cnn_stack.members.0.norm2.running_var,               torch.Size([64]),               torch.Size([64]),                           True\n",
      " res_cnn_stack.members.0.norm2.num_batches_tracked,                 torch.Size([]),                 torch.Size([]),                           True\n",
      "              res_cnn_stack.members.0.conv2.weight,        torch.Size([64, 64, 3]),        torch.Size([64, 64, 3]),                           True\n",
      "                res_cnn_stack.members.0.conv2.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "              res_cnn_stack.members.1.norm1.weight,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "                res_cnn_stack.members.1.norm1.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "        res_cnn_stack.members.1.norm1.running_mean,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "         res_cnn_stack.members.1.norm1.running_var,               torch.Size([64]),               torch.Size([64]),                           True\n",
      " res_cnn_stack.members.1.norm1.num_batches_tracked,                 torch.Size([]),                 torch.Size([]),                           True\n",
      "              res_cnn_stack.members.1.conv1.weight,        torch.Size([64, 64, 3]),        torch.Size([64, 64, 3]),                           True\n",
      "                res_cnn_stack.members.1.conv1.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "              res_cnn_stack.members.1.norm2.weight,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "                res_cnn_stack.members.1.norm2.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "        res_cnn_stack.members.1.norm2.running_mean,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "         res_cnn_stack.members.1.norm2.running_var,               torch.Size([64]),               torch.Size([64]),                           True\n",
      " res_cnn_stack.members.1.norm2.num_batches_tracked,                 torch.Size([]),                 torch.Size([]),                           True\n",
      "              res_cnn_stack.members.1.conv2.weight,        torch.Size([64, 64, 3]),        torch.Size([64, 64, 3]),                           True\n",
      "                res_cnn_stack.members.1.conv2.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "              res_cnn_stack.members.2.norm1.weight,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "                res_cnn_stack.members.2.norm1.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "        res_cnn_stack.members.2.norm1.running_mean,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "         res_cnn_stack.members.2.norm1.running_var,               torch.Size([64]),               torch.Size([64]),                           True\n",
      " res_cnn_stack.members.2.norm1.num_batches_tracked,                 torch.Size([]),                 torch.Size([]),                           True\n",
      "              res_cnn_stack.members.2.conv1.weight,        torch.Size([64, 64, 3]),        torch.Size([64, 64, 3]),                           True\n",
      "                res_cnn_stack.members.2.conv1.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "              res_cnn_stack.members.2.norm2.weight,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "                res_cnn_stack.members.2.norm2.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "        res_cnn_stack.members.2.norm2.running_mean,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "         res_cnn_stack.members.2.norm2.running_var,               torch.Size([64]),               torch.Size([64]),                           True\n",
      " res_cnn_stack.members.2.norm2.num_batches_tracked,                 torch.Size([]),                 torch.Size([]),                           True\n",
      "              res_cnn_stack.members.2.conv2.weight,        torch.Size([64, 64, 3]),        torch.Size([64, 64, 3]),                           True\n",
      "                res_cnn_stack.members.2.conv2.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "              res_cnn_stack.members.3.norm1.weight,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "                res_cnn_stack.members.3.norm1.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "        res_cnn_stack.members.3.norm1.running_mean,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "         res_cnn_stack.members.3.norm1.running_var,               torch.Size([64]),               torch.Size([64]),                           True\n",
      " res_cnn_stack.members.3.norm1.num_batches_tracked,                 torch.Size([]),                 torch.Size([]),                           True\n",
      "              res_cnn_stack.members.3.conv1.weight,        torch.Size([64, 64, 3]),        torch.Size([64, 64, 3]),                           True\n",
      "                res_cnn_stack.members.3.conv1.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "              res_cnn_stack.members.3.norm2.weight,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "                res_cnn_stack.members.3.norm2.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "        res_cnn_stack.members.3.norm2.running_mean,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "         res_cnn_stack.members.3.norm2.running_var,               torch.Size([64]),               torch.Size([64]),                           True\n",
      " res_cnn_stack.members.3.norm2.num_batches_tracked,                 torch.Size([]),                 torch.Size([]),                           True\n",
      "              res_cnn_stack.members.3.conv2.weight,        torch.Size([64, 64, 3]),        torch.Size([64, 64, 3]),                           True\n",
      "                res_cnn_stack.members.3.conv2.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "              res_cnn_stack.members.4.norm1.weight,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "                res_cnn_stack.members.4.norm1.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "        res_cnn_stack.members.4.norm1.running_mean,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "         res_cnn_stack.members.4.norm1.running_var,               torch.Size([64]),               torch.Size([64]),                           True\n",
      " res_cnn_stack.members.4.norm1.num_batches_tracked,                 torch.Size([]),                 torch.Size([]),                           True\n",
      "              res_cnn_stack.members.4.conv1.weight,        torch.Size([64, 64, 2]),        torch.Size([64, 64, 2]),                           True\n",
      "                res_cnn_stack.members.4.conv1.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "              res_cnn_stack.members.4.norm2.weight,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "                res_cnn_stack.members.4.norm2.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "        res_cnn_stack.members.4.norm2.running_mean,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "         res_cnn_stack.members.4.norm2.running_var,               torch.Size([64]),               torch.Size([64]),                           True\n",
      " res_cnn_stack.members.4.norm2.num_batches_tracked,                 torch.Size([]),                 torch.Size([]),                           True\n",
      "              res_cnn_stack.members.4.conv2.weight,        torch.Size([64, 64, 2]),        torch.Size([64, 64, 2]),                           True\n",
      "                res_cnn_stack.members.4.conv2.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "              res_cnn_stack.members.5.norm1.weight,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "                res_cnn_stack.members.5.norm1.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "        res_cnn_stack.members.5.norm1.running_mean,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "         res_cnn_stack.members.5.norm1.running_var,               torch.Size([64]),               torch.Size([64]),                           True\n",
      " res_cnn_stack.members.5.norm1.num_batches_tracked,                 torch.Size([]),                 torch.Size([]),                           True\n",
      "              res_cnn_stack.members.5.conv1.weight,        torch.Size([64, 64, 3]),        torch.Size([64, 64, 3]),                           True\n",
      "                res_cnn_stack.members.5.conv1.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "              res_cnn_stack.members.5.norm2.weight,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "                res_cnn_stack.members.5.norm2.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "        res_cnn_stack.members.5.norm2.running_mean,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "         res_cnn_stack.members.5.norm2.running_var,               torch.Size([64]),               torch.Size([64]),                           True\n",
      " res_cnn_stack.members.5.norm2.num_batches_tracked,                 torch.Size([]),                 torch.Size([]),                           True\n",
      "              res_cnn_stack.members.5.conv2.weight,        torch.Size([64, 64, 3]),        torch.Size([64, 64, 3]),                           True\n",
      "                res_cnn_stack.members.5.conv2.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "              res_cnn_stack.members.6.norm1.weight,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "                res_cnn_stack.members.6.norm1.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "        res_cnn_stack.members.6.norm1.running_mean,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "         res_cnn_stack.members.6.norm1.running_var,               torch.Size([64]),               torch.Size([64]),                           True\n",
      " res_cnn_stack.members.6.norm1.num_batches_tracked,                 torch.Size([]),                 torch.Size([]),                           True\n",
      "              res_cnn_stack.members.6.conv1.weight,        torch.Size([64, 64, 2]),        torch.Size([64, 64, 2]),                           True\n",
      "                res_cnn_stack.members.6.conv1.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "              res_cnn_stack.members.6.norm2.weight,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "                res_cnn_stack.members.6.norm2.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "        res_cnn_stack.members.6.norm2.running_mean,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "         res_cnn_stack.members.6.norm2.running_var,               torch.Size([64]),               torch.Size([64]),                           True\n",
      " res_cnn_stack.members.6.norm2.num_batches_tracked,                 torch.Size([]),                 torch.Size([]),                           True\n",
      "              res_cnn_stack.members.6.conv2.weight,        torch.Size([64, 64, 2]),        torch.Size([64, 64, 2]),                           True\n",
      "                res_cnn_stack.members.6.conv2.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "         bi_lstm_stack.members.0.lstm.weight_ih_l0,           torch.Size([64, 64]),           torch.Size([64, 64]),                           True\n",
      "         bi_lstm_stack.members.0.lstm.weight_hh_l0,           torch.Size([64, 16]),           torch.Size([64, 16]),                           True\n",
      "           bi_lstm_stack.members.0.lstm.bias_ih_l0,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "           bi_lstm_stack.members.0.lstm.bias_hh_l0,               torch.Size([64]),               torch.Size([64]),                           True\n",
      " bi_lstm_stack.members.0.lstm.weight_ih_l0_reverse,           torch.Size([64, 64]),           torch.Size([64, 64]),                           True\n",
      " bi_lstm_stack.members.0.lstm.weight_hh_l0_reverse,           torch.Size([64, 16]),           torch.Size([64, 16]),                           True\n",
      "   bi_lstm_stack.members.0.lstm.bias_ih_l0_reverse,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "   bi_lstm_stack.members.0.lstm.bias_hh_l0_reverse,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "               bi_lstm_stack.members.0.conv.weight,        torch.Size([16, 32, 1]),        torch.Size([16, 32, 1]),                           True\n",
      "                 bi_lstm_stack.members.0.conv.bias,               torch.Size([16]),               torch.Size([16]),                           True\n",
      "               bi_lstm_stack.members.0.norm.weight,               torch.Size([16]),               torch.Size([16]),                           True\n",
      "                 bi_lstm_stack.members.0.norm.bias,               torch.Size([16]),               torch.Size([16]),                           True\n",
      "         bi_lstm_stack.members.0.norm.running_mean,               torch.Size([16]),               torch.Size([16]),                           True\n",
      "          bi_lstm_stack.members.0.norm.running_var,               torch.Size([16]),               torch.Size([16]),                           True\n",
      "  bi_lstm_stack.members.0.norm.num_batches_tracked,                 torch.Size([]),                 torch.Size([]),                           True\n",
      "         bi_lstm_stack.members.1.lstm.weight_ih_l0,           torch.Size([64, 16]),           torch.Size([64, 16]),                           True\n",
      "         bi_lstm_stack.members.1.lstm.weight_hh_l0,           torch.Size([64, 16]),           torch.Size([64, 16]),                           True\n",
      "           bi_lstm_stack.members.1.lstm.bias_ih_l0,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "           bi_lstm_stack.members.1.lstm.bias_hh_l0,               torch.Size([64]),               torch.Size([64]),                           True\n",
      " bi_lstm_stack.members.1.lstm.weight_ih_l0_reverse,           torch.Size([64, 16]),           torch.Size([64, 16]),                           True\n",
      " bi_lstm_stack.members.1.lstm.weight_hh_l0_reverse,           torch.Size([64, 16]),           torch.Size([64, 16]),                           True\n",
      "   bi_lstm_stack.members.1.lstm.bias_ih_l0_reverse,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "   bi_lstm_stack.members.1.lstm.bias_hh_l0_reverse,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "               bi_lstm_stack.members.1.conv.weight,        torch.Size([16, 32, 1]),        torch.Size([16, 32, 1]),                           True\n",
      "                 bi_lstm_stack.members.1.conv.bias,               torch.Size([16]),               torch.Size([16]),                           True\n",
      "               bi_lstm_stack.members.1.norm.weight,               torch.Size([16]),               torch.Size([16]),                           True\n",
      "                 bi_lstm_stack.members.1.norm.bias,               torch.Size([16]),               torch.Size([16]),                           True\n",
      "         bi_lstm_stack.members.1.norm.running_mean,               torch.Size([16]),               torch.Size([16]),                           True\n",
      "          bi_lstm_stack.members.1.norm.running_var,               torch.Size([16]),               torch.Size([16]),                           True\n",
      "  bi_lstm_stack.members.1.norm.num_batches_tracked,                 torch.Size([]),                 torch.Size([]),                           True\n",
      "         bi_lstm_stack.members.2.lstm.weight_ih_l0,           torch.Size([64, 16]),           torch.Size([64, 16]),                           True\n",
      "         bi_lstm_stack.members.2.lstm.weight_hh_l0,           torch.Size([64, 16]),           torch.Size([64, 16]),                           True\n",
      "           bi_lstm_stack.members.2.lstm.bias_ih_l0,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "           bi_lstm_stack.members.2.lstm.bias_hh_l0,               torch.Size([64]),               torch.Size([64]),                           True\n",
      " bi_lstm_stack.members.2.lstm.weight_ih_l0_reverse,           torch.Size([64, 16]),           torch.Size([64, 16]),                           True\n",
      " bi_lstm_stack.members.2.lstm.weight_hh_l0_reverse,           torch.Size([64, 16]),           torch.Size([64, 16]),                           True\n",
      "   bi_lstm_stack.members.2.lstm.bias_ih_l0_reverse,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "   bi_lstm_stack.members.2.lstm.bias_hh_l0_reverse,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "               bi_lstm_stack.members.2.conv.weight,        torch.Size([16, 32, 1]),        torch.Size([16, 32, 1]),                           True\n",
      "                 bi_lstm_stack.members.2.conv.bias,               torch.Size([16]),               torch.Size([16]),                           True\n",
      "               bi_lstm_stack.members.2.norm.weight,               torch.Size([16]),               torch.Size([16]),                           True\n",
      "                 bi_lstm_stack.members.2.norm.bias,               torch.Size([16]),               torch.Size([16]),                           True\n",
      "         bi_lstm_stack.members.2.norm.running_mean,               torch.Size([16]),               torch.Size([16]),                           True\n",
      "          bi_lstm_stack.members.2.norm.running_var,               torch.Size([16]),               torch.Size([16]),                           True\n",
      "  bi_lstm_stack.members.2.norm.num_batches_tracked,                 torch.Size([]),                 torch.Size([]),                           True\n",
      "                       transformer_d0.attention.Wx,           torch.Size([16, 32]),           torch.Size([16, 32]),                           True\n",
      "                       transformer_d0.attention.Wt,           torch.Size([16, 32]),           torch.Size([16, 32]),                           True\n",
      "                       transformer_d0.attention.bh,               torch.Size([32]),               torch.Size([32]),                           True\n",
      "                       transformer_d0.attention.Wa,            torch.Size([32, 1]),            torch.Size([32, 1]),                           True\n",
      "                       transformer_d0.attention.ba,                torch.Size([1]),                torch.Size([1]),                           True\n",
      "                        transformer_d0.norm1.gamma,            torch.Size([16, 1]),            torch.Size([16, 1]),                           True\n",
      "                         transformer_d0.norm1.beta,            torch.Size([16, 1]),            torch.Size([16, 1]),                           True\n",
      "                     transformer_d0.ff.lin1.weight,          torch.Size([128, 16]),          torch.Size([128, 16]),                           True\n",
      "                       transformer_d0.ff.lin1.bias,              torch.Size([128]),              torch.Size([128]),                           True\n",
      "                     transformer_d0.ff.lin2.weight,          torch.Size([16, 128]),          torch.Size([16, 128]),                           True\n",
      "                       transformer_d0.ff.lin2.bias,               torch.Size([16]),               torch.Size([16]),                           True\n",
      "                        transformer_d0.norm2.gamma,            torch.Size([16, 1]),            torch.Size([16, 1]),                           True\n",
      "                         transformer_d0.norm2.beta,            torch.Size([16, 1]),            torch.Size([16, 1]),                           True\n",
      "                        transformer_d.attention.Wx,           torch.Size([16, 32]),           torch.Size([16, 32]),                           True\n",
      "                        transformer_d.attention.Wt,           torch.Size([16, 32]),           torch.Size([16, 32]),                           True\n",
      "                        transformer_d.attention.bh,               torch.Size([32]),               torch.Size([32]),                           True\n",
      "                        transformer_d.attention.Wa,            torch.Size([32, 1]),            torch.Size([32, 1]),                           True\n",
      "                        transformer_d.attention.ba,                torch.Size([1]),                torch.Size([1]),                           True\n",
      "                         transformer_d.norm1.gamma,            torch.Size([16, 1]),            torch.Size([16, 1]),                           True\n",
      "                          transformer_d.norm1.beta,            torch.Size([16, 1]),            torch.Size([16, 1]),                           True\n",
      "                      transformer_d.ff.lin1.weight,          torch.Size([128, 16]),          torch.Size([128, 16]),                           True\n",
      "                        transformer_d.ff.lin1.bias,              torch.Size([128]),              torch.Size([128]),                           True\n",
      "                      transformer_d.ff.lin2.weight,          torch.Size([16, 128]),          torch.Size([16, 128]),                           True\n",
      "                        transformer_d.ff.lin2.bias,               torch.Size([16]),               torch.Size([16]),                           True\n",
      "                         transformer_d.norm2.gamma,            torch.Size([16, 1]),            torch.Size([16, 1]),                           True\n",
      "                          transformer_d.norm2.beta,            torch.Size([16, 1]),            torch.Size([16, 1]),                           True\n",
      "                          decoder_d.convs.0.weight,        torch.Size([64, 16, 3]),        torch.Size([64, 16, 3]),                           True\n",
      "                            decoder_d.convs.0.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "                          decoder_d.convs.1.weight,        torch.Size([64, 64, 5]),        torch.Size([64, 64, 5]),                           True\n",
      "                            decoder_d.convs.1.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "                          decoder_d.convs.2.weight,        torch.Size([32, 64, 5]),        torch.Size([32, 64, 5]),                           True\n",
      "                            decoder_d.convs.2.bias,               torch.Size([32]),               torch.Size([32]),                           True\n",
      "                          decoder_d.convs.3.weight,        torch.Size([32, 32, 7]),        torch.Size([32, 32, 7]),                           True\n",
      "                            decoder_d.convs.3.bias,               torch.Size([32]),               torch.Size([32]),                           True\n",
      "                          decoder_d.convs.4.weight,        torch.Size([16, 32, 7]),        torch.Size([16, 32, 7]),                           True\n",
      "                            decoder_d.convs.4.bias,               torch.Size([16]),               torch.Size([16]),                           True\n",
      "                          decoder_d.convs.5.weight,        torch.Size([16, 16, 9]),        torch.Size([16, 16, 9]),                           True\n",
      "                            decoder_d.convs.5.bias,               torch.Size([16]),               torch.Size([16]),                           True\n",
      "                          decoder_d.convs.6.weight,        torch.Size([8, 16, 11]),        torch.Size([8, 16, 11]),                           True\n",
      "                            decoder_d.convs.6.bias,                torch.Size([8]),                torch.Size([8]),                           True\n",
      "                                     conv_d.weight,         torch.Size([1, 8, 11]),         torch.Size([1, 8, 11]),                           True\n",
      "                                       conv_d.bias,                torch.Size([1]),                torch.Size([1]),                           True\n",
      "                         pick_lstms.0.weight_ih_l0,           torch.Size([64, 16]),           torch.Size([64, 16]),                           True\n",
      "                         pick_lstms.0.weight_hh_l0,           torch.Size([64, 16]),           torch.Size([64, 16]),                           True\n",
      "                           pick_lstms.0.bias_ih_l0,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "                           pick_lstms.0.bias_hh_l0,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "                         pick_lstms.1.weight_ih_l0,           torch.Size([64, 16]),    NOT PRESENT IN TARGET MODEL,                              -\n",
      "                         pick_lstms.1.weight_hh_l0,           torch.Size([64, 16]),    NOT PRESENT IN TARGET MODEL,                              -\n",
      "                           pick_lstms.1.bias_ih_l0,               torch.Size([64]),    NOT PRESENT IN TARGET MODEL,                              -\n",
      "                           pick_lstms.1.bias_hh_l0,               torch.Size([64]),    NOT PRESENT IN TARGET MODEL,                              -\n",
      "                              pick_attentions.0.Wx,           torch.Size([16, 32]),           torch.Size([16, 32]),                           True\n",
      "                              pick_attentions.0.Wt,           torch.Size([16, 32]),           torch.Size([16, 32]),                           True\n",
      "                              pick_attentions.0.bh,               torch.Size([32]),               torch.Size([32]),                           True\n",
      "                              pick_attentions.0.Wa,            torch.Size([32, 1]),            torch.Size([32, 1]),                           True\n",
      "                              pick_attentions.0.ba,                torch.Size([1]),                torch.Size([1]),                           True\n",
      "                              pick_attentions.1.Wx,           torch.Size([16, 32]),    NOT PRESENT IN TARGET MODEL,                              -\n",
      "                              pick_attentions.1.Wt,           torch.Size([16, 32]),    NOT PRESENT IN TARGET MODEL,                              -\n",
      "                              pick_attentions.1.bh,               torch.Size([32]),    NOT PRESENT IN TARGET MODEL,                              -\n",
      "                              pick_attentions.1.Wa,            torch.Size([32, 1]),    NOT PRESENT IN TARGET MODEL,                              -\n",
      "                              pick_attentions.1.ba,                torch.Size([1]),    NOT PRESENT IN TARGET MODEL,                              -\n",
      "                    pick_decoders.0.convs.0.weight,        torch.Size([64, 16, 3]),        torch.Size([64, 16, 3]),                           True\n",
      "                      pick_decoders.0.convs.0.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "                    pick_decoders.0.convs.1.weight,        torch.Size([64, 64, 5]),        torch.Size([64, 64, 5]),                           True\n",
      "                      pick_decoders.0.convs.1.bias,               torch.Size([64]),               torch.Size([64]),                           True\n",
      "                    pick_decoders.0.convs.2.weight,        torch.Size([32, 64, 5]),        torch.Size([32, 64, 5]),                           True\n",
      "                      pick_decoders.0.convs.2.bias,               torch.Size([32]),               torch.Size([32]),                           True\n",
      "                    pick_decoders.0.convs.3.weight,        torch.Size([32, 32, 7]),        torch.Size([32, 32, 7]),                           True\n",
      "                      pick_decoders.0.convs.3.bias,               torch.Size([32]),               torch.Size([32]),                           True\n",
      "                    pick_decoders.0.convs.4.weight,        torch.Size([16, 32, 7]),        torch.Size([16, 32, 7]),                           True\n",
      "                      pick_decoders.0.convs.4.bias,               torch.Size([16]),               torch.Size([16]),                           True\n",
      "                    pick_decoders.0.convs.5.weight,        torch.Size([16, 16, 9]),        torch.Size([16, 16, 9]),                           True\n",
      "                      pick_decoders.0.convs.5.bias,               torch.Size([16]),               torch.Size([16]),                           True\n",
      "                    pick_decoders.0.convs.6.weight,        torch.Size([8, 16, 11]),        torch.Size([8, 16, 11]),                           True\n",
      "                      pick_decoders.0.convs.6.bias,                torch.Size([8]),                torch.Size([8]),                           True\n",
      "                    pick_decoders.1.convs.0.weight,        torch.Size([64, 16, 3]),    NOT PRESENT IN TARGET MODEL,                              -\n",
      "                      pick_decoders.1.convs.0.bias,               torch.Size([64]),    NOT PRESENT IN TARGET MODEL,                              -\n",
      "                    pick_decoders.1.convs.1.weight,        torch.Size([64, 64, 5]),    NOT PRESENT IN TARGET MODEL,                              -\n",
      "                      pick_decoders.1.convs.1.bias,               torch.Size([64]),    NOT PRESENT IN TARGET MODEL,                              -\n",
      "                    pick_decoders.1.convs.2.weight,        torch.Size([32, 64, 5]),    NOT PRESENT IN TARGET MODEL,                              -\n",
      "                      pick_decoders.1.convs.2.bias,               torch.Size([32]),    NOT PRESENT IN TARGET MODEL,                              -\n",
      "                    pick_decoders.1.convs.3.weight,        torch.Size([32, 32, 7]),    NOT PRESENT IN TARGET MODEL,                              -\n",
      "                      pick_decoders.1.convs.3.bias,               torch.Size([32]),    NOT PRESENT IN TARGET MODEL,                              -\n",
      "                    pick_decoders.1.convs.4.weight,        torch.Size([16, 32, 7]),    NOT PRESENT IN TARGET MODEL,                              -\n",
      "                      pick_decoders.1.convs.4.bias,               torch.Size([16]),    NOT PRESENT IN TARGET MODEL,                              -\n",
      "                    pick_decoders.1.convs.5.weight,        torch.Size([16, 16, 9]),    NOT PRESENT IN TARGET MODEL,                              -\n",
      "                      pick_decoders.1.convs.5.bias,               torch.Size([16]),    NOT PRESENT IN TARGET MODEL,                              -\n",
      "                    pick_decoders.1.convs.6.weight,        torch.Size([8, 16, 11]),    NOT PRESENT IN TARGET MODEL,                              -\n",
      "                      pick_decoders.1.convs.6.bias,                torch.Size([8]),    NOT PRESENT IN TARGET MODEL,                              -\n",
      "                               pick_convs.0.weight,         torch.Size([1, 8, 11]),         torch.Size([1, 8, 11]),                           True\n",
      "                                 pick_convs.0.bias,                torch.Size([1]),                torch.Size([1]),                           True\n",
      "                               pick_convs.1.weight,         torch.Size([1, 8, 11]),    NOT PRESENT IN TARGET MODEL,                              -\n",
      "                                 pick_convs.1.bias,                torch.Size([1]),    NOT PRESENT IN TARGET MODEL,                              -\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/visitor_cp1/miniconda3/lib/python3.12/site-packages/seisbench/models/base.py:489: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_weights = torch.load(f\"{path_pt}\")\n"
     ]
    }
   ],
   "source": [
    "''' Define model\n",
    "'''\n",
    "seisbench.use_backup_repository()\n",
    "delete_target_layers = ['pick_decoders.1.']\n",
    "target = sbm.EQTransformer(sampling_rate=sampling_rate, classes=classes, in_channels=in_channels, phases=phases)\n",
    "source = sbm.EQTransformer.from_pretrained(benchmark_dataset)\n",
    "\n",
    "\n",
    "source_state_dict = source.state_dict()\n",
    "target_state_dict = target.state_dict()\n",
    "format_print = \"%50s, %30s, %30s, %30s\"\n",
    "# Find the layers to remove, these would correspond to the S class.\n",
    "print(format_print % ('NAME OF THE LAYER', 'SHAPE OF SOURCE MODEL', 'SHAPE OF TARGET MODEL', 'MATCH BETWEEN SHAPES'))\n",
    "for key in source_state_dict.keys():\n",
    "    try: \n",
    "        print(format_print % (key, source_state_dict[key].shape, target_state_dict[key].shape, source_state_dict[key].shape ==target_state_dict[key].shape) )\n",
    "    except:\n",
    "        print(format_print % (key, source_state_dict[key].shape, 'NOT PRESENT IN TARGET MODEL', '-') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01b7b5ba-d96c-4da3-b770-969f17948d8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer pick_lstms.1.weight_ih_l0 deleted successfully.\n",
      "Layer pick_lstms.1.weight_hh_l0 deleted successfully.\n",
      "Layer pick_lstms.1.bias_ih_l0 deleted successfully.\n",
      "Layer pick_lstms.1.bias_hh_l0 deleted successfully.\n",
      "Layer pick_attentions.1.Wx deleted successfully.\n",
      "Layer pick_attentions.1.Wt deleted successfully.\n",
      "Layer pick_attentions.1.bh deleted successfully.\n",
      "Layer pick_attentions.1.Wa deleted successfully.\n",
      "Layer pick_attentions.1.ba deleted successfully.\n",
      "Layer pick_decoders.1.convs.0.weight deleted successfully.\n",
      "Layer pick_decoders.1.convs.0.bias deleted successfully.\n",
      "Layer pick_decoders.1.convs.1.weight deleted successfully.\n",
      "Layer pick_decoders.1.convs.1.bias deleted successfully.\n",
      "Layer pick_decoders.1.convs.2.weight deleted successfully.\n",
      "Layer pick_decoders.1.convs.2.bias deleted successfully.\n",
      "Layer pick_decoders.1.convs.3.weight deleted successfully.\n",
      "Layer pick_decoders.1.convs.3.bias deleted successfully.\n",
      "Layer pick_decoders.1.convs.4.weight deleted successfully.\n",
      "Layer pick_decoders.1.convs.4.bias deleted successfully.\n",
      "Layer pick_decoders.1.convs.5.weight deleted successfully.\n",
      "Layer pick_decoders.1.convs.5.bias deleted successfully.\n",
      "Layer pick_decoders.1.convs.6.weight deleted successfully.\n",
      "Layer pick_decoders.1.convs.6.bias deleted successfully.\n",
      "Layer pick_convs.1.weight deleted successfully.\n",
      "Layer pick_convs.1.bias deleted successfully.\n",
      "\n",
      " Layers of target and source models are the same: True\n"
     ]
    }
   ],
   "source": [
    "# The first step would be to get rid of all layers which are not present in the TARGET model\n",
    "keys_to_delete = [key for key in source_state_dict.keys() if key not in target_state_dict]\n",
    "for key in keys_to_delete:\n",
    "    source_state_dict.pop(key)\n",
    "    print(f'Layer {key} deleted successfully.')\n",
    "print(f'\\n Layers of target and source models are the same: {source_state_dict.keys() == target_state_dict.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f9fdede-9fd0-4ac1-b914-00a468938c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the original input layer (3 channels): torch.Size([8, 3, 11])\n",
      "Shape of the reshaped input layer (1 channel): torch.Size([8, 1, 11])\n"
     ]
    }
   ],
   "source": [
    "# The second step would be to change the input convolution layer, one channel instead o three.\n",
    "# What channel to keep, E, N or Z? What their corresponding index is, 0, 1  and 2?\n",
    "newsize = torch.Size([8, 1, 11])\n",
    "reshaped_source_state_dict = source_state_dict.copy()\n",
    "first_channels_to_drop = 2\n",
    "reshaped_source_state_dict['encoder.convs.0.weight'] = reshaped_source_state_dict['encoder.convs.0.weight'][:, first_channels_to_drop:, :] # Drop two of three channels, the first twos which correspond E and N components.\n",
    "print(f\"Shape of the original input layer (3 channels): {source_state_dict['encoder.convs.0.weight'].shape}\")\n",
    "print(f\"Shape of the reshaped input layer (1 channel): {reshaped_source_state_dict['encoder.convs.0.weight'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "104e3c78-e210-475c-942c-386eabab4aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pretrained weights\n",
    "save_path = base_path_data\n",
    "torch.save(reshaped_source_state_dict, 'stead_pretrained_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d43f3e0-30ac-4eeb-ba35-35d2bdff4aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2274539/1340543111.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('stead_pretrained_weights.pt')); # Load pretrained weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0,                                  NAME OF THE LAYER,          SHAPE OF SOURCE MODEL\n",
      "         0,                             encoder.convs.0.weight,         torch.Size([8, 1, 11])\n",
      "         1,                               encoder.convs.0.bias,                torch.Size([8])\n",
      "         2,                             encoder.convs.1.weight,         torch.Size([16, 8, 9])\n",
      "         3,                               encoder.convs.1.bias,               torch.Size([16])\n",
      "         4,                             encoder.convs.2.weight,        torch.Size([16, 16, 7])\n",
      "         5,                               encoder.convs.2.bias,               torch.Size([16])\n",
      "         6,                             encoder.convs.3.weight,        torch.Size([32, 16, 7])\n",
      "         7,                               encoder.convs.3.bias,               torch.Size([32])\n",
      "         8,                             encoder.convs.4.weight,        torch.Size([32, 32, 5])\n",
      "         9,                               encoder.convs.4.bias,               torch.Size([32])\n",
      "        10,                             encoder.convs.5.weight,        torch.Size([64, 32, 5])\n",
      "        11,                               encoder.convs.5.bias,               torch.Size([64])\n",
      "        12,                             encoder.convs.6.weight,        torch.Size([64, 64, 3])\n",
      "        13,                               encoder.convs.6.bias,               torch.Size([64])\n",
      "        14,               res_cnn_stack.members.0.norm1.weight,               torch.Size([64])\n",
      "        15,                 res_cnn_stack.members.0.norm1.bias,               torch.Size([64])\n",
      "        16,         res_cnn_stack.members.0.norm1.running_mean,               torch.Size([64])\n",
      "        17,          res_cnn_stack.members.0.norm1.running_var,               torch.Size([64])\n",
      "        18,  res_cnn_stack.members.0.norm1.num_batches_tracked,                 torch.Size([])\n",
      "        19,               res_cnn_stack.members.0.conv1.weight,        torch.Size([64, 64, 3])\n",
      "        20,                 res_cnn_stack.members.0.conv1.bias,               torch.Size([64])\n",
      "        21,               res_cnn_stack.members.0.norm2.weight,               torch.Size([64])\n",
      "        22,                 res_cnn_stack.members.0.norm2.bias,               torch.Size([64])\n",
      "        23,         res_cnn_stack.members.0.norm2.running_mean,               torch.Size([64])\n",
      "        24,          res_cnn_stack.members.0.norm2.running_var,               torch.Size([64])\n",
      "        25,  res_cnn_stack.members.0.norm2.num_batches_tracked,                 torch.Size([])\n",
      "        26,               res_cnn_stack.members.0.conv2.weight,        torch.Size([64, 64, 3])\n",
      "        27,                 res_cnn_stack.members.0.conv2.bias,               torch.Size([64])\n",
      "        28,               res_cnn_stack.members.1.norm1.weight,               torch.Size([64])\n",
      "        29,                 res_cnn_stack.members.1.norm1.bias,               torch.Size([64])\n",
      "        30,         res_cnn_stack.members.1.norm1.running_mean,               torch.Size([64])\n",
      "        31,          res_cnn_stack.members.1.norm1.running_var,               torch.Size([64])\n",
      "        32,  res_cnn_stack.members.1.norm1.num_batches_tracked,                 torch.Size([])\n",
      "        33,               res_cnn_stack.members.1.conv1.weight,        torch.Size([64, 64, 3])\n",
      "        34,                 res_cnn_stack.members.1.conv1.bias,               torch.Size([64])\n",
      "        35,               res_cnn_stack.members.1.norm2.weight,               torch.Size([64])\n",
      "        36,                 res_cnn_stack.members.1.norm2.bias,               torch.Size([64])\n",
      "        37,         res_cnn_stack.members.1.norm2.running_mean,               torch.Size([64])\n",
      "        38,          res_cnn_stack.members.1.norm2.running_var,               torch.Size([64])\n",
      "        39,  res_cnn_stack.members.1.norm2.num_batches_tracked,                 torch.Size([])\n",
      "        40,               res_cnn_stack.members.1.conv2.weight,        torch.Size([64, 64, 3])\n",
      "        41,                 res_cnn_stack.members.1.conv2.bias,               torch.Size([64])\n",
      "        42,               res_cnn_stack.members.2.norm1.weight,               torch.Size([64])\n",
      "        43,                 res_cnn_stack.members.2.norm1.bias,               torch.Size([64])\n",
      "        44,         res_cnn_stack.members.2.norm1.running_mean,               torch.Size([64])\n",
      "        45,          res_cnn_stack.members.2.norm1.running_var,               torch.Size([64])\n",
      "        46,  res_cnn_stack.members.2.norm1.num_batches_tracked,                 torch.Size([])\n",
      "        47,               res_cnn_stack.members.2.conv1.weight,        torch.Size([64, 64, 3])\n",
      "        48,                 res_cnn_stack.members.2.conv1.bias,               torch.Size([64])\n",
      "        49,               res_cnn_stack.members.2.norm2.weight,               torch.Size([64])\n",
      "        50,                 res_cnn_stack.members.2.norm2.bias,               torch.Size([64])\n",
      "        51,         res_cnn_stack.members.2.norm2.running_mean,               torch.Size([64])\n",
      "        52,          res_cnn_stack.members.2.norm2.running_var,               torch.Size([64])\n",
      "        53,  res_cnn_stack.members.2.norm2.num_batches_tracked,                 torch.Size([])\n",
      "        54,               res_cnn_stack.members.2.conv2.weight,        torch.Size([64, 64, 3])\n",
      "        55,                 res_cnn_stack.members.2.conv2.bias,               torch.Size([64])\n",
      "        56,               res_cnn_stack.members.3.norm1.weight,               torch.Size([64])\n",
      "        57,                 res_cnn_stack.members.3.norm1.bias,               torch.Size([64])\n",
      "        58,         res_cnn_stack.members.3.norm1.running_mean,               torch.Size([64])\n",
      "        59,          res_cnn_stack.members.3.norm1.running_var,               torch.Size([64])\n",
      "        60,  res_cnn_stack.members.3.norm1.num_batches_tracked,                 torch.Size([])\n",
      "        61,               res_cnn_stack.members.3.conv1.weight,        torch.Size([64, 64, 3])\n",
      "        62,                 res_cnn_stack.members.3.conv1.bias,               torch.Size([64])\n",
      "        63,               res_cnn_stack.members.3.norm2.weight,               torch.Size([64])\n",
      "        64,                 res_cnn_stack.members.3.norm2.bias,               torch.Size([64])\n",
      "        65,         res_cnn_stack.members.3.norm2.running_mean,               torch.Size([64])\n",
      "        66,          res_cnn_stack.members.3.norm2.running_var,               torch.Size([64])\n",
      "        67,  res_cnn_stack.members.3.norm2.num_batches_tracked,                 torch.Size([])\n",
      "        68,               res_cnn_stack.members.3.conv2.weight,        torch.Size([64, 64, 3])\n",
      "        69,                 res_cnn_stack.members.3.conv2.bias,               torch.Size([64])\n",
      "        70,               res_cnn_stack.members.4.norm1.weight,               torch.Size([64])\n",
      "        71,                 res_cnn_stack.members.4.norm1.bias,               torch.Size([64])\n",
      "        72,         res_cnn_stack.members.4.norm1.running_mean,               torch.Size([64])\n",
      "        73,          res_cnn_stack.members.4.norm1.running_var,               torch.Size([64])\n",
      "        74,  res_cnn_stack.members.4.norm1.num_batches_tracked,                 torch.Size([])\n",
      "        75,               res_cnn_stack.members.4.conv1.weight,        torch.Size([64, 64, 2])\n",
      "        76,                 res_cnn_stack.members.4.conv1.bias,               torch.Size([64])\n",
      "        77,               res_cnn_stack.members.4.norm2.weight,               torch.Size([64])\n",
      "        78,                 res_cnn_stack.members.4.norm2.bias,               torch.Size([64])\n",
      "        79,         res_cnn_stack.members.4.norm2.running_mean,               torch.Size([64])\n",
      "        80,          res_cnn_stack.members.4.norm2.running_var,               torch.Size([64])\n",
      "        81,  res_cnn_stack.members.4.norm2.num_batches_tracked,                 torch.Size([])\n",
      "        82,               res_cnn_stack.members.4.conv2.weight,        torch.Size([64, 64, 2])\n",
      "        83,                 res_cnn_stack.members.4.conv2.bias,               torch.Size([64])\n",
      "        84,               res_cnn_stack.members.5.norm1.weight,               torch.Size([64])\n",
      "        85,                 res_cnn_stack.members.5.norm1.bias,               torch.Size([64])\n",
      "        86,         res_cnn_stack.members.5.norm1.running_mean,               torch.Size([64])\n",
      "        87,          res_cnn_stack.members.5.norm1.running_var,               torch.Size([64])\n",
      "        88,  res_cnn_stack.members.5.norm1.num_batches_tracked,                 torch.Size([])\n",
      "        89,               res_cnn_stack.members.5.conv1.weight,        torch.Size([64, 64, 3])\n",
      "        90,                 res_cnn_stack.members.5.conv1.bias,               torch.Size([64])\n",
      "        91,               res_cnn_stack.members.5.norm2.weight,               torch.Size([64])\n",
      "        92,                 res_cnn_stack.members.5.norm2.bias,               torch.Size([64])\n",
      "        93,         res_cnn_stack.members.5.norm2.running_mean,               torch.Size([64])\n",
      "        94,          res_cnn_stack.members.5.norm2.running_var,               torch.Size([64])\n",
      "        95,  res_cnn_stack.members.5.norm2.num_batches_tracked,                 torch.Size([])\n",
      "        96,               res_cnn_stack.members.5.conv2.weight,        torch.Size([64, 64, 3])\n",
      "        97,                 res_cnn_stack.members.5.conv2.bias,               torch.Size([64])\n",
      "        98,               res_cnn_stack.members.6.norm1.weight,               torch.Size([64])\n",
      "        99,                 res_cnn_stack.members.6.norm1.bias,               torch.Size([64])\n",
      "       100,         res_cnn_stack.members.6.norm1.running_mean,               torch.Size([64])\n",
      "       101,          res_cnn_stack.members.6.norm1.running_var,               torch.Size([64])\n",
      "       102,  res_cnn_stack.members.6.norm1.num_batches_tracked,                 torch.Size([])\n",
      "       103,               res_cnn_stack.members.6.conv1.weight,        torch.Size([64, 64, 2])\n",
      "       104,                 res_cnn_stack.members.6.conv1.bias,               torch.Size([64])\n",
      "       105,               res_cnn_stack.members.6.norm2.weight,               torch.Size([64])\n",
      "       106,                 res_cnn_stack.members.6.norm2.bias,               torch.Size([64])\n",
      "       107,         res_cnn_stack.members.6.norm2.running_mean,               torch.Size([64])\n",
      "       108,          res_cnn_stack.members.6.norm2.running_var,               torch.Size([64])\n",
      "       109,  res_cnn_stack.members.6.norm2.num_batches_tracked,                 torch.Size([])\n",
      "       110,               res_cnn_stack.members.6.conv2.weight,        torch.Size([64, 64, 2])\n",
      "       111,                 res_cnn_stack.members.6.conv2.bias,               torch.Size([64])\n",
      "       112,          bi_lstm_stack.members.0.lstm.weight_ih_l0,           torch.Size([64, 64])\n",
      "       113,          bi_lstm_stack.members.0.lstm.weight_hh_l0,           torch.Size([64, 16])\n",
      "       114,            bi_lstm_stack.members.0.lstm.bias_ih_l0,               torch.Size([64])\n",
      "       115,            bi_lstm_stack.members.0.lstm.bias_hh_l0,               torch.Size([64])\n",
      "       116,  bi_lstm_stack.members.0.lstm.weight_ih_l0_reverse,           torch.Size([64, 64])\n",
      "       117,  bi_lstm_stack.members.0.lstm.weight_hh_l0_reverse,           torch.Size([64, 16])\n",
      "       118,    bi_lstm_stack.members.0.lstm.bias_ih_l0_reverse,               torch.Size([64])\n",
      "       119,    bi_lstm_stack.members.0.lstm.bias_hh_l0_reverse,               torch.Size([64])\n",
      "       120,                bi_lstm_stack.members.0.conv.weight,        torch.Size([16, 32, 1])\n",
      "       121,                  bi_lstm_stack.members.0.conv.bias,               torch.Size([16])\n",
      "       122,                bi_lstm_stack.members.0.norm.weight,               torch.Size([16])\n",
      "       123,                  bi_lstm_stack.members.0.norm.bias,               torch.Size([16])\n",
      "       124,          bi_lstm_stack.members.0.norm.running_mean,               torch.Size([16])\n",
      "       125,           bi_lstm_stack.members.0.norm.running_var,               torch.Size([16])\n",
      "       126,   bi_lstm_stack.members.0.norm.num_batches_tracked,                 torch.Size([])\n",
      "       127,          bi_lstm_stack.members.1.lstm.weight_ih_l0,           torch.Size([64, 16])\n",
      "       128,          bi_lstm_stack.members.1.lstm.weight_hh_l0,           torch.Size([64, 16])\n",
      "       129,            bi_lstm_stack.members.1.lstm.bias_ih_l0,               torch.Size([64])\n",
      "       130,            bi_lstm_stack.members.1.lstm.bias_hh_l0,               torch.Size([64])\n",
      "       131,  bi_lstm_stack.members.1.lstm.weight_ih_l0_reverse,           torch.Size([64, 16])\n",
      "       132,  bi_lstm_stack.members.1.lstm.weight_hh_l0_reverse,           torch.Size([64, 16])\n",
      "       133,    bi_lstm_stack.members.1.lstm.bias_ih_l0_reverse,               torch.Size([64])\n",
      "       134,    bi_lstm_stack.members.1.lstm.bias_hh_l0_reverse,               torch.Size([64])\n",
      "       135,                bi_lstm_stack.members.1.conv.weight,        torch.Size([16, 32, 1])\n",
      "       136,                  bi_lstm_stack.members.1.conv.bias,               torch.Size([16])\n",
      "       137,                bi_lstm_stack.members.1.norm.weight,               torch.Size([16])\n",
      "       138,                  bi_lstm_stack.members.1.norm.bias,               torch.Size([16])\n",
      "       139,          bi_lstm_stack.members.1.norm.running_mean,               torch.Size([16])\n",
      "       140,           bi_lstm_stack.members.1.norm.running_var,               torch.Size([16])\n",
      "       141,   bi_lstm_stack.members.1.norm.num_batches_tracked,                 torch.Size([])\n",
      "       142,          bi_lstm_stack.members.2.lstm.weight_ih_l0,           torch.Size([64, 16])\n",
      "       143,          bi_lstm_stack.members.2.lstm.weight_hh_l0,           torch.Size([64, 16])\n",
      "       144,            bi_lstm_stack.members.2.lstm.bias_ih_l0,               torch.Size([64])\n",
      "       145,            bi_lstm_stack.members.2.lstm.bias_hh_l0,               torch.Size([64])\n",
      "       146,  bi_lstm_stack.members.2.lstm.weight_ih_l0_reverse,           torch.Size([64, 16])\n",
      "       147,  bi_lstm_stack.members.2.lstm.weight_hh_l0_reverse,           torch.Size([64, 16])\n",
      "       148,    bi_lstm_stack.members.2.lstm.bias_ih_l0_reverse,               torch.Size([64])\n",
      "       149,    bi_lstm_stack.members.2.lstm.bias_hh_l0_reverse,               torch.Size([64])\n",
      "       150,                bi_lstm_stack.members.2.conv.weight,        torch.Size([16, 32, 1])\n",
      "       151,                  bi_lstm_stack.members.2.conv.bias,               torch.Size([16])\n",
      "       152,                bi_lstm_stack.members.2.norm.weight,               torch.Size([16])\n",
      "       153,                  bi_lstm_stack.members.2.norm.bias,               torch.Size([16])\n",
      "       154,          bi_lstm_stack.members.2.norm.running_mean,               torch.Size([16])\n",
      "       155,           bi_lstm_stack.members.2.norm.running_var,               torch.Size([16])\n",
      "       156,   bi_lstm_stack.members.2.norm.num_batches_tracked,                 torch.Size([])\n",
      "       157,                        transformer_d0.attention.Wx,           torch.Size([16, 32])\n",
      "       158,                        transformer_d0.attention.Wt,           torch.Size([16, 32])\n",
      "       159,                        transformer_d0.attention.bh,               torch.Size([32])\n",
      "       160,                        transformer_d0.attention.Wa,            torch.Size([32, 1])\n",
      "       161,                        transformer_d0.attention.ba,                torch.Size([1])\n",
      "       162,                         transformer_d0.norm1.gamma,            torch.Size([16, 1])\n",
      "       163,                          transformer_d0.norm1.beta,            torch.Size([16, 1])\n",
      "       164,                      transformer_d0.ff.lin1.weight,          torch.Size([128, 16])\n",
      "       165,                        transformer_d0.ff.lin1.bias,              torch.Size([128])\n",
      "       166,                      transformer_d0.ff.lin2.weight,          torch.Size([16, 128])\n",
      "       167,                        transformer_d0.ff.lin2.bias,               torch.Size([16])\n",
      "       168,                         transformer_d0.norm2.gamma,            torch.Size([16, 1])\n",
      "       169,                          transformer_d0.norm2.beta,            torch.Size([16, 1])\n",
      "       170,                         transformer_d.attention.Wx,           torch.Size([16, 32])\n",
      "       171,                         transformer_d.attention.Wt,           torch.Size([16, 32])\n",
      "       172,                         transformer_d.attention.bh,               torch.Size([32])\n",
      "       173,                         transformer_d.attention.Wa,            torch.Size([32, 1])\n",
      "       174,                         transformer_d.attention.ba,                torch.Size([1])\n",
      "       175,                          transformer_d.norm1.gamma,            torch.Size([16, 1])\n",
      "       176,                           transformer_d.norm1.beta,            torch.Size([16, 1])\n",
      "       177,                       transformer_d.ff.lin1.weight,          torch.Size([128, 16])\n",
      "       178,                         transformer_d.ff.lin1.bias,              torch.Size([128])\n",
      "       179,                       transformer_d.ff.lin2.weight,          torch.Size([16, 128])\n",
      "       180,                         transformer_d.ff.lin2.bias,               torch.Size([16])\n",
      "       181,                          transformer_d.norm2.gamma,            torch.Size([16, 1])\n",
      "       182,                           transformer_d.norm2.beta,            torch.Size([16, 1])\n",
      "       183,                           decoder_d.convs.0.weight,        torch.Size([64, 16, 3])\n",
      "       184,                             decoder_d.convs.0.bias,               torch.Size([64])\n",
      "       185,                           decoder_d.convs.1.weight,        torch.Size([64, 64, 5])\n",
      "       186,                             decoder_d.convs.1.bias,               torch.Size([64])\n",
      "       187,                           decoder_d.convs.2.weight,        torch.Size([32, 64, 5])\n",
      "       188,                             decoder_d.convs.2.bias,               torch.Size([32])\n",
      "       189,                           decoder_d.convs.3.weight,        torch.Size([32, 32, 7])\n",
      "       190,                             decoder_d.convs.3.bias,               torch.Size([32])\n",
      "       191,                           decoder_d.convs.4.weight,        torch.Size([16, 32, 7])\n",
      "       192,                             decoder_d.convs.4.bias,               torch.Size([16])\n",
      "       193,                           decoder_d.convs.5.weight,        torch.Size([16, 16, 9])\n",
      "       194,                             decoder_d.convs.5.bias,               torch.Size([16])\n",
      "       195,                           decoder_d.convs.6.weight,        torch.Size([8, 16, 11])\n",
      "       196,                             decoder_d.convs.6.bias,                torch.Size([8])\n",
      "       197,                                      conv_d.weight,         torch.Size([1, 8, 11])\n",
      "       198,                                        conv_d.bias,                torch.Size([1])\n",
      "       199,                          pick_lstms.0.weight_ih_l0,           torch.Size([64, 16])\n",
      "       200,                          pick_lstms.0.weight_hh_l0,           torch.Size([64, 16])\n",
      "       201,                            pick_lstms.0.bias_ih_l0,               torch.Size([64])\n",
      "       202,                            pick_lstms.0.bias_hh_l0,               torch.Size([64])\n",
      "       203,                               pick_attentions.0.Wx,           torch.Size([16, 32])\n",
      "       204,                               pick_attentions.0.Wt,           torch.Size([16, 32])\n",
      "       205,                               pick_attentions.0.bh,               torch.Size([32])\n",
      "       206,                               pick_attentions.0.Wa,            torch.Size([32, 1])\n",
      "       207,                               pick_attentions.0.ba,                torch.Size([1])\n",
      "       208,                     pick_decoders.0.convs.0.weight,        torch.Size([64, 16, 3])\n",
      "       209,                       pick_decoders.0.convs.0.bias,               torch.Size([64])\n",
      "       210,                     pick_decoders.0.convs.1.weight,        torch.Size([64, 64, 5])\n",
      "       211,                       pick_decoders.0.convs.1.bias,               torch.Size([64])\n",
      "       212,                     pick_decoders.0.convs.2.weight,        torch.Size([32, 64, 5])\n",
      "       213,                       pick_decoders.0.convs.2.bias,               torch.Size([32])\n",
      "       214,                     pick_decoders.0.convs.3.weight,        torch.Size([32, 32, 7])\n",
      "       215,                       pick_decoders.0.convs.3.bias,               torch.Size([32])\n",
      "       216,                     pick_decoders.0.convs.4.weight,        torch.Size([16, 32, 7])\n",
      "       217,                       pick_decoders.0.convs.4.bias,               torch.Size([16])\n",
      "       218,                     pick_decoders.0.convs.5.weight,        torch.Size([16, 16, 9])\n",
      "       219,                       pick_decoders.0.convs.5.bias,               torch.Size([16])\n",
      "       220,                     pick_decoders.0.convs.6.weight,        torch.Size([8, 16, 11])\n",
      "       221,                       pick_decoders.0.convs.6.bias,                torch.Size([8])\n",
      "       222,                                pick_convs.0.weight,         torch.Size([1, 8, 11])\n",
      "       223,                                  pick_convs.0.bias,                torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Load the model with pretrained weights and apply transfer learning using whale calls in the last layer\n",
    "# You can chose to set in_Samples to 2201 or to 4601. The first option would use only the existing data but the second one will complete the short waveforms by padding with 0s.\n",
    "# I think padding with 0s in a signficant part of the waveform may not be realistic and introduce bias to the training data, so I choose to work with the samllest length of the training data, which is 2201.\n",
    "model = sbm.EQTransformer(in_channels=in_channels, classes=classes, phases=phases, sampling_rate=sampling_rate, in_samples=in_samples) \n",
    "model.load_state_dict(torch.load('stead_pretrained_weights.pt')); # Load pretrained weights\n",
    "format_print = \"%10i, %50s, %30s\"\n",
    "# Define the layer(s) for transfer learning\n",
    "state_dict = model.state_dict()\n",
    "print(format_print % (0, 'NAME OF THE LAYER', 'SHAPE OF SOURCE MODEL'))\n",
    "for i, key in enumerate(state_dict.keys()):\n",
    "    print(format_print % (i, key, state_dict[key].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31b3ddb3-1907-416b-babe-121873d3832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path_data = 'data/augmentation_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13557118-c205-488a-8689-734c8a18a048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(sampling_rate:int, component_order:str, base_path=Path('.')):\n",
    "    ''' Loads the training data written in Seisbench format.\n",
    "\n",
    "    Parameters\n",
    "        base_path (str): Path of the seisbench data (It is recommended to locate it in the same workspace folder).\n",
    "        sampling_rate (int): Sampling rate of the data and the training.\n",
    "        component_order (str): Order of the components.\n",
    "    Returns\n",
    "        Tuple[Dataset, Dataset, Dataset]: A tuple containing Training, development and test datasets that will be used for training of the Seisbench model.\n",
    "    '''\n",
    "    data = sbd.WaveformDataset(base_path, sampling_rate = sampling_rate, component_order=component_order)\n",
    "    train, dev, test = data.train_dev_test()\n",
    "    return train, dev, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c87b05c-2e48-4877-8b9c-ac7758dd6a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[train, dev, test] = load_data(sampling_rate=sampling_rate, component_order=component_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "727269ed-1391-4ae1-a87b-830577984c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756895\n",
      "44511\n",
      "88808\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(dev))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc65311d-9a50-4142-b9ca-f028eef5c535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_augmentations(train, dev, test):\n",
    "    phase_dict = {\n",
    "        'trace_P_1_arrival_sample': \"P\",\n",
    "        'trace_P_2_arrival_sample': \"P\",\n",
    "    }\n",
    "    # Here I add the augmentations recommended to train the EQTransformer model.\n",
    "\n",
    "    train_generator = sbg.GenericGenerator(train)\n",
    "    dev_generator = sbg.GenericGenerator(dev)\n",
    "    test_generator = sbg.GenericGenerator(test)\n",
    "    sample_boundaries=[None, None]\n",
    "    detection_fixed_window=4/(1/sampling_rate) # seconds/delta original is 9\n",
    "    p_phases = [key for key, val in phase_dict.items() if val == 'P']\n",
    "\n",
    "    augmentations = [\n",
    "        # Take a random window from the whole waveform\n",
    "        #sbg.WindowAroundSample(list(phase_dict.keys()), samples_before=model.in_samples*3, windowlen=model.in_samples*4, selection=\"random\", strategy=\"variable\"),\n",
    "        sbg.RandomWindow(windowlen=model.in_samples, strategy=\"pad\"),\n",
    "        # Normalize the data\n",
    "        sbg.Normalize(demean_axis=-1, amp_norm_axis=-1, amp_norm_type=\"peak\"),\n",
    "        # Change the type of the numbers\n",
    "        sbg.ChangeDtype(np.float32),\n",
    "        # Create a probabilistic labeller, it takes the input pick and transform it into a triangle so that pick could be found in a window of time instead of in a single point.\n",
    "        sbg.ProbabilisticLabeller(label_columns=phase_dict, sigma=20, dim=0),   # you can change sigma\n",
    "        # Create a detection labeller having a fixed window time. Since we seek to find detections, this is a very important parameter.\n",
    "        sbg.DetectionLabeller(fixed_window=detection_fixed_window, p_phases=p_phases, key=('X', 'detections'), p_time_before=int(.1 / (1/sampling_rate))),\n",
    "    ]\n",
    "\n",
    "    train_generator.add_augmentations(augmentations)\n",
    "    dev_generator.add_augmentations(augmentations)\n",
    "    test_generator.add_augmentations(augmentations)\n",
    "    \n",
    "    return train_generator, dev_generator, test_generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e75c2c8-f7d9-42dc-b20a-9324b8065c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create generators for the augmentations\n",
    "[train_generator, dev_generator, test_generator] = create_augmentations(train, dev, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3ccdeb4-7508-4a97-95db-5ffc54c8bdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_augmented_data(number_plots):\n",
    "    \n",
    "    for i in range(number_plots):\n",
    "        gen = train_generator\n",
    "        sample = gen[np.random.randint(len(gen))]\n",
    "\n",
    "        fig = plt.figure(figsize=(15, 10))\n",
    "        axs = fig.subplots(2, 1, sharex=True, gridspec_kw={\"hspace\": 0, \"height_ratios\": [3, 1]})\n",
    "        axs[0].plot(sample[\"X\"].T, label='Waveform')\n",
    "        axs[1].plot(sample[\"y\"][0].T, label='P prediction')\n",
    "        axs[1].plot(sample[\"y\"][1].T, label='Ruido')\n",
    "        axs[1].plot(sample[\"detections\"].T, label='Detection')\n",
    "        axs[1].legend()\n",
    "        axs[0].legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "852667cb-1941-4f6c-b5de-da0889c3cac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot an example of augmented data to check the result of augmentations\n",
    "plot_augmented_data(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69d2e3e1-9e00-454d-8f7c-e8e6413f8840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the data to the Pytorch data loader, which will automatically handle parallel processing and batching\n",
    "# Creates two loaders, one for training and another for validation\n",
    "def create_data_loaders(batch_size, num_workers):\n",
    "    train_loader = DataLoader(train_generator, batch_size=batch_size, shuffle=True, num_workers=num_workers, worker_init_fn=worker_seeding, \n",
    "                             persistent_workers=False)\n",
    "    dev_loader = DataLoader(dev_generator, batch_size=batch_size, shuffle=False, num_workers=num_workers, worker_init_fn=worker_seeding,\n",
    "                           persistent_workers=False)\n",
    "    test_loader = DataLoader(test_generator, batch_size=batch_size, shuffle=False, num_workers=num_workers, worker_init_fn=worker_seeding,\n",
    "                             persistent_workers=False)\n",
    "    \n",
    "    return train_loader, dev_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f79c325-b1c0-4b50-b7bf-495a661bd608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores: 160\n"
     ]
    }
   ],
   "source": [
    "num_cores = multiprocessing.cpu_count()\n",
    "print(f\"Number of CPU cores: {num_cores}\")\n",
    "\n",
    "num_workers_ = num_cores; batch_size_ = 2**8\n",
    "[train_loader, dev_loader, test_loader] = create_data_loaders(batch_size = batch_size_, num_workers = num_workers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a9bf630-00cf-4e99-a5ae-68218a74a315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The smaller the learning rate, the more processing time. Find an appropriate learning rate.\n",
    "# Here we load the optimizer and Loss functions.\n",
    "\n",
    "learning_rate = 1e-3   # you can change this\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss = torch.nn.BCELoss()\n",
    "\n",
    "def loss_fn(y_pred, batch, loss_weights, eps=1e-5):\n",
    "\n",
    "    p_true = batch['y'][:,0]\n",
    "    det_true = batch['detections'][:,0]\n",
    "    p_pred = y_pred[:,0]\n",
    "    det_pred = y_pred[:,1]\n",
    "    \n",
    "    # vector binary cross entropy loss\n",
    "    return (loss_weights[0]*loss(input=det_pred.float(), target=det_true.float()) + \n",
    "            loss_weights[1]*loss(input=p_pred.float(), target=p_true.float()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbab1867-4b6e-4ae1-ad81-6d1241519825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions that calculate sthe accuracy of the training\n",
    "# This depends upon the given threshold for the peaks\n",
    "def calc_acc(pred,batch, loss_weights):\n",
    "    \n",
    "    # Simple accuracy measure in % taking into account all samples of all training signals\n",
    "    p_true = batch['y'][:,0] # Torch tensor of size (Nbatch, Nsamples)\n",
    "    det_true = batch['detections'][:,0]\n",
    "    p_pred = pred[:,0]\n",
    "    det_pred = pred[:,1]\n",
    "    \n",
    "    p_max = torch.max(p_pred,1).values.unsqueeze(1)\n",
    "    thr_det = 0.5\n",
    "\n",
    "    p_pred = (p_pred >= thr_det).float()\n",
    "\n",
    "    det_pred = (det_pred > thr_det).float()\n",
    "    \n",
    "    acc_p = (p_pred == p_true).float().sum()\n",
    "    acc_det = (det_pred == det_true).float().sum()\n",
    "    \n",
    "    acc_p = (100 * acc_p / torch.numel(p_pred)).numpy()\n",
    "    acc_det = (100 * acc_det / torch.numel(det_pred)).numpy()\n",
    "    \n",
    "    acc = (loss_weights[0]*acc_det) + (loss_weights[1]*acc_p)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eaabda13-7d2d-4eb1-8b4d-19ba1490f74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop for the training\n",
    "def train_loop(dataloader, loss_weights):\n",
    "    num_batches = len(dataloader)\n",
    "    size = len(dataloader.dataset)\n",
    "    loss_cum = 0\n",
    "    acc_cum = 0\n",
    "    for batch_id, batch in enumerate(dataloader):\n",
    "        \n",
    "        # Compute prediction and loss\n",
    "        pred = model(batch[\"X\"].to(model.device))\n",
    "        # converting the default tuple output in a Tensor\n",
    "        tensor_all = []\n",
    "        stack_ = torch.stack([pred[0], pred[1]])\n",
    "        for j in range(len(stack_[0])):\n",
    "            tensor_current = torch.stack([stack_[0][j], stack_[1][j]])\n",
    "            tensor_all.append(tensor_current)\n",
    "        tensor_all = torch.stack(tensor_all)\n",
    "            \n",
    "        #\n",
    "        \n",
    "        loss = loss_fn(tensor_all, batch, loss_weights)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_id % 5 == 0:\n",
    "            acc = calc_acc(tensor_all,batch, loss_weights)\n",
    "            loss, current = loss.item(), batch_id * batch[\"X\"].shape[0]\n",
    "            print(f\"loss: {loss:>7f}, accuracy: {acc:>.3f}%, batch [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "        loss_cum += loss\n",
    "        acc_cum += acc\n",
    "        \n",
    "    loss_cum /= num_batches\n",
    "    acc_cum /= num_batches\n",
    "        \n",
    "    return loss_cum, acc_cum\n",
    "\n",
    "\n",
    "def val_loop(dataloader, loss_weights):\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "\n",
    "            pred = model(batch[\"X\"].to(model.device))\n",
    "            \n",
    "            # converting the default tuple output in a Tensor\n",
    "            tensor_all = []\n",
    "            stack_ = torch.stack([pred[0], pred[1]])\n",
    "            for j in range(len(stack_[0])):\n",
    "                tensor_current = torch.stack([stack_[0][j], stack_[1][j]])\n",
    "                tensor_all.append(tensor_current)\n",
    "            tensor_all = torch.stack(tensor_all)\n",
    "            \n",
    "\n",
    "            #\n",
    "            test_loss += loss_fn(tensor_all, batch, loss_weights).item()\n",
    "            test_acc += calc_acc(tensor_all,batch, loss_weights)\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    test_acc /= num_batches\n",
    "    \n",
    "    print(f\"Test avg loss: {test_loss:>8f}, test avg accuracy: {test_acc:>.3f}% \\n\")\n",
    "    \n",
    "    return np.round(test_loss,6), test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "faac9fee-0039-4ce1-96ef-bc78af39b054",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_layers_detection = [\n",
    "\n",
    "'encoder.convs.0.weight', # encoder\n",
    "'encoder.convs.0.bias',\n",
    "'encoder.convs.1.weight',\n",
    "'encoder.convs.1.bias',\n",
    "'encoder.convs.2.weight',\n",
    "'encoder.convs.2.bias',\n",
    "'encoder.convs.3.weight',\n",
    "'encoder.convs.3.bias',\n",
    "'encoder.convs.4.weight',\n",
    "'encoder.convs.4.bias',\n",
    "'encoder.convs.5.weight',\n",
    "'encoder.convs.5.bias',\n",
    "'encoder.convs.6.weight',\n",
    "'encoder.convs.6.bias',\n",
    "\n",
    "'res_cnn_stack.members.0.norm1.weight',  # res-cnn - member 0\n",
    "'res_cnn_stack.members.0.norm1.bias',\n",
    "'res_cnn_stack.members.0.conv1.weight',\n",
    "'res_cnn_stack.members.0.conv1.bias',\n",
    "'res_cnn_stack.members.0.norm2.weight',\n",
    "'res_cnn_stack.members.0.norm2.bias',\n",
    "'res_cnn_stack.members.0.conv2.weight',\n",
    "'res_cnn_stack.members.0.conv2.bias',\n",
    "\n",
    "'res_cnn_stack.members.1.norm1.weight', # res-cnn - member 1\n",
    "'res_cnn_stack.members.1.norm1.bias',\n",
    "'res_cnn_stack.members.1.conv1.weight',\n",
    "'res_cnn_stack.members.1.conv1.bias',\n",
    "'res_cnn_stack.members.1.norm2.weight',\n",
    "'res_cnn_stack.members.1.norm2.bias',\n",
    "'res_cnn_stack.members.1.conv2.weight',\n",
    "'res_cnn_stack.members.1.conv2.bias',\n",
    "\n",
    "'res_cnn_stack.members.2.norm1.weight', # res-cnn - member 2\n",
    "'res_cnn_stack.members.2.norm1.bias',\n",
    "'res_cnn_stack.members.2.conv1.weight',\n",
    "'res_cnn_stack.members.2.conv1.bias',\n",
    "'res_cnn_stack.members.2.norm2.weight',\n",
    "'res_cnn_stack.members.2.norm2.bias',\n",
    "'res_cnn_stack.members.2.conv2.weight',\n",
    "'res_cnn_stack.members.2.conv2.bias',\n",
    "\n",
    "'res_cnn_stack.members.3.norm1.weight', # res-cnn -member 3\n",
    "'res_cnn_stack.members.3.norm1.bias',\n",
    "'res_cnn_stack.members.3.conv1.weight',\n",
    "'res_cnn_stack.members.3.conv1.bias',\n",
    "'res_cnn_stack.members.3.norm2.weight',\n",
    "'res_cnn_stack.members.3.norm2.bias',\n",
    "'res_cnn_stack.members.3.conv2.weight',\n",
    "'res_cnn_stack.members.3.conv2.bias',\n",
    "\n",
    "\n",
    "'res_cnn_stack.members.4.norm1.weight', # res-cnn - member 4\n",
    "'res_cnn_stack.members.4.norm1.bias',\n",
    "'res_cnn_stack.members.4.conv1.weight',\n",
    "'res_cnn_stack.members.4.conv1.bias',\n",
    "'res_cnn_stack.members.4.norm2.weight',\n",
    "'res_cnn_stack.members.4.norm2.bias',\n",
    "'res_cnn_stack.members.4.conv2.weight',\n",
    "'res_cnn_stack.members.4.conv2.bias',\n",
    "\n",
    "'res_cnn_stack.members.5.norm1.weight', # res-cnn - member 5\n",
    "'res_cnn_stack.members.5.norm1.bias',\n",
    "'res_cnn_stack.members.5.conv1.weight',\n",
    "'res_cnn_stack.members.5.conv1.bias',\n",
    "'res_cnn_stack.members.5.norm2.weight',\n",
    "'res_cnn_stack.members.5.norm2.bias',\n",
    "'res_cnn_stack.members.5.conv2.weight',\n",
    "'res_cnn_stack.members.5.conv2.bias',\n",
    "\n",
    "'res_cnn_stack.members.6.norm1.weight', # res_cnn - member 6\n",
    "'res_cnn_stack.members.6.norm1.bias',\n",
    "'res_cnn_stack.members.6.conv1.weight',\n",
    "'res_cnn_stack.members.6.conv1.bias',\n",
    "'res_cnn_stack.members.6.norm2.weight',\n",
    "'res_cnn_stack.members.6.norm2.bias',\n",
    "'res_cnn_stack.members.6.conv2.weight',\n",
    "'res_cnn_stack.members.6.conv2.bias',\n",
    "\n",
    "\n",
    "'bi_lstm_stack.members.0.lstm.weight_ih_l0', # bi-lstm - member 0\n",
    "'bi_lstm_stack.members.0.lstm.weight_hh_l0',\n",
    "'bi_lstm_stack.members.0.lstm.bias_ih_l0',\n",
    "'bi_lstm_stack.members.0.lstm.bias_hh_l0',\n",
    "'bi_lstm_stack.members.0.lstm.weight_ih_l0_reverse',\n",
    "'bi_lstm_stack.members.0.lstm.weight_hh_l0_reverse',\n",
    "'bi_lstm_stack.members.0.lstm.bias_ih_l0_reverse',\n",
    "'bi_lstm_stack.members.0.lstm.bias_hh_l0_reverse',\n",
    "'bi_lstm_stack.members.0.conv.weight',\n",
    "'bi_lstm_stack.members.0.conv.bias',\n",
    "'bi_lstm_stack.members.0.norm.weight',\n",
    "'bi_lstm_stack.members.0.norm.bias',\n",
    "\n",
    "'bi_lstm_stack.members.1.lstm.weight_ih_l0', # bi-lstm - member 1 \n",
    "'bi_lstm_stack.members.1.lstm.weight_hh_l0',\n",
    "'bi_lstm_stack.members.1.lstm.bias_ih_l0',\n",
    "'bi_lstm_stack.members.1.lstm.bias_hh_l0',\n",
    "'bi_lstm_stack.members.1.lstm.weight_ih_l0_reverse',\n",
    "'bi_lstm_stack.members.1.lstm.weight_hh_l0_reverse',\n",
    "'bi_lstm_stack.members.1.lstm.bias_ih_l0_reverse',\n",
    "'bi_lstm_stack.members.1.lstm.bias_hh_l0_reverse',\n",
    "'bi_lstm_stack.members.1.conv.weight',\n",
    "'bi_lstm_stack.members.1.conv.bias',\n",
    "'bi_lstm_stack.members.1.norm.weight',\n",
    "'bi_lstm_stack.members.1.norm.bias',\n",
    "\n",
    "'bi_lstm_stack.members.2.lstm.weight_ih_l0', # bi/lstm - member 2\n",
    "'bi_lstm_stack.members.2.lstm.weight_hh_l0',\n",
    "'bi_lstm_stack.members.2.lstm.bias_ih_l0',\n",
    "'bi_lstm_stack.members.2.lstm.bias_hh_l0',\n",
    "'bi_lstm_stack.members.2.lstm.weight_ih_l0_reverse',\n",
    "'bi_lstm_stack.members.2.lstm.weight_hh_l0_reverse',\n",
    "'bi_lstm_stack.members.2.lstm.bias_ih_l0_reverse',\n",
    "'bi_lstm_stack.members.2.lstm.bias_hh_l0_reverse',\n",
    "'bi_lstm_stack.members.2.conv.weight',\n",
    "'bi_lstm_stack.members.2.conv.bias',\n",
    "'bi_lstm_stack.members.2.norm.weight',\n",
    " 'bi_lstm_stack.members.2.norm.bias',\n",
    "  \n",
    "  'transformer_d0.attention.Wx', # Global attention - Detections and Picks\n",
    "'transformer_d0.attention.Wt',\n",
    "'transformer_d0.attention.bh',\n",
    "'transformer_d0.attention.Wa',\n",
    "'transformer_d0.attention.ba',\n",
    "'transformer_d0.norm1.gamma',\n",
    "'transformer_d0.norm1.beta',\n",
    "'transformer_d0.ff.lin1.weight',\n",
    "'transformer_d0.ff.lin1.bias',\n",
    "'transformer_d0.ff.lin2.weight',\n",
    "'transformer_d0.ff.lin2.bias',\n",
    "'transformer_d0.norm2.gamma',\n",
    "'transformer_d0.norm2.beta',\n",
    "\n",
    "\n",
    "  'transformer_d.attention.Wx', # Global attention for the detection.\n",
    " 'transformer_d.attention.Wt',\n",
    " 'transformer_d.attention.bh',\n",
    " 'transformer_d.attention.Wa',\n",
    " 'transformer_d.attention.ba',\n",
    " 'transformer_d.norm1.gamma',\n",
    " 'transformer_d.norm1.beta',\n",
    " 'transformer_d.ff.lin1.weight',\n",
    " 'transformer_d.ff.lin1.bias',\n",
    " 'transformer_d.ff.lin2.weight',\n",
    " 'transformer_d.ff.lin2.bias',\n",
    " 'transformer_d.norm2.gamma',\n",
    " 'transformer_d.norm2.beta',\n",
    "\n",
    "  '', # There is no local attention for detection\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "\n",
    "'', # there is no LSTM block for detection\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "\n",
    "\n",
    "'decoder_d.convs.0.weight', # Decoders - Last 8 layers (0r 16)\n",
    "'decoder_d.convs.0.bias',\n",
    "'decoder_d.convs.1.weight',\n",
    "'decoder_d.convs.1.bias',\n",
    "'decoder_d.convs.2.weight',\n",
    "'decoder_d.convs.2.bias',\n",
    "'decoder_d.convs.3.weight',\n",
    "'decoder_d.convs.3.bias',\n",
    "'decoder_d.convs.4.weight',\n",
    "'decoder_d.convs.4.bias',\n",
    "'decoder_d.convs.5.weight',\n",
    "'decoder_d.convs.5.bias',\n",
    "'decoder_d.convs.6.weight',\n",
    "'decoder_d.convs.6.bias',\n",
    "'conv_d.weight',\n",
    "'conv_d.bias',\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "trainable_layers_P_pick = [\n",
    " \n",
    "     '', # encoder\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    " \n",
    "     '', # res_cnn - member 0\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    " \n",
    "    '', # res_cnn - member 1\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    " \n",
    "   '', # res_cnn - member 2\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "\n",
    "  '', # res_cnn - member 3\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "\n",
    "  '', # res_cnn - member 4\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "\n",
    "  '', # res_cnn - member 5\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "\n",
    "  '', # res_cnn - member 6\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  \n",
    "\n",
    "  '', # bi-lstm - member 0\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  \n",
    "  '', # bi-lstm - member 1\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  \n",
    "  '', # bi-lstm - member 2\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "\n",
    "\n",
    "  '', # Global attention  - Detection and picks\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "\n",
    "  '', # Global attention for the detection.\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "  '',\n",
    "\n",
    "  'pick_lstms.0.weight_ih_l0', # LSTM block - Last 17 layers (4 more for LSTM and 5 more for local attention)\n",
    "  'pick_lstms.0.weight_hh_l0',\n",
    "  'pick_lstms.0.bias_ih_l0',\n",
    "  'pick_lstms.0.bias_hh_l0',\n",
    "  \n",
    "  'pick_attentions.0.Wx', # local attention to P pick - Last 13 layers (5 more for local attention)\n",
    "  'pick_attentions.0.Wt',\n",
    "  'pick_attentions.0.bh',\n",
    "  'pick_attentions.0.Wa',\n",
    "  'pick_attentions.0.ba',\n",
    "\n",
    "\n",
    "  'pick_decoders.0.convs.0.weight', # Decoders - Last 8 layers (0r 16)\n",
    "  'pick_decoders.0.convs.0.bias',\n",
    "  'pick_decoders.0.convs.1.weight',\n",
    "  'pick_decoders.0.convs.1.bias',\n",
    "  'pick_decoders.0.convs.2.weight',\n",
    "  'pick_decoders.0.convs.2.bias',\n",
    "  'pick_decoders.0.convs.3.weight',\n",
    "  'pick_decoders.0.convs.3.bias',\n",
    "  'pick_decoders.0.convs.4.weight',\n",
    "  'pick_decoders.0.convs.4.bias',\n",
    "  'pick_decoders.0.convs.5.weight',\n",
    "  'pick_decoders.0.convs.5.bias',\n",
    "  'pick_decoders.0.convs.6.weight',\n",
    "  'pick_decoders.0.convs.6.bias',\n",
    "  'pick_convs.0.weight',\n",
    "  'pick_convs.0.bias',\n",
    "\n",
    "  \n",
    "\n",
    "  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae543a99-ea2f-4bd4-babc-ff97c8bb54cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'pick_lstms.0.weight_ih_l0', 'pick_lstms.0.weight_hh_l0', 'pick_lstms.0.bias_ih_l0', 'pick_lstms.0.bias_hh_l0', 'pick_attentions.0.Wx', 'pick_attentions.0.Wt', 'pick_attentions.0.bh', 'pick_attentions.0.Wa', 'pick_attentions.0.ba', 'pick_decoders.0.convs.0.weight', 'pick_decoders.0.convs.0.bias', 'pick_decoders.0.convs.1.weight', 'pick_decoders.0.convs.1.bias', 'pick_decoders.0.convs.2.weight', 'pick_decoders.0.convs.2.bias', 'pick_decoders.0.convs.3.weight', 'pick_decoders.0.convs.3.bias', 'pick_decoders.0.convs.4.weight', 'pick_decoders.0.convs.4.bias', 'pick_decoders.0.convs.5.weight', 'pick_decoders.0.convs.5.bias', 'pick_decoders.0.convs.6.weight', 'pick_decoders.0.convs.6.bias', 'pick_convs.0.weight', 'pick_convs.0.bias']\n",
      "['encoder.convs.0.weight', 'encoder.convs.0.bias', 'encoder.convs.1.weight', 'encoder.convs.1.bias', 'encoder.convs.2.weight', 'encoder.convs.2.bias', 'encoder.convs.3.weight', 'encoder.convs.3.bias', 'encoder.convs.4.weight', 'encoder.convs.4.bias', 'encoder.convs.5.weight', 'encoder.convs.5.bias', 'encoder.convs.6.weight', 'encoder.convs.6.bias', 'res_cnn_stack.members.0.norm1.weight', 'res_cnn_stack.members.0.norm1.bias', 'res_cnn_stack.members.0.conv1.weight', 'res_cnn_stack.members.0.conv1.bias', 'res_cnn_stack.members.0.norm2.weight', 'res_cnn_stack.members.0.norm2.bias', 'res_cnn_stack.members.0.conv2.weight', 'res_cnn_stack.members.0.conv2.bias', 'res_cnn_stack.members.1.norm1.weight', 'res_cnn_stack.members.1.norm1.bias', 'res_cnn_stack.members.1.conv1.weight', 'res_cnn_stack.members.1.conv1.bias', 'res_cnn_stack.members.1.norm2.weight', 'res_cnn_stack.members.1.norm2.bias', 'res_cnn_stack.members.1.conv2.weight', 'res_cnn_stack.members.1.conv2.bias', 'res_cnn_stack.members.2.norm1.weight', 'res_cnn_stack.members.2.norm1.bias', 'res_cnn_stack.members.2.conv1.weight', 'res_cnn_stack.members.2.conv1.bias', 'res_cnn_stack.members.2.norm2.weight', 'res_cnn_stack.members.2.norm2.bias', 'res_cnn_stack.members.2.conv2.weight', 'res_cnn_stack.members.2.conv2.bias', 'res_cnn_stack.members.3.norm1.weight', 'res_cnn_stack.members.3.norm1.bias', 'res_cnn_stack.members.3.conv1.weight', 'res_cnn_stack.members.3.conv1.bias', 'res_cnn_stack.members.3.norm2.weight', 'res_cnn_stack.members.3.norm2.bias', 'res_cnn_stack.members.3.conv2.weight', 'res_cnn_stack.members.3.conv2.bias', 'res_cnn_stack.members.4.norm1.weight', 'res_cnn_stack.members.4.norm1.bias', 'res_cnn_stack.members.4.conv1.weight', 'res_cnn_stack.members.4.conv1.bias', 'res_cnn_stack.members.4.norm2.weight', 'res_cnn_stack.members.4.norm2.bias', 'res_cnn_stack.members.4.conv2.weight', 'res_cnn_stack.members.4.conv2.bias', 'res_cnn_stack.members.5.norm1.weight', 'res_cnn_stack.members.5.norm1.bias', 'res_cnn_stack.members.5.conv1.weight', 'res_cnn_stack.members.5.conv1.bias', 'res_cnn_stack.members.5.norm2.weight', 'res_cnn_stack.members.5.norm2.bias', 'res_cnn_stack.members.5.conv2.weight', 'res_cnn_stack.members.5.conv2.bias', 'res_cnn_stack.members.6.norm1.weight', 'res_cnn_stack.members.6.norm1.bias', 'res_cnn_stack.members.6.conv1.weight', 'res_cnn_stack.members.6.conv1.bias', 'res_cnn_stack.members.6.norm2.weight', 'res_cnn_stack.members.6.norm2.bias', 'res_cnn_stack.members.6.conv2.weight', 'res_cnn_stack.members.6.conv2.bias', 'bi_lstm_stack.members.0.lstm.weight_ih_l0', 'bi_lstm_stack.members.0.lstm.weight_hh_l0', 'bi_lstm_stack.members.0.lstm.bias_ih_l0', 'bi_lstm_stack.members.0.lstm.bias_hh_l0', 'bi_lstm_stack.members.0.lstm.weight_ih_l0_reverse', 'bi_lstm_stack.members.0.lstm.weight_hh_l0_reverse', 'bi_lstm_stack.members.0.lstm.bias_ih_l0_reverse', 'bi_lstm_stack.members.0.lstm.bias_hh_l0_reverse', 'bi_lstm_stack.members.0.conv.weight', 'bi_lstm_stack.members.0.conv.bias', 'bi_lstm_stack.members.0.norm.weight', 'bi_lstm_stack.members.0.norm.bias', 'bi_lstm_stack.members.1.lstm.weight_ih_l0', 'bi_lstm_stack.members.1.lstm.weight_hh_l0', 'bi_lstm_stack.members.1.lstm.bias_ih_l0', 'bi_lstm_stack.members.1.lstm.bias_hh_l0', 'bi_lstm_stack.members.1.lstm.weight_ih_l0_reverse', 'bi_lstm_stack.members.1.lstm.weight_hh_l0_reverse', 'bi_lstm_stack.members.1.lstm.bias_ih_l0_reverse', 'bi_lstm_stack.members.1.lstm.bias_hh_l0_reverse', 'bi_lstm_stack.members.1.conv.weight', 'bi_lstm_stack.members.1.conv.bias', 'bi_lstm_stack.members.1.norm.weight', 'bi_lstm_stack.members.1.norm.bias', 'bi_lstm_stack.members.2.lstm.weight_ih_l0', 'bi_lstm_stack.members.2.lstm.weight_hh_l0', 'bi_lstm_stack.members.2.lstm.bias_ih_l0', 'bi_lstm_stack.members.2.lstm.bias_hh_l0', 'bi_lstm_stack.members.2.lstm.weight_ih_l0_reverse', 'bi_lstm_stack.members.2.lstm.weight_hh_l0_reverse', 'bi_lstm_stack.members.2.lstm.bias_ih_l0_reverse', 'bi_lstm_stack.members.2.lstm.bias_hh_l0_reverse', 'bi_lstm_stack.members.2.conv.weight', 'bi_lstm_stack.members.2.conv.bias', 'bi_lstm_stack.members.2.norm.weight', 'bi_lstm_stack.members.2.norm.bias', 'transformer_d0.attention.Wx', 'transformer_d0.attention.Wt', 'transformer_d0.attention.bh', 'transformer_d0.attention.Wa', 'transformer_d0.attention.ba', 'transformer_d0.norm1.gamma', 'transformer_d0.norm1.beta', 'transformer_d0.ff.lin1.weight', 'transformer_d0.ff.lin1.bias', 'transformer_d0.ff.lin2.weight', 'transformer_d0.ff.lin2.bias', 'transformer_d0.norm2.gamma', 'transformer_d0.norm2.beta', 'transformer_d.attention.Wx', 'transformer_d.attention.Wt', 'transformer_d.attention.bh', 'transformer_d.attention.Wa', 'transformer_d.attention.ba', 'transformer_d.norm1.gamma', 'transformer_d.norm1.beta', 'transformer_d.ff.lin1.weight', 'transformer_d.ff.lin1.bias', 'transformer_d.ff.lin2.weight', 'transformer_d.ff.lin2.bias', 'transformer_d.norm2.gamma', 'transformer_d.norm2.beta', '', '', '', '', '', '', '', '', '', 'decoder_d.convs.0.weight', 'decoder_d.convs.0.bias', 'decoder_d.convs.1.weight', 'decoder_d.convs.1.bias', 'decoder_d.convs.2.weight', 'decoder_d.convs.2.bias', 'decoder_d.convs.3.weight', 'decoder_d.convs.3.bias', 'decoder_d.convs.4.weight', 'decoder_d.convs.4.bias', 'decoder_d.convs.5.weight', 'decoder_d.convs.5.bias', 'decoder_d.convs.6.weight', 'decoder_d.convs.6.bias', 'conv_d.weight', 'conv_d.bias']\n",
      "INDEX                                              LAYER       TRAINABLE\n",
      "    0                             encoder.convs.0.weight            True\n",
      "    1                               encoder.convs.0.bias            True\n",
      "    2                             encoder.convs.1.weight            True\n",
      "    3                               encoder.convs.1.bias            True\n",
      "    4                             encoder.convs.2.weight            True\n",
      "    5                               encoder.convs.2.bias            True\n",
      "    6                             encoder.convs.3.weight            True\n",
      "    7                               encoder.convs.3.bias            True\n",
      "    8                             encoder.convs.4.weight            True\n",
      "    9                               encoder.convs.4.bias            True\n",
      "   10                             encoder.convs.5.weight            True\n",
      "   11                               encoder.convs.5.bias            True\n",
      "   12                             encoder.convs.6.weight            True\n",
      "   13                               encoder.convs.6.bias            True\n",
      "   14               res_cnn_stack.members.0.norm1.weight            True\n",
      "   15                 res_cnn_stack.members.0.norm1.bias            True\n",
      "   16               res_cnn_stack.members.0.conv1.weight            True\n",
      "   17                 res_cnn_stack.members.0.conv1.bias            True\n",
      "   18               res_cnn_stack.members.0.norm2.weight            True\n",
      "   19                 res_cnn_stack.members.0.norm2.bias            True\n",
      "   20               res_cnn_stack.members.0.conv2.weight            True\n",
      "   21                 res_cnn_stack.members.0.conv2.bias            True\n",
      "   22               res_cnn_stack.members.1.norm1.weight            True\n",
      "   23                 res_cnn_stack.members.1.norm1.bias            True\n",
      "   24               res_cnn_stack.members.1.conv1.weight            True\n",
      "   25                 res_cnn_stack.members.1.conv1.bias            True\n",
      "   26               res_cnn_stack.members.1.norm2.weight            True\n",
      "   27                 res_cnn_stack.members.1.norm2.bias            True\n",
      "   28               res_cnn_stack.members.1.conv2.weight            True\n",
      "   29                 res_cnn_stack.members.1.conv2.bias            True\n",
      "   30               res_cnn_stack.members.2.norm1.weight            True\n",
      "   31                 res_cnn_stack.members.2.norm1.bias            True\n",
      "   32               res_cnn_stack.members.2.conv1.weight            True\n",
      "   33                 res_cnn_stack.members.2.conv1.bias            True\n",
      "   34               res_cnn_stack.members.2.norm2.weight            True\n",
      "   35                 res_cnn_stack.members.2.norm2.bias            True\n",
      "   36               res_cnn_stack.members.2.conv2.weight            True\n",
      "   37                 res_cnn_stack.members.2.conv2.bias            True\n",
      "   38               res_cnn_stack.members.3.norm1.weight            True\n",
      "   39                 res_cnn_stack.members.3.norm1.bias            True\n",
      "   40               res_cnn_stack.members.3.conv1.weight            True\n",
      "   41                 res_cnn_stack.members.3.conv1.bias            True\n",
      "   42               res_cnn_stack.members.3.norm2.weight            True\n",
      "   43                 res_cnn_stack.members.3.norm2.bias            True\n",
      "   44               res_cnn_stack.members.3.conv2.weight            True\n",
      "   45                 res_cnn_stack.members.3.conv2.bias            True\n",
      "   46               res_cnn_stack.members.4.norm1.weight            True\n",
      "   47                 res_cnn_stack.members.4.norm1.bias            True\n",
      "   48               res_cnn_stack.members.4.conv1.weight            True\n",
      "   49                 res_cnn_stack.members.4.conv1.bias            True\n",
      "   50               res_cnn_stack.members.4.norm2.weight            True\n",
      "   51                 res_cnn_stack.members.4.norm2.bias            True\n",
      "   52               res_cnn_stack.members.4.conv2.weight            True\n",
      "   53                 res_cnn_stack.members.4.conv2.bias            True\n",
      "   54               res_cnn_stack.members.5.norm1.weight            True\n",
      "   55                 res_cnn_stack.members.5.norm1.bias            True\n",
      "   56               res_cnn_stack.members.5.conv1.weight            True\n",
      "   57                 res_cnn_stack.members.5.conv1.bias            True\n",
      "   58               res_cnn_stack.members.5.norm2.weight            True\n",
      "   59                 res_cnn_stack.members.5.norm2.bias            True\n",
      "   60               res_cnn_stack.members.5.conv2.weight            True\n",
      "   61                 res_cnn_stack.members.5.conv2.bias            True\n",
      "   62               res_cnn_stack.members.6.norm1.weight            True\n",
      "   63                 res_cnn_stack.members.6.norm1.bias            True\n",
      "   64               res_cnn_stack.members.6.conv1.weight            True\n",
      "   65                 res_cnn_stack.members.6.conv1.bias            True\n",
      "   66               res_cnn_stack.members.6.norm2.weight            True\n",
      "   67                 res_cnn_stack.members.6.norm2.bias            True\n",
      "   68               res_cnn_stack.members.6.conv2.weight            True\n",
      "   69                 res_cnn_stack.members.6.conv2.bias            True\n",
      "   70          bi_lstm_stack.members.0.lstm.weight_ih_l0            True\n",
      "   71          bi_lstm_stack.members.0.lstm.weight_hh_l0            True\n",
      "   72            bi_lstm_stack.members.0.lstm.bias_ih_l0            True\n",
      "   73            bi_lstm_stack.members.0.lstm.bias_hh_l0            True\n",
      "   74  bi_lstm_stack.members.0.lstm.weight_ih_l0_reverse            True\n",
      "   75  bi_lstm_stack.members.0.lstm.weight_hh_l0_reverse            True\n",
      "   76    bi_lstm_stack.members.0.lstm.bias_ih_l0_reverse            True\n",
      "   77    bi_lstm_stack.members.0.lstm.bias_hh_l0_reverse            True\n",
      "   78                bi_lstm_stack.members.0.conv.weight            True\n",
      "   79                  bi_lstm_stack.members.0.conv.bias            True\n",
      "   80                bi_lstm_stack.members.0.norm.weight            True\n",
      "   81                  bi_lstm_stack.members.0.norm.bias            True\n",
      "   82          bi_lstm_stack.members.1.lstm.weight_ih_l0            True\n",
      "   83          bi_lstm_stack.members.1.lstm.weight_hh_l0            True\n",
      "   84            bi_lstm_stack.members.1.lstm.bias_ih_l0            True\n",
      "   85            bi_lstm_stack.members.1.lstm.bias_hh_l0            True\n",
      "   86  bi_lstm_stack.members.1.lstm.weight_ih_l0_reverse            True\n",
      "   87  bi_lstm_stack.members.1.lstm.weight_hh_l0_reverse            True\n",
      "   88    bi_lstm_stack.members.1.lstm.bias_ih_l0_reverse            True\n",
      "   89    bi_lstm_stack.members.1.lstm.bias_hh_l0_reverse            True\n",
      "   90                bi_lstm_stack.members.1.conv.weight            True\n",
      "   91                  bi_lstm_stack.members.1.conv.bias            True\n",
      "   92                bi_lstm_stack.members.1.norm.weight            True\n",
      "   93                  bi_lstm_stack.members.1.norm.bias            True\n",
      "   94          bi_lstm_stack.members.2.lstm.weight_ih_l0            True\n",
      "   95          bi_lstm_stack.members.2.lstm.weight_hh_l0            True\n",
      "   96            bi_lstm_stack.members.2.lstm.bias_ih_l0            True\n",
      "   97            bi_lstm_stack.members.2.lstm.bias_hh_l0            True\n",
      "   98  bi_lstm_stack.members.2.lstm.weight_ih_l0_reverse            True\n",
      "   99  bi_lstm_stack.members.2.lstm.weight_hh_l0_reverse            True\n",
      "  100    bi_lstm_stack.members.2.lstm.bias_ih_l0_reverse            True\n",
      "  101    bi_lstm_stack.members.2.lstm.bias_hh_l0_reverse            True\n",
      "  102                bi_lstm_stack.members.2.conv.weight            True\n",
      "  103                  bi_lstm_stack.members.2.conv.bias            True\n",
      "  104                bi_lstm_stack.members.2.norm.weight            True\n",
      "  105                  bi_lstm_stack.members.2.norm.bias            True\n",
      "  106                        transformer_d0.attention.Wx            True\n",
      "  107                        transformer_d0.attention.Wt            True\n",
      "  108                        transformer_d0.attention.bh            True\n",
      "  109                        transformer_d0.attention.Wa            True\n",
      "  110                        transformer_d0.attention.ba            True\n",
      "  111                         transformer_d0.norm1.gamma            True\n",
      "  112                          transformer_d0.norm1.beta            True\n",
      "  113                      transformer_d0.ff.lin1.weight            True\n",
      "  114                        transformer_d0.ff.lin1.bias            True\n",
      "  115                      transformer_d0.ff.lin2.weight            True\n",
      "  116                        transformer_d0.ff.lin2.bias            True\n",
      "  117                         transformer_d0.norm2.gamma            True\n",
      "  118                          transformer_d0.norm2.beta            True\n",
      "  119                         transformer_d.attention.Wx            True\n",
      "  120                         transformer_d.attention.Wt            True\n",
      "  121                         transformer_d.attention.bh            True\n",
      "  122                         transformer_d.attention.Wa            True\n",
      "  123                         transformer_d.attention.ba            True\n",
      "  124                          transformer_d.norm1.gamma            True\n",
      "  125                           transformer_d.norm1.beta            True\n",
      "  126                       transformer_d.ff.lin1.weight            True\n",
      "  127                         transformer_d.ff.lin1.bias            True\n",
      "  128                       transformer_d.ff.lin2.weight            True\n",
      "  129                         transformer_d.ff.lin2.bias            True\n",
      "  130                          transformer_d.norm2.gamma            True\n",
      "  131                           transformer_d.norm2.beta            True\n",
      "  132                           decoder_d.convs.0.weight            True\n",
      "  133                             decoder_d.convs.0.bias            True\n",
      "  134                           decoder_d.convs.1.weight            True\n",
      "  135                             decoder_d.convs.1.bias            True\n",
      "  136                           decoder_d.convs.2.weight            True\n",
      "  137                             decoder_d.convs.2.bias            True\n",
      "  138                           decoder_d.convs.3.weight            True\n",
      "  139                             decoder_d.convs.3.bias            True\n",
      "  140                           decoder_d.convs.4.weight            True\n",
      "  141                             decoder_d.convs.4.bias            True\n",
      "  142                           decoder_d.convs.5.weight            True\n",
      "  143                             decoder_d.convs.5.bias            True\n",
      "  144                           decoder_d.convs.6.weight            True\n",
      "  145                             decoder_d.convs.6.bias            True\n",
      "  146                                      conv_d.weight            True\n",
      "  147                                        conv_d.bias            True\n",
      "  148                          pick_lstms.0.weight_ih_l0            True\n",
      "  149                          pick_lstms.0.weight_hh_l0            True\n",
      "  150                            pick_lstms.0.bias_ih_l0            True\n",
      "  151                            pick_lstms.0.bias_hh_l0            True\n",
      "  152                               pick_attentions.0.Wx            True\n",
      "  153                               pick_attentions.0.Wt            True\n",
      "  154                               pick_attentions.0.bh            True\n",
      "  155                               pick_attentions.0.Wa            True\n",
      "  156                               pick_attentions.0.ba            True\n",
      "  157                     pick_decoders.0.convs.0.weight            True\n",
      "  158                       pick_decoders.0.convs.0.bias            True\n",
      "  159                     pick_decoders.0.convs.1.weight            True\n",
      "  160                       pick_decoders.0.convs.1.bias            True\n",
      "  161                     pick_decoders.0.convs.2.weight            True\n",
      "  162                       pick_decoders.0.convs.2.bias            True\n",
      "  163                     pick_decoders.0.convs.3.weight            True\n",
      "  164                       pick_decoders.0.convs.3.bias            True\n",
      "  165                     pick_decoders.0.convs.4.weight            True\n",
      "  166                       pick_decoders.0.convs.4.bias            True\n",
      "  167                     pick_decoders.0.convs.5.weight            True\n",
      "  168                       pick_decoders.0.convs.5.bias            True\n",
      "  169                     pick_decoders.0.convs.6.weight            True\n",
      "  170                       pick_decoders.0.convs.6.bias            True\n",
      "  171                                pick_convs.0.weight            True\n",
      "  172                                  pick_convs.0.bias            True\n"
     ]
    }
   ],
   "source": [
    "# Make the last layers trainable (require_grad) and freeze the previous ones.\n",
    "# last_layer = 8  ################ CHANGE THIS ###################\n",
    "last_layer =157  ################ CHANGE THIS ################### \n",
    "format_print = \"%5i %50s %15s\"\n",
    "#target_P_layers = trainable_layers_P_pi1|ck[-last_layer*2:]\n",
    "target_P_layers = trainable_layers_P_pick[-last_layer:]\n",
    "print(target_P_layers)\n",
    "#target_Detection_layers = trainable_layers_detection[-last_layer*2:]\n",
    "target_Detection_layers = trainable_layers_detection[-last_layer:]\n",
    "print(target_Detection_layers)\n",
    "print(\"%5s %50s %15s\" % ('INDEX', 'LAYER', 'TRAINABLE'))\n",
    "for i, (name, param) in enumerate(model.named_parameters()):\n",
    "    if name in target_P_layers or name in target_Detection_layers:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "    print(format_print % (i, name, param.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03f1437a-0ec1-4747-8634-e7060a21999b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to execute the training.\n",
    "import gc\n",
    "\n",
    "def exec_train(thr_epochs, loss_weights, train_loader, dev_loader, test_loader, path_weights, model, path_stats, last_layers):\n",
    "    \n",
    "    df_info = pd.DataFrame()\n",
    "    df_info['Detection weight'] = loss_weights[0]\n",
    "    df_info['P weight'] = loss_weights[1]\n",
    "    df_info['Learning rate'] = learning_rate\n",
    "\n",
    "    count = 0\n",
    "    t = 105\n",
    "    min_val_loss = 100\n",
    "\n",
    "    # IN case the process is stopped and you need to resume the last point of the training.\n",
    "    best_val_loss = [0, 0]\n",
    "    if t == 0:\n",
    "        val_loss_ls = []\n",
    "        val_acc_ls = []\n",
    "        epoch_ls = []\n",
    "        train_loss_ls = []\n",
    "        train_acc_ls = []\n",
    "        test_loss_ls = []\n",
    "        test_acc_ls = []\n",
    "    else:\n",
    "        # Path to your pickle file\n",
    "        file_name = 'stats.pkl'\n",
    "        \n",
    "        # Open the file in read-binary ('rb') mode\n",
    "        with open(file_name, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "        epoch_ls, train_loss_ls, train_acc_ls, val_loss_ls, val_acc_ls, test_loss_ls, test_acc_ls = data\n",
    "        model.load_state_dict(torch.load(f'ep105_wc_weights.pt')) # CHANGE THIS EACH TIME YOU RERUN THE CODE\n",
    "        min_val_loss = np.min(val_loss_ls)\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    while count!=thr_epochs:\n",
    "        \n",
    "        t += 1\n",
    "        print(f'Epoch {t}\\n------------------------')\n",
    "        [train_loss, train_acc] = train_loop(train_loader, loss_weights) # train metrics\n",
    "\n",
    "        # Validation loop with no tracking gradient\n",
    "        with torch.no_grad():\n",
    "            [val_loss, val_acc] = val_loop(dev_loader, loss_weights) # test metrics\n",
    "            [test_loss, test_acc] = val_loop(test_loader, loss_weights) # test metrics\n",
    "\n",
    "        # epochs\n",
    "        epoch_ls.append(t)\n",
    "        # train\n",
    "        train_loss_ls.append(train_loss)\n",
    "        train_acc_ls.append(train_acc)\n",
    "        # validation\n",
    "        val_loss_ls.append(val_loss)\n",
    "        val_acc_ls.append(val_acc)\n",
    "        # test\n",
    "        test_loss_ls.append(test_loss)\n",
    "        test_acc_ls.append(test_acc)\n",
    "\n",
    "        if val_loss<min_val_loss:  # intiialize the best loss\n",
    "            min_val_loss = val_loss\n",
    "            print('---------------------------------------------- BEST LOSS: {} ------------------------------------'.format(min_val_loss))\n",
    "            count = 0\n",
    "            # Save the weights locally for the best model\n",
    "            torch.save(model.state_dict(), f'ep{str(t)}_' + path_weights)\n",
    "            best_val_loss[0] = t\n",
    "            best_val_loss[1] = min_val_loss\n",
    "        else:\n",
    "            count += 1\n",
    "\n",
    "        torch.save(model.state_dict(), f'temp_ep{str(t)}' + path_weights)\n",
    "\n",
    "        stats_ls = [epoch_ls, train_loss_ls, train_acc_ls, val_loss_ls, val_acc_ls, test_loss_ls, test_acc_ls]\n",
    "        with open(path_stats, \"wb\") as open_file:\n",
    "            pickle.dump(stats_ls, open_file)\n",
    "\n",
    "        # Clear large variables and invoke garbage collection\n",
    "        del train_loss, train_acc, val_loss, val_acc, test_loss, test_acc\n",
    "        gc.collect()\n",
    "        \n",
    "        #file_name = path_stats\n",
    "        #open_file = open(file_name, \"wb\")\n",
    "        #pickle.dump(stats_ls, open_file)\n",
    "        #open_file.close()\n",
    "\n",
    "    df_info['Epochs'] = best_val_loss[0]\n",
    "    df_info['Best validation loss'] = best_val_loss[1]\n",
    "    df_info['Trained last layers'] = last_layers\n",
    "    df_info.to_excel(path_weights.replace('pt', 'xlsx'))\n",
    "        \n",
    "    return epoch_ls, train_loss_ls, train_acc_ls, val_loss_ls, val_acc_ls, test_loss_ls, test_acc_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a6d1bb3-5ebf-47bb-8484-1aaa7b95a540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/visitor_cp1/miniconda3/lib/python3.12/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/tmp/ipykernel_2274539/2629472207.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'ep105_wc_weights.pt')) # CHANGE THIS EACH TIME YOU RERUN THE CODE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.025799, accuracy: 92.884%, batch [    0/756895]\n",
      "loss: 0.019449, accuracy: 93.053%, batch [ 1280/756895]\n",
      "loss: 0.021125, accuracy: 92.946%, batch [ 2560/756895]\n",
      "loss: 0.017635, accuracy: 93.123%, batch [ 3840/756895]\n",
      "loss: 0.016663, accuracy: 93.076%, batch [ 5120/756895]\n",
      "loss: 0.020072, accuracy: 93.009%, batch [ 6400/756895]\n",
      "loss: 0.017334, accuracy: 93.054%, batch [ 7680/756895]\n",
      "loss: 0.017064, accuracy: 93.080%, batch [ 8960/756895]\n",
      "loss: 0.020432, accuracy: 92.960%, batch [10240/756895]\n",
      "loss: 0.016315, accuracy: 93.336%, batch [11520/756895]\n",
      "loss: 0.016713, accuracy: 93.187%, batch [12800/756895]\n",
      "loss: 0.019909, accuracy: 93.120%, batch [14080/756895]\n",
      "loss: 0.015742, accuracy: 93.078%, batch [15360/756895]\n",
      "loss: 0.018793, accuracy: 92.941%, batch [16640/756895]\n",
      "loss: 0.019152, accuracy: 92.974%, batch [17920/756895]\n",
      "loss: 0.019002, accuracy: 93.155%, batch [19200/756895]\n",
      "loss: 0.016205, accuracy: 93.203%, batch [20480/756895]\n",
      "loss: 0.015827, accuracy: 93.284%, batch [21760/756895]\n",
      "loss: 0.017969, accuracy: 93.002%, batch [23040/756895]\n",
      "loss: 0.018196, accuracy: 93.105%, batch [24320/756895]\n",
      "loss: 0.014764, accuracy: 93.236%, batch [25600/756895]\n",
      "loss: 0.021865, accuracy: 92.851%, batch [26880/756895]\n",
      "loss: 0.020369, accuracy: 92.948%, batch [28160/756895]\n",
      "loss: 0.018356, accuracy: 93.063%, batch [29440/756895]\n",
      "loss: 0.020564, accuracy: 93.123%, batch [30720/756895]\n",
      "loss: 0.016580, accuracy: 92.981%, batch [32000/756895]\n",
      "loss: 0.018225, accuracy: 93.033%, batch [33280/756895]\n",
      "loss: 0.020066, accuracy: 93.191%, batch [34560/756895]\n",
      "loss: 0.019244, accuracy: 93.146%, batch [35840/756895]\n",
      "loss: 0.017559, accuracy: 93.160%, batch [37120/756895]\n",
      "loss: 0.022914, accuracy: 93.029%, batch [38400/756895]\n",
      "loss: 0.024860, accuracy: 93.128%, batch [39680/756895]\n",
      "loss: 0.021076, accuracy: 92.818%, batch [40960/756895]\n",
      "loss: 0.022065, accuracy: 93.225%, batch [42240/756895]\n",
      "loss: 0.016221, accuracy: 93.050%, batch [43520/756895]\n",
      "loss: 0.017810, accuracy: 93.090%, batch [44800/756895]\n",
      "loss: 0.016767, accuracy: 93.036%, batch [46080/756895]\n",
      "loss: 0.021982, accuracy: 92.995%, batch [47360/756895]\n",
      "loss: 0.016444, accuracy: 93.152%, batch [48640/756895]\n",
      "loss: 0.016175, accuracy: 93.154%, batch [49920/756895]\n",
      "loss: 0.025369, accuracy: 92.826%, batch [51200/756895]\n",
      "loss: 0.016336, accuracy: 93.055%, batch [52480/756895]\n",
      "loss: 0.025210, accuracy: 92.901%, batch [53760/756895]\n",
      "loss: 0.016464, accuracy: 93.044%, batch [55040/756895]\n",
      "loss: 0.016736, accuracy: 93.059%, batch [56320/756895]\n",
      "loss: 0.016631, accuracy: 93.045%, batch [57600/756895]\n",
      "loss: 0.022276, accuracy: 92.939%, batch [58880/756895]\n",
      "loss: 0.015524, accuracy: 93.205%, batch [60160/756895]\n",
      "loss: 0.020144, accuracy: 93.126%, batch [61440/756895]\n",
      "loss: 0.016848, accuracy: 93.213%, batch [62720/756895]\n",
      "loss: 0.018669, accuracy: 92.971%, batch [64000/756895]\n",
      "loss: 0.017836, accuracy: 93.134%, batch [65280/756895]\n",
      "loss: 0.019325, accuracy: 93.101%, batch [66560/756895]\n",
      "loss: 0.016839, accuracy: 93.054%, batch [67840/756895]\n",
      "loss: 0.019709, accuracy: 93.127%, batch [69120/756895]\n",
      "loss: 0.017945, accuracy: 93.051%, batch [70400/756895]\n",
      "loss: 0.018065, accuracy: 93.157%, batch [71680/756895]\n",
      "loss: 0.013746, accuracy: 93.317%, batch [72960/756895]\n",
      "loss: 0.013341, accuracy: 93.252%, batch [74240/756895]\n",
      "loss: 0.021887, accuracy: 92.951%, batch [75520/756895]\n",
      "loss: 0.017121, accuracy: 93.185%, batch [76800/756895]\n",
      "loss: 0.016025, accuracy: 93.288%, batch [78080/756895]\n",
      "loss: 0.017358, accuracy: 93.135%, batch [79360/756895]\n",
      "loss: 0.017937, accuracy: 93.172%, batch [80640/756895]\n",
      "loss: 0.017453, accuracy: 93.171%, batch [81920/756895]\n",
      "loss: 0.019847, accuracy: 93.080%, batch [83200/756895]\n",
      "loss: 0.020644, accuracy: 93.200%, batch [84480/756895]\n",
      "loss: 0.015148, accuracy: 93.323%, batch [85760/756895]\n",
      "loss: 0.020302, accuracy: 93.050%, batch [87040/756895]\n",
      "loss: 0.019064, accuracy: 93.153%, batch [88320/756895]\n",
      "loss: 0.019875, accuracy: 93.083%, batch [89600/756895]\n",
      "loss: 0.018590, accuracy: 92.918%, batch [90880/756895]\n",
      "loss: 0.016743, accuracy: 93.278%, batch [92160/756895]\n",
      "loss: 0.016447, accuracy: 93.119%, batch [93440/756895]\n",
      "loss: 0.020524, accuracy: 92.964%, batch [94720/756895]\n",
      "loss: 0.017783, accuracy: 93.074%, batch [96000/756895]\n",
      "loss: 0.017918, accuracy: 93.030%, batch [97280/756895]\n",
      "loss: 0.018024, accuracy: 93.084%, batch [98560/756895]\n",
      "loss: 0.022765, accuracy: 93.004%, batch [99840/756895]\n",
      "loss: 0.019776, accuracy: 92.922%, batch [101120/756895]\n",
      "loss: 0.022396, accuracy: 92.870%, batch [102400/756895]\n",
      "loss: 0.014394, accuracy: 93.351%, batch [103680/756895]\n",
      "loss: 0.017203, accuracy: 93.237%, batch [104960/756895]\n",
      "loss: 0.018458, accuracy: 92.973%, batch [106240/756895]\n",
      "loss: 0.016259, accuracy: 93.199%, batch [107520/756895]\n",
      "loss: 0.017818, accuracy: 93.187%, batch [108800/756895]\n",
      "loss: 0.017146, accuracy: 93.137%, batch [110080/756895]\n",
      "loss: 0.020927, accuracy: 92.824%, batch [111360/756895]\n",
      "loss: 0.023166, accuracy: 92.915%, batch [112640/756895]\n",
      "loss: 0.016337, accuracy: 93.247%, batch [113920/756895]\n",
      "loss: 0.022709, accuracy: 92.985%, batch [115200/756895]\n",
      "loss: 0.019128, accuracy: 93.059%, batch [116480/756895]\n",
      "loss: 0.015181, accuracy: 93.100%, batch [117760/756895]\n",
      "loss: 0.019726, accuracy: 92.938%, batch [119040/756895]\n",
      "loss: 0.018655, accuracy: 93.017%, batch [120320/756895]\n",
      "loss: 0.015617, accuracy: 93.297%, batch [121600/756895]\n",
      "loss: 0.019650, accuracy: 93.024%, batch [122880/756895]\n",
      "loss: 0.016072, accuracy: 93.143%, batch [124160/756895]\n",
      "loss: 0.020744, accuracy: 92.905%, batch [125440/756895]\n",
      "loss: 0.015864, accuracy: 93.092%, batch [126720/756895]\n",
      "loss: 0.025133, accuracy: 92.813%, batch [128000/756895]\n",
      "loss: 0.018800, accuracy: 93.105%, batch [129280/756895]\n",
      "loss: 0.018449, accuracy: 92.978%, batch [130560/756895]\n",
      "loss: 0.017900, accuracy: 93.209%, batch [131840/756895]\n",
      "loss: 0.021142, accuracy: 93.167%, batch [133120/756895]\n",
      "loss: 0.023501, accuracy: 93.084%, batch [134400/756895]\n",
      "loss: 0.018859, accuracy: 92.994%, batch [135680/756895]\n",
      "loss: 0.021640, accuracy: 92.961%, batch [136960/756895]\n",
      "loss: 0.017766, accuracy: 93.044%, batch [138240/756895]\n",
      "loss: 0.019669, accuracy: 93.150%, batch [139520/756895]\n",
      "loss: 0.021736, accuracy: 92.836%, batch [140800/756895]\n",
      "loss: 0.019294, accuracy: 93.042%, batch [142080/756895]\n",
      "loss: 0.016463, accuracy: 93.066%, batch [143360/756895]\n",
      "loss: 0.029448, accuracy: 92.879%, batch [144640/756895]\n",
      "loss: 0.019294, accuracy: 93.113%, batch [145920/756895]\n",
      "loss: 0.016722, accuracy: 93.104%, batch [147200/756895]\n",
      "loss: 0.024037, accuracy: 92.780%, batch [148480/756895]\n",
      "loss: 0.016888, accuracy: 93.157%, batch [149760/756895]\n",
      "loss: 0.016479, accuracy: 93.230%, batch [151040/756895]\n",
      "loss: 0.018861, accuracy: 92.866%, batch [152320/756895]\n",
      "loss: 0.016248, accuracy: 93.228%, batch [153600/756895]\n",
      "loss: 0.017335, accuracy: 93.014%, batch [154880/756895]\n",
      "loss: 0.015320, accuracy: 93.249%, batch [156160/756895]\n",
      "loss: 0.019312, accuracy: 92.967%, batch [157440/756895]\n",
      "loss: 0.016474, accuracy: 93.294%, batch [158720/756895]\n",
      "loss: 0.019517, accuracy: 93.077%, batch [160000/756895]\n",
      "loss: 0.019792, accuracy: 93.014%, batch [161280/756895]\n",
      "loss: 0.023902, accuracy: 93.012%, batch [162560/756895]\n",
      "loss: 0.018319, accuracy: 92.886%, batch [163840/756895]\n",
      "loss: 0.020919, accuracy: 93.050%, batch [165120/756895]\n",
      "loss: 0.020687, accuracy: 92.890%, batch [166400/756895]\n",
      "loss: 0.019840, accuracy: 93.186%, batch [167680/756895]\n",
      "loss: 0.016208, accuracy: 93.218%, batch [168960/756895]\n",
      "loss: 0.015440, accuracy: 93.190%, batch [170240/756895]\n",
      "loss: 0.019804, accuracy: 93.015%, batch [171520/756895]\n",
      "loss: 0.021093, accuracy: 92.713%, batch [172800/756895]\n",
      "loss: 0.022266, accuracy: 92.955%, batch [174080/756895]\n",
      "loss: 0.015370, accuracy: 93.277%, batch [175360/756895]\n",
      "loss: 0.017720, accuracy: 93.083%, batch [176640/756895]\n",
      "loss: 0.020654, accuracy: 92.908%, batch [177920/756895]\n",
      "loss: 0.014209, accuracy: 93.275%, batch [179200/756895]\n",
      "loss: 0.021503, accuracy: 92.862%, batch [180480/756895]\n",
      "loss: 0.020112, accuracy: 92.831%, batch [181760/756895]\n",
      "loss: 0.018692, accuracy: 93.170%, batch [183040/756895]\n",
      "loss: 0.020927, accuracy: 92.818%, batch [184320/756895]\n",
      "loss: 0.019270, accuracy: 93.173%, batch [185600/756895]\n",
      "loss: 0.024253, accuracy: 92.929%, batch [186880/756895]\n",
      "loss: 0.018650, accuracy: 93.238%, batch [188160/756895]\n",
      "loss: 0.016849, accuracy: 93.264%, batch [189440/756895]\n",
      "loss: 0.016315, accuracy: 93.047%, batch [190720/756895]\n",
      "loss: 0.017344, accuracy: 92.976%, batch [192000/756895]\n",
      "loss: 0.024281, accuracy: 92.855%, batch [193280/756895]\n",
      "loss: 0.019234, accuracy: 92.884%, batch [194560/756895]\n",
      "loss: 0.019146, accuracy: 92.921%, batch [195840/756895]\n",
      "loss: 0.023509, accuracy: 92.824%, batch [197120/756895]\n",
      "loss: 0.018144, accuracy: 92.971%, batch [198400/756895]\n",
      "loss: 0.020783, accuracy: 92.831%, batch [199680/756895]\n",
      "loss: 0.018918, accuracy: 93.143%, batch [200960/756895]\n",
      "loss: 0.019534, accuracy: 93.126%, batch [202240/756895]\n",
      "loss: 0.017136, accuracy: 93.109%, batch [203520/756895]\n",
      "loss: 0.017553, accuracy: 92.984%, batch [204800/756895]\n",
      "loss: 0.017171, accuracy: 93.317%, batch [206080/756895]\n",
      "loss: 0.017345, accuracy: 92.913%, batch [207360/756895]\n",
      "loss: 0.016194, accuracy: 93.226%, batch [208640/756895]\n",
      "loss: 0.019435, accuracy: 93.060%, batch [209920/756895]\n",
      "loss: 0.016870, accuracy: 93.121%, batch [211200/756895]\n",
      "loss: 0.020431, accuracy: 92.965%, batch [212480/756895]\n",
      "loss: 0.017777, accuracy: 92.941%, batch [213760/756895]\n",
      "loss: 0.017212, accuracy: 93.087%, batch [215040/756895]\n",
      "loss: 0.017552, accuracy: 93.219%, batch [216320/756895]\n",
      "loss: 0.018690, accuracy: 93.106%, batch [217600/756895]\n",
      "loss: 0.021109, accuracy: 92.812%, batch [218880/756895]\n",
      "loss: 0.019782, accuracy: 93.017%, batch [220160/756895]\n",
      "loss: 0.017456, accuracy: 93.243%, batch [221440/756895]\n",
      "loss: 0.018795, accuracy: 92.998%, batch [222720/756895]\n",
      "loss: 0.017473, accuracy: 93.153%, batch [224000/756895]\n",
      "loss: 0.015164, accuracy: 93.267%, batch [225280/756895]\n",
      "loss: 0.014617, accuracy: 93.257%, batch [226560/756895]\n",
      "loss: 0.020425, accuracy: 92.958%, batch [227840/756895]\n",
      "loss: 0.016059, accuracy: 93.194%, batch [229120/756895]\n",
      "loss: 0.016679, accuracy: 93.148%, batch [230400/756895]\n",
      "loss: 0.020299, accuracy: 92.751%, batch [231680/756895]\n",
      "loss: 0.017926, accuracy: 92.998%, batch [232960/756895]\n",
      "loss: 0.018017, accuracy: 92.962%, batch [234240/756895]\n",
      "loss: 0.017702, accuracy: 93.158%, batch [235520/756895]\n",
      "loss: 0.015732, accuracy: 93.101%, batch [236800/756895]\n",
      "loss: 0.016587, accuracy: 93.243%, batch [238080/756895]\n",
      "loss: 0.020468, accuracy: 92.821%, batch [239360/756895]\n",
      "loss: 0.021590, accuracy: 92.852%, batch [240640/756895]\n",
      "loss: 0.021259, accuracy: 92.930%, batch [241920/756895]\n",
      "loss: 0.019439, accuracy: 93.059%, batch [243200/756895]\n",
      "loss: 0.017146, accuracy: 93.230%, batch [244480/756895]\n",
      "loss: 0.017967, accuracy: 93.127%, batch [245760/756895]\n",
      "loss: 0.016611, accuracy: 93.138%, batch [247040/756895]\n",
      "loss: 0.015738, accuracy: 93.272%, batch [248320/756895]\n",
      "loss: 0.026003, accuracy: 92.968%, batch [249600/756895]\n",
      "loss: 0.019525, accuracy: 93.001%, batch [250880/756895]\n",
      "loss: 0.022317, accuracy: 92.951%, batch [252160/756895]\n",
      "loss: 0.015550, accuracy: 93.286%, batch [253440/756895]\n",
      "loss: 0.017539, accuracy: 93.096%, batch [254720/756895]\n",
      "loss: 0.024637, accuracy: 92.921%, batch [256000/756895]\n",
      "loss: 0.016994, accuracy: 92.958%, batch [257280/756895]\n",
      "loss: 0.016092, accuracy: 93.061%, batch [258560/756895]\n",
      "loss: 0.018609, accuracy: 93.256%, batch [259840/756895]\n",
      "loss: 0.018755, accuracy: 93.129%, batch [261120/756895]\n",
      "loss: 0.023962, accuracy: 92.979%, batch [262400/756895]\n",
      "loss: 0.022822, accuracy: 92.735%, batch [263680/756895]\n",
      "loss: 0.019762, accuracy: 93.134%, batch [264960/756895]\n",
      "loss: 0.025518, accuracy: 93.005%, batch [266240/756895]\n",
      "loss: 0.021030, accuracy: 92.921%, batch [267520/756895]\n",
      "loss: 0.022847, accuracy: 92.863%, batch [268800/756895]\n",
      "loss: 0.022983, accuracy: 92.815%, batch [270080/756895]\n",
      "loss: 0.019280, accuracy: 93.114%, batch [271360/756895]\n",
      "loss: 0.023452, accuracy: 92.954%, batch [272640/756895]\n",
      "loss: 0.015885, accuracy: 93.138%, batch [273920/756895]\n",
      "loss: 0.015860, accuracy: 93.220%, batch [275200/756895]\n",
      "loss: 0.016813, accuracy: 93.085%, batch [276480/756895]\n",
      "loss: 0.021778, accuracy: 93.217%, batch [277760/756895]\n",
      "loss: 0.017022, accuracy: 93.138%, batch [279040/756895]\n",
      "loss: 0.014681, accuracy: 93.304%, batch [280320/756895]\n",
      "loss: 0.015896, accuracy: 93.064%, batch [281600/756895]\n",
      "loss: 0.017962, accuracy: 93.269%, batch [282880/756895]\n",
      "loss: 0.015984, accuracy: 93.246%, batch [284160/756895]\n",
      "loss: 0.020635, accuracy: 93.060%, batch [285440/756895]\n",
      "loss: 0.015038, accuracy: 93.289%, batch [286720/756895]\n",
      "loss: 0.017335, accuracy: 93.200%, batch [288000/756895]\n",
      "loss: 0.023093, accuracy: 92.876%, batch [289280/756895]\n",
      "loss: 0.016335, accuracy: 93.222%, batch [290560/756895]\n",
      "loss: 0.022908, accuracy: 93.091%, batch [291840/756895]\n",
      "loss: 0.018945, accuracy: 93.061%, batch [293120/756895]\n",
      "loss: 0.017108, accuracy: 93.176%, batch [294400/756895]\n",
      "loss: 0.021171, accuracy: 93.069%, batch [295680/756895]\n",
      "loss: 0.019887, accuracy: 93.148%, batch [296960/756895]\n",
      "loss: 0.014661, accuracy: 93.199%, batch [298240/756895]\n",
      "loss: 0.021055, accuracy: 92.915%, batch [299520/756895]\n",
      "loss: 0.018568, accuracy: 93.190%, batch [300800/756895]\n",
      "loss: 0.017499, accuracy: 93.093%, batch [302080/756895]\n",
      "loss: 0.015625, accuracy: 93.174%, batch [303360/756895]\n",
      "loss: 0.017029, accuracy: 93.150%, batch [304640/756895]\n",
      "loss: 0.017796, accuracy: 93.108%, batch [305920/756895]\n",
      "loss: 0.014646, accuracy: 93.153%, batch [307200/756895]\n",
      "loss: 0.027272, accuracy: 92.959%, batch [308480/756895]\n",
      "loss: 0.017326, accuracy: 93.140%, batch [309760/756895]\n",
      "loss: 0.019184, accuracy: 93.135%, batch [311040/756895]\n",
      "loss: 0.023631, accuracy: 92.940%, batch [312320/756895]\n",
      "loss: 0.018534, accuracy: 92.952%, batch [313600/756895]\n",
      "loss: 0.017604, accuracy: 93.145%, batch [314880/756895]\n",
      "loss: 0.015724, accuracy: 93.132%, batch [316160/756895]\n",
      "loss: 0.019335, accuracy: 93.045%, batch [317440/756895]\n",
      "loss: 0.021167, accuracy: 92.977%, batch [318720/756895]\n",
      "loss: 0.025015, accuracy: 92.871%, batch [320000/756895]\n",
      "loss: 0.028070, accuracy: 92.791%, batch [321280/756895]\n",
      "loss: 0.023629, accuracy: 92.978%, batch [322560/756895]\n",
      "loss: 0.024535, accuracy: 92.800%, batch [323840/756895]\n",
      "loss: 0.016524, accuracy: 93.085%, batch [325120/756895]\n",
      "loss: 0.015806, accuracy: 93.181%, batch [326400/756895]\n",
      "loss: 0.021872, accuracy: 92.809%, batch [327680/756895]\n",
      "loss: 0.019173, accuracy: 93.132%, batch [328960/756895]\n",
      "loss: 0.017346, accuracy: 93.201%, batch [330240/756895]\n",
      "loss: 0.015180, accuracy: 93.278%, batch [331520/756895]\n",
      "loss: 0.015392, accuracy: 93.235%, batch [332800/756895]\n",
      "loss: 0.023019, accuracy: 93.080%, batch [334080/756895]\n",
      "loss: 0.017628, accuracy: 93.117%, batch [335360/756895]\n",
      "loss: 0.022050, accuracy: 93.000%, batch [336640/756895]\n",
      "loss: 0.017487, accuracy: 93.272%, batch [337920/756895]\n",
      "loss: 0.015718, accuracy: 93.213%, batch [339200/756895]\n",
      "loss: 0.018478, accuracy: 92.971%, batch [340480/756895]\n",
      "loss: 0.021257, accuracy: 92.785%, batch [341760/756895]\n",
      "loss: 0.020171, accuracy: 93.099%, batch [343040/756895]\n",
      "loss: 0.017050, accuracy: 93.181%, batch [344320/756895]\n",
      "loss: 0.018702, accuracy: 93.164%, batch [345600/756895]\n",
      "loss: 0.022598, accuracy: 92.841%, batch [346880/756895]\n",
      "loss: 0.018146, accuracy: 93.117%, batch [348160/756895]\n",
      "loss: 0.019121, accuracy: 93.123%, batch [349440/756895]\n",
      "loss: 0.019732, accuracy: 92.873%, batch [350720/756895]\n",
      "loss: 0.015771, accuracy: 93.271%, batch [352000/756895]\n",
      "loss: 0.020891, accuracy: 93.107%, batch [353280/756895]\n",
      "loss: 0.019137, accuracy: 93.029%, batch [354560/756895]\n",
      "loss: 0.017600, accuracy: 93.080%, batch [355840/756895]\n",
      "loss: 0.022310, accuracy: 93.081%, batch [357120/756895]\n",
      "loss: 0.019748, accuracy: 92.983%, batch [358400/756895]\n",
      "loss: 0.016988, accuracy: 93.114%, batch [359680/756895]\n",
      "loss: 0.017679, accuracy: 93.158%, batch [360960/756895]\n",
      "loss: 0.016695, accuracy: 92.964%, batch [362240/756895]\n",
      "loss: 0.028340, accuracy: 92.804%, batch [363520/756895]\n",
      "loss: 0.016248, accuracy: 93.165%, batch [364800/756895]\n",
      "loss: 0.014071, accuracy: 93.159%, batch [366080/756895]\n",
      "loss: 0.018400, accuracy: 93.095%, batch [367360/756895]\n",
      "loss: 0.022186, accuracy: 92.853%, batch [368640/756895]\n",
      "loss: 0.017731, accuracy: 93.176%, batch [369920/756895]\n",
      "loss: 0.025866, accuracy: 92.865%, batch [371200/756895]\n",
      "loss: 0.017965, accuracy: 93.008%, batch [372480/756895]\n",
      "loss: 0.015180, accuracy: 93.157%, batch [373760/756895]\n",
      "loss: 0.015583, accuracy: 93.294%, batch [375040/756895]\n",
      "loss: 0.022462, accuracy: 93.128%, batch [376320/756895]\n",
      "loss: 0.016271, accuracy: 93.260%, batch [377600/756895]\n",
      "loss: 0.018536, accuracy: 93.134%, batch [378880/756895]\n",
      "loss: 0.022203, accuracy: 92.903%, batch [380160/756895]\n",
      "loss: 0.019131, accuracy: 92.951%, batch [381440/756895]\n",
      "loss: 0.016001, accuracy: 93.152%, batch [382720/756895]\n",
      "loss: 0.024952, accuracy: 92.908%, batch [384000/756895]\n",
      "loss: 0.018480, accuracy: 93.190%, batch [385280/756895]\n",
      "loss: 0.018056, accuracy: 92.966%, batch [386560/756895]\n",
      "loss: 0.015556, accuracy: 93.035%, batch [387840/756895]\n",
      "loss: 0.017325, accuracy: 93.148%, batch [389120/756895]\n",
      "loss: 0.016608, accuracy: 93.117%, batch [390400/756895]\n",
      "loss: 0.016562, accuracy: 93.342%, batch [391680/756895]\n",
      "loss: 0.018322, accuracy: 93.037%, batch [392960/756895]\n",
      "loss: 0.017318, accuracy: 93.194%, batch [394240/756895]\n",
      "loss: 0.016452, accuracy: 93.158%, batch [395520/756895]\n",
      "loss: 0.017687, accuracy: 93.106%, batch [396800/756895]\n",
      "loss: 0.019234, accuracy: 93.088%, batch [398080/756895]\n",
      "loss: 0.020298, accuracy: 93.067%, batch [399360/756895]\n",
      "loss: 0.018048, accuracy: 93.110%, batch [400640/756895]\n",
      "loss: 0.017813, accuracy: 93.056%, batch [401920/756895]\n",
      "loss: 0.016026, accuracy: 93.118%, batch [403200/756895]\n",
      "loss: 0.021738, accuracy: 92.899%, batch [404480/756895]\n",
      "loss: 0.014193, accuracy: 93.319%, batch [405760/756895]\n",
      "loss: 0.025632, accuracy: 92.927%, batch [407040/756895]\n",
      "loss: 0.017975, accuracy: 92.995%, batch [408320/756895]\n",
      "loss: 0.020084, accuracy: 92.922%, batch [409600/756895]\n",
      "loss: 0.019792, accuracy: 92.863%, batch [410880/756895]\n",
      "loss: 0.017413, accuracy: 93.029%, batch [412160/756895]\n",
      "loss: 0.016848, accuracy: 93.132%, batch [413440/756895]\n",
      "loss: 0.021986, accuracy: 92.890%, batch [414720/756895]\n",
      "loss: 0.014633, accuracy: 93.421%, batch [416000/756895]\n",
      "loss: 0.016429, accuracy: 93.120%, batch [417280/756895]\n",
      "loss: 0.020278, accuracy: 92.960%, batch [418560/756895]\n",
      "loss: 0.021005, accuracy: 93.048%, batch [419840/756895]\n",
      "loss: 0.017326, accuracy: 93.196%, batch [421120/756895]\n",
      "loss: 0.013803, accuracy: 93.079%, batch [422400/756895]\n",
      "loss: 0.024304, accuracy: 92.913%, batch [423680/756895]\n",
      "loss: 0.019823, accuracy: 93.076%, batch [424960/756895]\n",
      "loss: 0.018852, accuracy: 93.220%, batch [426240/756895]\n",
      "loss: 0.026440, accuracy: 92.849%, batch [427520/756895]\n",
      "loss: 0.017768, accuracy: 93.195%, batch [428800/756895]\n",
      "loss: 0.014552, accuracy: 93.386%, batch [430080/756895]\n",
      "loss: 0.016007, accuracy: 93.123%, batch [431360/756895]\n",
      "loss: 0.014950, accuracy: 93.243%, batch [432640/756895]\n",
      "loss: 0.019637, accuracy: 93.126%, batch [433920/756895]\n",
      "loss: 0.016331, accuracy: 93.087%, batch [435200/756895]\n",
      "loss: 0.021180, accuracy: 93.035%, batch [436480/756895]\n",
      "loss: 0.019332, accuracy: 93.123%, batch [437760/756895]\n",
      "loss: 0.015149, accuracy: 93.156%, batch [439040/756895]\n",
      "loss: 0.017408, accuracy: 93.194%, batch [440320/756895]\n",
      "loss: 0.019587, accuracy: 93.076%, batch [441600/756895]\n",
      "loss: 0.017915, accuracy: 93.223%, batch [442880/756895]\n",
      "loss: 0.020403, accuracy: 92.986%, batch [444160/756895]\n",
      "loss: 0.022066, accuracy: 92.887%, batch [445440/756895]\n",
      "loss: 0.020475, accuracy: 92.905%, batch [446720/756895]\n",
      "loss: 0.020844, accuracy: 93.143%, batch [448000/756895]\n",
      "loss: 0.017794, accuracy: 93.134%, batch [449280/756895]\n",
      "loss: 0.015864, accuracy: 92.917%, batch [450560/756895]\n",
      "loss: 0.017254, accuracy: 93.178%, batch [451840/756895]\n",
      "loss: 0.018791, accuracy: 92.931%, batch [453120/756895]\n",
      "loss: 0.017363, accuracy: 93.089%, batch [454400/756895]\n",
      "loss: 0.021274, accuracy: 93.008%, batch [455680/756895]\n",
      "loss: 0.019074, accuracy: 92.886%, batch [456960/756895]\n",
      "loss: 0.017872, accuracy: 93.161%, batch [458240/756895]\n",
      "loss: 0.018735, accuracy: 93.148%, batch [459520/756895]\n",
      "loss: 0.017888, accuracy: 93.125%, batch [460800/756895]\n",
      "loss: 0.019091, accuracy: 93.049%, batch [462080/756895]\n",
      "loss: 0.020907, accuracy: 93.021%, batch [463360/756895]\n",
      "loss: 0.020067, accuracy: 92.984%, batch [464640/756895]\n",
      "loss: 0.019200, accuracy: 93.095%, batch [465920/756895]\n",
      "loss: 0.021098, accuracy: 93.168%, batch [467200/756895]\n",
      "loss: 0.026171, accuracy: 92.890%, batch [468480/756895]\n",
      "loss: 0.022789, accuracy: 92.872%, batch [469760/756895]\n",
      "loss: 0.019045, accuracy: 93.018%, batch [471040/756895]\n",
      "loss: 0.016371, accuracy: 93.071%, batch [472320/756895]\n",
      "loss: 0.017083, accuracy: 93.026%, batch [473600/756895]\n",
      "loss: 0.024053, accuracy: 92.797%, batch [474880/756895]\n",
      "loss: 0.018109, accuracy: 93.057%, batch [476160/756895]\n",
      "loss: 0.018014, accuracy: 93.134%, batch [477440/756895]\n",
      "loss: 0.015183, accuracy: 93.266%, batch [478720/756895]\n",
      "loss: 0.021923, accuracy: 92.990%, batch [480000/756895]\n",
      "loss: 0.021991, accuracy: 92.941%, batch [481280/756895]\n",
      "loss: 0.019990, accuracy: 92.810%, batch [482560/756895]\n",
      "loss: 0.019971, accuracy: 92.928%, batch [483840/756895]\n",
      "loss: 0.022240, accuracy: 92.882%, batch [485120/756895]\n",
      "loss: 0.021186, accuracy: 92.910%, batch [486400/756895]\n",
      "loss: 0.016818, accuracy: 93.169%, batch [487680/756895]\n",
      "loss: 0.018985, accuracy: 92.957%, batch [488960/756895]\n",
      "loss: 0.022982, accuracy: 93.012%, batch [490240/756895]\n",
      "loss: 0.018465, accuracy: 93.146%, batch [491520/756895]\n",
      "loss: 0.018884, accuracy: 93.014%, batch [492800/756895]\n",
      "loss: 0.015692, accuracy: 93.074%, batch [494080/756895]\n",
      "loss: 0.017021, accuracy: 93.170%, batch [495360/756895]\n",
      "loss: 0.020584, accuracy: 92.872%, batch [496640/756895]\n",
      "loss: 0.018537, accuracy: 93.030%, batch [497920/756895]\n",
      "loss: 0.018648, accuracy: 93.240%, batch [499200/756895]\n",
      "loss: 0.018566, accuracy: 93.033%, batch [500480/756895]\n",
      "loss: 0.016879, accuracy: 93.158%, batch [501760/756895]\n",
      "loss: 0.014993, accuracy: 93.197%, batch [503040/756895]\n",
      "loss: 0.018747, accuracy: 93.140%, batch [504320/756895]\n",
      "loss: 0.016908, accuracy: 93.227%, batch [505600/756895]\n",
      "loss: 0.021464, accuracy: 93.021%, batch [506880/756895]\n",
      "loss: 0.025263, accuracy: 92.810%, batch [508160/756895]\n",
      "loss: 0.016282, accuracy: 93.222%, batch [509440/756895]\n",
      "loss: 0.019392, accuracy: 93.150%, batch [510720/756895]\n",
      "loss: 0.018329, accuracy: 93.117%, batch [512000/756895]\n",
      "loss: 0.017626, accuracy: 93.121%, batch [513280/756895]\n",
      "loss: 0.017858, accuracy: 93.011%, batch [514560/756895]\n",
      "loss: 0.020100, accuracy: 92.783%, batch [515840/756895]\n",
      "loss: 0.021022, accuracy: 93.039%, batch [517120/756895]\n",
      "loss: 0.020212, accuracy: 92.928%, batch [518400/756895]\n",
      "loss: 0.018668, accuracy: 93.041%, batch [519680/756895]\n",
      "loss: 0.019433, accuracy: 93.137%, batch [520960/756895]\n",
      "loss: 0.014257, accuracy: 93.196%, batch [522240/756895]\n",
      "loss: 0.021063, accuracy: 93.024%, batch [523520/756895]\n",
      "loss: 0.019861, accuracy: 93.128%, batch [524800/756895]\n",
      "loss: 0.027985, accuracy: 92.828%, batch [526080/756895]\n",
      "loss: 0.017858, accuracy: 93.179%, batch [527360/756895]\n",
      "loss: 0.019724, accuracy: 93.027%, batch [528640/756895]\n",
      "loss: 0.022684, accuracy: 92.974%, batch [529920/756895]\n",
      "loss: 0.016866, accuracy: 93.102%, batch [531200/756895]\n",
      "loss: 0.020637, accuracy: 93.043%, batch [532480/756895]\n",
      "loss: 0.016157, accuracy: 93.287%, batch [533760/756895]\n",
      "loss: 0.026336, accuracy: 92.984%, batch [535040/756895]\n",
      "loss: 0.021261, accuracy: 92.871%, batch [536320/756895]\n",
      "loss: 0.020226, accuracy: 92.884%, batch [537600/756895]\n",
      "loss: 0.019105, accuracy: 93.046%, batch [538880/756895]\n",
      "loss: 0.021370, accuracy: 92.876%, batch [540160/756895]\n",
      "loss: 0.025761, accuracy: 92.959%, batch [541440/756895]\n",
      "loss: 0.019074, accuracy: 93.067%, batch [542720/756895]\n",
      "loss: 0.017430, accuracy: 93.212%, batch [544000/756895]\n",
      "loss: 0.018132, accuracy: 93.280%, batch [545280/756895]\n",
      "loss: 0.016109, accuracy: 93.264%, batch [546560/756895]\n",
      "loss: 0.018025, accuracy: 93.206%, batch [547840/756895]\n",
      "loss: 0.018183, accuracy: 92.917%, batch [549120/756895]\n",
      "loss: 0.018962, accuracy: 93.013%, batch [550400/756895]\n",
      "loss: 0.020753, accuracy: 93.064%, batch [551680/756895]\n",
      "loss: 0.017081, accuracy: 92.970%, batch [552960/756895]\n",
      "loss: 0.024135, accuracy: 92.914%, batch [554240/756895]\n",
      "loss: 0.023260, accuracy: 93.094%, batch [555520/756895]\n",
      "loss: 0.020693, accuracy: 92.973%, batch [556800/756895]\n",
      "loss: 0.015991, accuracy: 93.098%, batch [558080/756895]\n",
      "loss: 0.022388, accuracy: 92.991%, batch [559360/756895]\n",
      "loss: 0.019896, accuracy: 93.044%, batch [560640/756895]\n",
      "loss: 0.022624, accuracy: 92.920%, batch [561920/756895]\n",
      "loss: 0.017338, accuracy: 93.106%, batch [563200/756895]\n",
      "loss: 0.019848, accuracy: 93.049%, batch [564480/756895]\n",
      "loss: 0.018185, accuracy: 93.020%, batch [565760/756895]\n",
      "loss: 0.017814, accuracy: 93.091%, batch [567040/756895]\n",
      "loss: 0.016538, accuracy: 93.202%, batch [568320/756895]\n",
      "loss: 0.021357, accuracy: 92.755%, batch [569600/756895]\n",
      "loss: 0.019671, accuracy: 92.963%, batch [570880/756895]\n",
      "loss: 0.017259, accuracy: 93.131%, batch [572160/756895]\n",
      "loss: 0.016932, accuracy: 93.188%, batch [573440/756895]\n",
      "loss: 0.016743, accuracy: 93.233%, batch [574720/756895]\n",
      "loss: 0.020531, accuracy: 93.057%, batch [576000/756895]\n",
      "loss: 0.019532, accuracy: 93.151%, batch [577280/756895]\n",
      "loss: 0.017505, accuracy: 93.261%, batch [578560/756895]\n",
      "loss: 0.016339, accuracy: 93.131%, batch [579840/756895]\n",
      "loss: 0.018258, accuracy: 93.074%, batch [581120/756895]\n",
      "loss: 0.018482, accuracy: 93.085%, batch [582400/756895]\n",
      "loss: 0.014112, accuracy: 93.285%, batch [583680/756895]\n",
      "loss: 0.017663, accuracy: 93.132%, batch [584960/756895]\n",
      "loss: 0.023148, accuracy: 93.098%, batch [586240/756895]\n",
      "loss: 0.019029, accuracy: 93.127%, batch [587520/756895]\n",
      "loss: 0.017465, accuracy: 93.000%, batch [588800/756895]\n",
      "loss: 0.018057, accuracy: 92.920%, batch [590080/756895]\n",
      "loss: 0.016586, accuracy: 93.307%, batch [591360/756895]\n",
      "loss: 0.017657, accuracy: 93.022%, batch [592640/756895]\n",
      "loss: 0.019680, accuracy: 93.062%, batch [593920/756895]\n",
      "loss: 0.024421, accuracy: 92.885%, batch [595200/756895]\n",
      "loss: 0.023442, accuracy: 92.995%, batch [596480/756895]\n",
      "loss: 0.016890, accuracy: 93.116%, batch [597760/756895]\n",
      "loss: 0.016207, accuracy: 93.211%, batch [599040/756895]\n",
      "loss: 0.018270, accuracy: 92.976%, batch [600320/756895]\n",
      "loss: 0.015776, accuracy: 93.331%, batch [601600/756895]\n",
      "loss: 0.019138, accuracy: 93.047%, batch [602880/756895]\n",
      "loss: 0.017613, accuracy: 92.967%, batch [604160/756895]\n",
      "loss: 0.019601, accuracy: 92.911%, batch [605440/756895]\n",
      "loss: 0.017609, accuracy: 93.226%, batch [606720/756895]\n",
      "loss: 0.020536, accuracy: 93.019%, batch [608000/756895]\n",
      "loss: 0.017745, accuracy: 93.199%, batch [609280/756895]\n",
      "loss: 0.017701, accuracy: 93.086%, batch [610560/756895]\n",
      "loss: 0.018189, accuracy: 93.081%, batch [611840/756895]\n",
      "loss: 0.018997, accuracy: 93.157%, batch [613120/756895]\n",
      "loss: 0.023107, accuracy: 92.969%, batch [614400/756895]\n",
      "loss: 0.015826, accuracy: 93.156%, batch [615680/756895]\n",
      "loss: 0.024362, accuracy: 93.200%, batch [616960/756895]\n",
      "loss: 0.022031, accuracy: 93.055%, batch [618240/756895]\n",
      "loss: 0.017999, accuracy: 92.932%, batch [619520/756895]\n",
      "loss: 0.014615, accuracy: 93.211%, batch [620800/756895]\n",
      "loss: 0.013966, accuracy: 93.274%, batch [622080/756895]\n",
      "loss: 0.021186, accuracy: 93.060%, batch [623360/756895]\n",
      "loss: 0.023745, accuracy: 92.824%, batch [624640/756895]\n",
      "loss: 0.015984, accuracy: 93.130%, batch [625920/756895]\n",
      "loss: 0.024918, accuracy: 92.803%, batch [627200/756895]\n",
      "loss: 0.025215, accuracy: 92.940%, batch [628480/756895]\n",
      "loss: 0.018096, accuracy: 92.950%, batch [629760/756895]\n",
      "loss: 0.018967, accuracy: 93.127%, batch [631040/756895]\n",
      "loss: 0.018928, accuracy: 93.058%, batch [632320/756895]\n",
      "loss: 0.014281, accuracy: 93.253%, batch [633600/756895]\n",
      "loss: 0.019981, accuracy: 92.986%, batch [634880/756895]\n",
      "loss: 0.017743, accuracy: 93.118%, batch [636160/756895]\n",
      "loss: 0.015269, accuracy: 93.277%, batch [637440/756895]\n",
      "loss: 0.018667, accuracy: 93.072%, batch [638720/756895]\n",
      "loss: 0.022115, accuracy: 93.109%, batch [640000/756895]\n",
      "loss: 0.017937, accuracy: 93.054%, batch [641280/756895]\n",
      "loss: 0.023428, accuracy: 92.974%, batch [642560/756895]\n",
      "loss: 0.022294, accuracy: 92.902%, batch [643840/756895]\n",
      "loss: 0.017461, accuracy: 93.090%, batch [645120/756895]\n",
      "loss: 0.018077, accuracy: 93.035%, batch [646400/756895]\n",
      "loss: 0.016505, accuracy: 93.133%, batch [647680/756895]\n",
      "loss: 0.023189, accuracy: 92.895%, batch [648960/756895]\n",
      "loss: 0.021815, accuracy: 92.956%, batch [650240/756895]\n",
      "loss: 0.017434, accuracy: 93.092%, batch [651520/756895]\n",
      "loss: 0.022482, accuracy: 93.062%, batch [652800/756895]\n",
      "loss: 0.018619, accuracy: 93.178%, batch [654080/756895]\n",
      "loss: 0.016233, accuracy: 93.154%, batch [655360/756895]\n",
      "loss: 0.021986, accuracy: 93.191%, batch [656640/756895]\n",
      "loss: 0.017145, accuracy: 92.971%, batch [657920/756895]\n",
      "loss: 0.019653, accuracy: 92.991%, batch [659200/756895]\n",
      "loss: 0.017127, accuracy: 93.167%, batch [660480/756895]\n",
      "loss: 0.018398, accuracy: 92.852%, batch [661760/756895]\n",
      "loss: 0.019118, accuracy: 93.065%, batch [663040/756895]\n",
      "loss: 0.019941, accuracy: 92.964%, batch [664320/756895]\n",
      "loss: 0.015974, accuracy: 93.134%, batch [665600/756895]\n",
      "loss: 0.020249, accuracy: 93.094%, batch [666880/756895]\n",
      "loss: 0.016813, accuracy: 93.254%, batch [668160/756895]\n",
      "loss: 0.018475, accuracy: 92.932%, batch [669440/756895]\n",
      "loss: 0.022398, accuracy: 92.930%, batch [670720/756895]\n",
      "loss: 0.019090, accuracy: 93.124%, batch [672000/756895]\n",
      "loss: 0.016077, accuracy: 93.133%, batch [673280/756895]\n",
      "loss: 0.015041, accuracy: 93.083%, batch [674560/756895]\n",
      "loss: 0.023752, accuracy: 92.747%, batch [675840/756895]\n",
      "loss: 0.017644, accuracy: 92.960%, batch [677120/756895]\n",
      "loss: 0.017362, accuracy: 93.115%, batch [678400/756895]\n",
      "loss: 0.014466, accuracy: 93.275%, batch [679680/756895]\n",
      "loss: 0.019536, accuracy: 93.108%, batch [680960/756895]\n",
      "loss: 0.020929, accuracy: 93.040%, batch [682240/756895]\n",
      "loss: 0.020214, accuracy: 93.111%, batch [683520/756895]\n",
      "loss: 0.022043, accuracy: 92.924%, batch [684800/756895]\n",
      "loss: 0.019446, accuracy: 93.043%, batch [686080/756895]\n",
      "loss: 0.019263, accuracy: 93.141%, batch [687360/756895]\n",
      "loss: 0.019739, accuracy: 93.257%, batch [688640/756895]\n",
      "loss: 0.016262, accuracy: 92.921%, batch [689920/756895]\n",
      "loss: 0.018193, accuracy: 93.134%, batch [691200/756895]\n",
      "loss: 0.018806, accuracy: 92.793%, batch [692480/756895]\n",
      "loss: 0.022088, accuracy: 92.860%, batch [693760/756895]\n",
      "loss: 0.016449, accuracy: 93.099%, batch [695040/756895]\n",
      "loss: 0.017362, accuracy: 93.268%, batch [696320/756895]\n",
      "loss: 0.014557, accuracy: 93.373%, batch [697600/756895]\n",
      "loss: 0.018919, accuracy: 93.155%, batch [698880/756895]\n",
      "loss: 0.016195, accuracy: 93.183%, batch [700160/756895]\n",
      "loss: 0.017176, accuracy: 92.919%, batch [701440/756895]\n",
      "loss: 0.014955, accuracy: 93.245%, batch [702720/756895]\n",
      "loss: 0.016864, accuracy: 93.196%, batch [704000/756895]\n",
      "loss: 0.018057, accuracy: 93.051%, batch [705280/756895]\n",
      "loss: 0.018099, accuracy: 93.188%, batch [706560/756895]\n",
      "loss: 0.020953, accuracy: 93.073%, batch [707840/756895]\n",
      "loss: 0.018641, accuracy: 93.136%, batch [709120/756895]\n",
      "loss: 0.019004, accuracy: 93.006%, batch [710400/756895]\n",
      "loss: 0.016901, accuracy: 93.221%, batch [711680/756895]\n",
      "loss: 0.016619, accuracy: 93.174%, batch [712960/756895]\n",
      "loss: 0.017732, accuracy: 93.280%, batch [714240/756895]\n",
      "loss: 0.017173, accuracy: 93.131%, batch [715520/756895]\n",
      "loss: 0.018792, accuracy: 93.045%, batch [716800/756895]\n",
      "loss: 0.018546, accuracy: 93.132%, batch [718080/756895]\n",
      "loss: 0.016785, accuracy: 93.265%, batch [719360/756895]\n",
      "loss: 0.014382, accuracy: 93.222%, batch [720640/756895]\n",
      "loss: 0.015092, accuracy: 93.149%, batch [721920/756895]\n",
      "loss: 0.015503, accuracy: 93.155%, batch [723200/756895]\n",
      "loss: 0.015372, accuracy: 93.262%, batch [724480/756895]\n",
      "loss: 0.018230, accuracy: 93.075%, batch [725760/756895]\n",
      "loss: 0.018472, accuracy: 93.069%, batch [727040/756895]\n",
      "loss: 0.015763, accuracy: 93.115%, batch [728320/756895]\n",
      "loss: 0.018624, accuracy: 92.962%, batch [729600/756895]\n",
      "loss: 0.016135, accuracy: 93.186%, batch [730880/756895]\n",
      "loss: 0.018415, accuracy: 93.092%, batch [732160/756895]\n",
      "loss: 0.020765, accuracy: 92.994%, batch [733440/756895]\n",
      "loss: 0.021670, accuracy: 93.008%, batch [734720/756895]\n",
      "loss: 0.020545, accuracy: 93.070%, batch [736000/756895]\n",
      "loss: 0.015979, accuracy: 93.053%, batch [737280/756895]\n",
      "loss: 0.015246, accuracy: 93.148%, batch [738560/756895]\n",
      "loss: 0.018639, accuracy: 93.211%, batch [739840/756895]\n",
      "loss: 0.018284, accuracy: 93.019%, batch [741120/756895]\n",
      "loss: 0.016875, accuracy: 93.121%, batch [742400/756895]\n",
      "loss: 0.019346, accuracy: 93.006%, batch [743680/756895]\n",
      "loss: 0.020816, accuracy: 93.003%, batch [744960/756895]\n",
      "loss: 0.019704, accuracy: 92.966%, batch [746240/756895]\n",
      "loss: 0.016425, accuracy: 93.227%, batch [747520/756895]\n",
      "loss: 0.020158, accuracy: 93.068%, batch [748800/756895]\n",
      "loss: 0.017987, accuracy: 93.030%, batch [750080/756895]\n",
      "loss: 0.020361, accuracy: 93.059%, batch [751360/756895]\n",
      "loss: 0.022538, accuracy: 92.868%, batch [752640/756895]\n",
      "loss: 0.017012, accuracy: 93.016%, batch [753920/756895]\n",
      "loss: 0.018036, accuracy: 93.056%, batch [755200/756895]\n",
      "loss: 0.016706, accuracy: 93.122%, batch [756480/756895]\n",
      "Test avg loss: 0.019355, test avg accuracy: 93.033% \n",
      "\n",
      "Test avg loss: 0.019323, test avg accuracy: 93.051% \n",
      "\n",
      "Epoch 107\n",
      "------------------------\n",
      "loss: 0.016285, accuracy: 93.147%, batch [    0/756895]\n",
      "loss: 0.025204, accuracy: 92.808%, batch [ 1280/756895]\n",
      "loss: 0.016447, accuracy: 93.161%, batch [ 2560/756895]\n",
      "loss: 0.019838, accuracy: 93.084%, batch [ 3840/756895]\n",
      "loss: 0.013883, accuracy: 93.357%, batch [ 5120/756895]\n",
      "loss: 0.019541, accuracy: 93.147%, batch [ 6400/756895]\n",
      "loss: 0.017806, accuracy: 92.985%, batch [ 7680/756895]\n",
      "loss: 0.014955, accuracy: 93.120%, batch [ 8960/756895]\n",
      "loss: 0.022976, accuracy: 93.278%, batch [10240/756895]\n",
      "loss: 0.019624, accuracy: 92.968%, batch [11520/756895]\n",
      "loss: 0.020602, accuracy: 92.988%, batch [12800/756895]\n",
      "loss: 0.016178, accuracy: 93.324%, batch [14080/756895]\n",
      "loss: 0.019409, accuracy: 93.093%, batch [15360/756895]\n",
      "loss: 0.017332, accuracy: 93.082%, batch [16640/756895]\n",
      "loss: 0.019438, accuracy: 93.149%, batch [17920/756895]\n",
      "loss: 0.019520, accuracy: 93.039%, batch [19200/756895]\n",
      "loss: 0.015368, accuracy: 93.163%, batch [20480/756895]\n",
      "loss: 0.018462, accuracy: 93.041%, batch [21760/756895]\n",
      "loss: 0.018001, accuracy: 93.045%, batch [23040/756895]\n",
      "loss: 0.018423, accuracy: 93.050%, batch [24320/756895]\n",
      "loss: 0.015213, accuracy: 93.228%, batch [25600/756895]\n",
      "loss: 0.023830, accuracy: 93.012%, batch [26880/756895]\n",
      "loss: 0.016135, accuracy: 93.032%, batch [28160/756895]\n",
      "loss: 0.018374, accuracy: 93.032%, batch [29440/756895]\n",
      "loss: 0.017838, accuracy: 93.056%, batch [30720/756895]\n",
      "loss: 0.020031, accuracy: 92.855%, batch [32000/756895]\n",
      "loss: 0.020605, accuracy: 93.071%, batch [33280/756895]\n",
      "loss: 0.019570, accuracy: 93.038%, batch [34560/756895]\n",
      "loss: 0.017042, accuracy: 93.069%, batch [35840/756895]\n",
      "loss: 0.021395, accuracy: 92.973%, batch [37120/756895]\n",
      "loss: 0.017664, accuracy: 93.239%, batch [38400/756895]\n",
      "loss: 0.017595, accuracy: 92.887%, batch [39680/756895]\n",
      "loss: 0.016349, accuracy: 93.139%, batch [40960/756895]\n",
      "loss: 0.015465, accuracy: 93.189%, batch [42240/756895]\n",
      "loss: 0.017116, accuracy: 93.130%, batch [43520/756895]\n",
      "loss: 0.019021, accuracy: 93.069%, batch [44800/756895]\n",
      "loss: 0.018181, accuracy: 93.291%, batch [46080/756895]\n",
      "loss: 0.019749, accuracy: 93.102%, batch [47360/756895]\n",
      "loss: 0.023858, accuracy: 92.843%, batch [48640/756895]\n",
      "loss: 0.018530, accuracy: 93.123%, batch [49920/756895]\n",
      "loss: 0.016763, accuracy: 93.118%, batch [51200/756895]\n",
      "loss: 0.018966, accuracy: 93.111%, batch [52480/756895]\n",
      "loss: 0.020237, accuracy: 92.845%, batch [53760/756895]\n",
      "loss: 0.020756, accuracy: 93.108%, batch [55040/756895]\n",
      "loss: 0.016317, accuracy: 93.276%, batch [56320/756895]\n",
      "loss: 0.017109, accuracy: 93.191%, batch [57600/756895]\n",
      "loss: 0.017914, accuracy: 92.936%, batch [58880/756895]\n",
      "loss: 0.019328, accuracy: 93.064%, batch [60160/756895]\n",
      "loss: 0.024505, accuracy: 92.895%, batch [61440/756895]\n",
      "loss: 0.016090, accuracy: 93.111%, batch [62720/756895]\n",
      "loss: 0.014859, accuracy: 93.274%, batch [64000/756895]\n",
      "loss: 0.016997, accuracy: 93.089%, batch [65280/756895]\n",
      "loss: 0.020635, accuracy: 92.989%, batch [66560/756895]\n",
      "loss: 0.018567, accuracy: 93.107%, batch [67840/756895]\n",
      "loss: 0.018795, accuracy: 93.186%, batch [69120/756895]\n",
      "loss: 0.019398, accuracy: 92.945%, batch [70400/756895]\n",
      "loss: 0.019038, accuracy: 93.151%, batch [71680/756895]\n",
      "loss: 0.017966, accuracy: 92.965%, batch [72960/756895]\n",
      "loss: 0.022736, accuracy: 93.191%, batch [74240/756895]\n",
      "loss: 0.019019, accuracy: 93.172%, batch [75520/756895]\n",
      "loss: 0.018728, accuracy: 92.968%, batch [76800/756895]\n",
      "loss: 0.020199, accuracy: 93.137%, batch [78080/756895]\n",
      "loss: 0.021396, accuracy: 93.122%, batch [79360/756895]\n",
      "loss: 0.016196, accuracy: 92.996%, batch [80640/756895]\n",
      "loss: 0.016496, accuracy: 93.138%, batch [81920/756895]\n",
      "loss: 0.014997, accuracy: 93.131%, batch [83200/756895]\n",
      "loss: 0.018547, accuracy: 93.183%, batch [84480/756895]\n",
      "loss: 0.017459, accuracy: 92.985%, batch [85760/756895]\n",
      "loss: 0.019225, accuracy: 93.017%, batch [87040/756895]\n",
      "loss: 0.022281, accuracy: 92.730%, batch [88320/756895]\n",
      "loss: 0.017764, accuracy: 92.977%, batch [89600/756895]\n",
      "loss: 0.029063, accuracy: 92.685%, batch [90880/756895]\n",
      "loss: 0.019817, accuracy: 93.015%, batch [92160/756895]\n",
      "loss: 0.018574, accuracy: 93.110%, batch [93440/756895]\n",
      "loss: 0.019496, accuracy: 92.964%, batch [94720/756895]\n",
      "loss: 0.019571, accuracy: 92.962%, batch [96000/756895]\n",
      "loss: 0.020338, accuracy: 93.073%, batch [97280/756895]\n",
      "loss: 0.016542, accuracy: 93.145%, batch [98560/756895]\n",
      "loss: 0.021651, accuracy: 92.987%, batch [99840/756895]\n",
      "loss: 0.019960, accuracy: 92.877%, batch [101120/756895]\n",
      "loss: 0.016597, accuracy: 93.219%, batch [102400/756895]\n",
      "loss: 0.013296, accuracy: 93.283%, batch [103680/756895]\n",
      "loss: 0.021498, accuracy: 93.066%, batch [104960/756895]\n",
      "loss: 0.023982, accuracy: 92.856%, batch [106240/756895]\n",
      "loss: 0.022445, accuracy: 92.783%, batch [107520/756895]\n",
      "loss: 0.016183, accuracy: 92.941%, batch [108800/756895]\n",
      "loss: 0.016769, accuracy: 93.120%, batch [110080/756895]\n",
      "loss: 0.017273, accuracy: 93.119%, batch [111360/756895]\n",
      "loss: 0.018342, accuracy: 93.045%, batch [112640/756895]\n",
      "loss: 0.019966, accuracy: 93.013%, batch [113920/756895]\n",
      "loss: 0.026164, accuracy: 92.980%, batch [115200/756895]\n",
      "loss: 0.019279, accuracy: 93.029%, batch [116480/756895]\n",
      "loss: 0.017365, accuracy: 93.154%, batch [117760/756895]\n",
      "loss: 0.016079, accuracy: 93.150%, batch [119040/756895]\n",
      "loss: 0.022002, accuracy: 93.012%, batch [120320/756895]\n",
      "loss: 0.018058, accuracy: 92.966%, batch [121600/756895]\n",
      "loss: 0.018561, accuracy: 93.134%, batch [122880/756895]\n",
      "loss: 0.016419, accuracy: 93.106%, batch [124160/756895]\n",
      "loss: 0.018227, accuracy: 93.072%, batch [125440/756895]\n",
      "loss: 0.020135, accuracy: 92.965%, batch [126720/756895]\n",
      "loss: 0.020853, accuracy: 93.017%, batch [128000/756895]\n",
      "loss: 0.019032, accuracy: 92.933%, batch [129280/756895]\n",
      "loss: 0.019902, accuracy: 93.135%, batch [130560/756895]\n",
      "loss: 0.017516, accuracy: 93.073%, batch [131840/756895]\n",
      "loss: 0.017667, accuracy: 92.985%, batch [133120/756895]\n",
      "loss: 0.015494, accuracy: 93.180%, batch [134400/756895]\n",
      "loss: 0.020139, accuracy: 93.022%, batch [135680/756895]\n",
      "loss: 0.021423, accuracy: 93.035%, batch [136960/756895]\n",
      "loss: 0.014222, accuracy: 93.230%, batch [138240/756895]\n",
      "loss: 0.017771, accuracy: 93.102%, batch [139520/756895]\n",
      "loss: 0.020859, accuracy: 92.928%, batch [140800/756895]\n",
      "loss: 0.020271, accuracy: 93.028%, batch [142080/756895]\n",
      "loss: 0.017591, accuracy: 93.167%, batch [143360/756895]\n",
      "loss: 0.017127, accuracy: 93.149%, batch [144640/756895]\n",
      "loss: 0.024310, accuracy: 92.713%, batch [145920/756895]\n",
      "loss: 0.025001, accuracy: 92.898%, batch [147200/756895]\n",
      "loss: 0.018669, accuracy: 92.921%, batch [148480/756895]\n",
      "loss: 0.021646, accuracy: 92.886%, batch [149760/756895]\n",
      "loss: 0.015956, accuracy: 93.032%, batch [151040/756895]\n",
      "loss: 0.023518, accuracy: 92.814%, batch [152320/756895]\n",
      "loss: 0.014358, accuracy: 93.250%, batch [153600/756895]\n",
      "loss: 0.024757, accuracy: 92.880%, batch [154880/756895]\n",
      "loss: 0.014931, accuracy: 93.391%, batch [156160/756895]\n",
      "loss: 0.016998, accuracy: 93.190%, batch [157440/756895]\n",
      "loss: 0.017379, accuracy: 93.016%, batch [158720/756895]\n",
      "loss: 0.015225, accuracy: 93.225%, batch [160000/756895]\n",
      "loss: 0.021086, accuracy: 93.058%, batch [161280/756895]\n",
      "loss: 0.015688, accuracy: 93.181%, batch [162560/756895]\n",
      "loss: 0.020463, accuracy: 92.948%, batch [163840/756895]\n",
      "loss: 0.019528, accuracy: 92.907%, batch [165120/756895]\n",
      "loss: 0.021976, accuracy: 93.012%, batch [166400/756895]\n",
      "loss: 0.019214, accuracy: 93.034%, batch [167680/756895]\n",
      "loss: 0.019573, accuracy: 92.819%, batch [168960/756895]\n",
      "loss: 0.021933, accuracy: 93.014%, batch [170240/756895]\n",
      "loss: 0.020000, accuracy: 92.911%, batch [171520/756895]\n",
      "loss: 0.016052, accuracy: 93.148%, batch [172800/756895]\n",
      "loss: 0.018793, accuracy: 93.144%, batch [174080/756895]\n",
      "loss: 0.015468, accuracy: 93.245%, batch [175360/756895]\n",
      "loss: 0.016668, accuracy: 93.082%, batch [176640/756895]\n",
      "loss: 0.018307, accuracy: 93.057%, batch [177920/756895]\n",
      "loss: 0.019879, accuracy: 92.934%, batch [179200/756895]\n",
      "loss: 0.026066, accuracy: 92.899%, batch [180480/756895]\n",
      "loss: 0.019999, accuracy: 93.072%, batch [181760/756895]\n",
      "loss: 0.018391, accuracy: 92.960%, batch [183040/756895]\n",
      "loss: 0.017267, accuracy: 93.301%, batch [184320/756895]\n",
      "loss: 0.014566, accuracy: 93.071%, batch [185600/756895]\n",
      "loss: 0.016854, accuracy: 93.059%, batch [186880/756895]\n",
      "loss: 0.016855, accuracy: 93.232%, batch [188160/756895]\n",
      "loss: 0.020191, accuracy: 92.889%, batch [189440/756895]\n",
      "loss: 0.017675, accuracy: 92.961%, batch [190720/756895]\n",
      "loss: 0.019379, accuracy: 92.911%, batch [192000/756895]\n",
      "loss: 0.020317, accuracy: 93.201%, batch [193280/756895]\n",
      "loss: 0.022266, accuracy: 92.925%, batch [194560/756895]\n",
      "loss: 0.018326, accuracy: 93.126%, batch [195840/756895]\n",
      "loss: 0.018272, accuracy: 93.151%, batch [197120/756895]\n",
      "loss: 0.017120, accuracy: 93.165%, batch [198400/756895]\n",
      "loss: 0.017107, accuracy: 93.083%, batch [199680/756895]\n",
      "loss: 0.019554, accuracy: 93.098%, batch [200960/756895]\n",
      "loss: 0.022878, accuracy: 92.759%, batch [202240/756895]\n",
      "loss: 0.017145, accuracy: 93.057%, batch [203520/756895]\n",
      "loss: 0.015791, accuracy: 93.232%, batch [204800/756895]\n",
      "loss: 0.014819, accuracy: 93.258%, batch [206080/756895]\n",
      "loss: 0.019679, accuracy: 93.047%, batch [207360/756895]\n",
      "loss: 0.018398, accuracy: 92.878%, batch [208640/756895]\n",
      "loss: 0.014827, accuracy: 93.119%, batch [209920/756895]\n",
      "loss: 0.017286, accuracy: 93.145%, batch [211200/756895]\n",
      "loss: 0.017127, accuracy: 93.064%, batch [212480/756895]\n",
      "loss: 0.020892, accuracy: 93.001%, batch [213760/756895]\n",
      "loss: 0.018824, accuracy: 93.254%, batch [215040/756895]\n",
      "loss: 0.021747, accuracy: 93.032%, batch [216320/756895]\n",
      "loss: 0.015294, accuracy: 93.194%, batch [217600/756895]\n",
      "loss: 0.020896, accuracy: 93.046%, batch [218880/756895]\n",
      "loss: 0.017605, accuracy: 93.042%, batch [220160/756895]\n",
      "loss: 0.023759, accuracy: 92.903%, batch [221440/756895]\n",
      "loss: 0.022271, accuracy: 93.122%, batch [222720/756895]\n",
      "loss: 0.021225, accuracy: 93.085%, batch [224000/756895]\n",
      "loss: 0.019269, accuracy: 93.256%, batch [225280/756895]\n",
      "loss: 0.025325, accuracy: 92.858%, batch [226560/756895]\n",
      "loss: 0.024271, accuracy: 92.944%, batch [227840/756895]\n",
      "loss: 0.019365, accuracy: 92.830%, batch [229120/756895]\n",
      "loss: 0.017186, accuracy: 93.154%, batch [230400/756895]\n",
      "loss: 0.016435, accuracy: 93.064%, batch [231680/756895]\n",
      "loss: 0.015626, accuracy: 93.072%, batch [232960/756895]\n",
      "loss: 0.017460, accuracy: 93.147%, batch [234240/756895]\n",
      "loss: 0.017660, accuracy: 92.981%, batch [235520/756895]\n",
      "loss: 0.019601, accuracy: 93.208%, batch [236800/756895]\n",
      "loss: 0.018984, accuracy: 92.878%, batch [238080/756895]\n",
      "loss: 0.019755, accuracy: 93.014%, batch [239360/756895]\n",
      "loss: 0.024907, accuracy: 92.695%, batch [240640/756895]\n",
      "loss: 0.021591, accuracy: 92.933%, batch [241920/756895]\n",
      "loss: 0.017759, accuracy: 92.992%, batch [243200/756895]\n",
      "loss: 0.020940, accuracy: 92.935%, batch [244480/756895]\n",
      "loss: 0.019872, accuracy: 93.215%, batch [245760/756895]\n",
      "loss: 0.017185, accuracy: 93.165%, batch [247040/756895]\n",
      "loss: 0.022798, accuracy: 92.997%, batch [248320/756895]\n",
      "loss: 0.018693, accuracy: 93.100%, batch [249600/756895]\n",
      "loss: 0.020109, accuracy: 93.032%, batch [250880/756895]\n",
      "loss: 0.020982, accuracy: 93.040%, batch [252160/756895]\n",
      "loss: 0.019693, accuracy: 92.945%, batch [253440/756895]\n",
      "loss: 0.016582, accuracy: 93.131%, batch [254720/756895]\n",
      "loss: 0.018719, accuracy: 92.857%, batch [256000/756895]\n",
      "loss: 0.016509, accuracy: 93.250%, batch [257280/756895]\n",
      "loss: 0.015065, accuracy: 93.110%, batch [258560/756895]\n",
      "loss: 0.013762, accuracy: 93.267%, batch [259840/756895]\n",
      "loss: 0.017314, accuracy: 93.149%, batch [261120/756895]\n",
      "loss: 0.020219, accuracy: 92.979%, batch [262400/756895]\n",
      "loss: 0.020379, accuracy: 92.893%, batch [263680/756895]\n",
      "loss: 0.023800, accuracy: 92.976%, batch [264960/756895]\n",
      "loss: 0.019238, accuracy: 93.035%, batch [266240/756895]\n",
      "loss: 0.018536, accuracy: 93.032%, batch [267520/756895]\n",
      "loss: 0.014543, accuracy: 93.233%, batch [268800/756895]\n",
      "loss: 0.018208, accuracy: 93.127%, batch [270080/756895]\n",
      "loss: 0.021343, accuracy: 93.073%, batch [271360/756895]\n",
      "loss: 0.022996, accuracy: 93.096%, batch [272640/756895]\n",
      "loss: 0.017799, accuracy: 93.110%, batch [273920/756895]\n",
      "loss: 0.018691, accuracy: 92.998%, batch [275200/756895]\n",
      "loss: 0.018964, accuracy: 93.062%, batch [276480/756895]\n",
      "loss: 0.021006, accuracy: 93.000%, batch [277760/756895]\n",
      "loss: 0.013041, accuracy: 93.265%, batch [279040/756895]\n",
      "loss: 0.019921, accuracy: 93.123%, batch [280320/756895]\n",
      "loss: 0.020012, accuracy: 92.776%, batch [281600/756895]\n",
      "loss: 0.017181, accuracy: 93.232%, batch [282880/756895]\n",
      "loss: 0.017663, accuracy: 93.157%, batch [284160/756895]\n",
      "loss: 0.017286, accuracy: 93.115%, batch [285440/756895]\n",
      "loss: 0.017337, accuracy: 93.180%, batch [286720/756895]\n",
      "loss: 0.018658, accuracy: 92.994%, batch [288000/756895]\n",
      "loss: 0.014991, accuracy: 93.081%, batch [289280/756895]\n",
      "loss: 0.025937, accuracy: 92.987%, batch [290560/756895]\n",
      "loss: 0.014770, accuracy: 93.138%, batch [291840/756895]\n",
      "loss: 0.017984, accuracy: 92.969%, batch [293120/756895]\n",
      "loss: 0.020265, accuracy: 93.058%, batch [294400/756895]\n",
      "loss: 0.019905, accuracy: 93.053%, batch [295680/756895]\n",
      "loss: 0.014944, accuracy: 93.150%, batch [296960/756895]\n",
      "loss: 0.014684, accuracy: 93.037%, batch [298240/756895]\n",
      "loss: 0.015454, accuracy: 93.162%, batch [299520/756895]\n",
      "loss: 0.019342, accuracy: 92.984%, batch [300800/756895]\n",
      "loss: 0.019796, accuracy: 93.028%, batch [302080/756895]\n",
      "loss: 0.014275, accuracy: 93.335%, batch [303360/756895]\n",
      "loss: 0.015894, accuracy: 93.130%, batch [304640/756895]\n",
      "loss: 0.015256, accuracy: 93.216%, batch [305920/756895]\n",
      "loss: 0.018775, accuracy: 93.060%, batch [307200/756895]\n",
      "loss: 0.018514, accuracy: 93.154%, batch [308480/756895]\n",
      "loss: 0.025959, accuracy: 92.800%, batch [309760/756895]\n",
      "loss: 0.018954, accuracy: 93.037%, batch [311040/756895]\n",
      "loss: 0.022335, accuracy: 92.784%, batch [312320/756895]\n",
      "loss: 0.019300, accuracy: 92.958%, batch [313600/756895]\n",
      "loss: 0.018410, accuracy: 93.136%, batch [314880/756895]\n",
      "loss: 0.019820, accuracy: 93.096%, batch [316160/756895]\n",
      "loss: 0.016992, accuracy: 93.124%, batch [317440/756895]\n",
      "loss: 0.017488, accuracy: 92.998%, batch [318720/756895]\n",
      "loss: 0.023918, accuracy: 92.879%, batch [320000/756895]\n",
      "loss: 0.016663, accuracy: 93.146%, batch [321280/756895]\n",
      "loss: 0.017009, accuracy: 93.114%, batch [322560/756895]\n",
      "loss: 0.017802, accuracy: 92.934%, batch [323840/756895]\n",
      "loss: 0.018233, accuracy: 93.167%, batch [325120/756895]\n",
      "loss: 0.018470, accuracy: 92.992%, batch [326400/756895]\n",
      "loss: 0.016494, accuracy: 93.174%, batch [327680/756895]\n",
      "loss: 0.016857, accuracy: 93.046%, batch [328960/756895]\n",
      "loss: 0.016461, accuracy: 93.098%, batch [330240/756895]\n",
      "loss: 0.018873, accuracy: 93.147%, batch [331520/756895]\n",
      "loss: 0.017253, accuracy: 93.139%, batch [332800/756895]\n",
      "loss: 0.022653, accuracy: 93.112%, batch [334080/756895]\n",
      "loss: 0.023563, accuracy: 92.938%, batch [335360/756895]\n",
      "loss: 0.014410, accuracy: 93.390%, batch [336640/756895]\n",
      "loss: 0.017566, accuracy: 93.086%, batch [337920/756895]\n",
      "loss: 0.018550, accuracy: 93.007%, batch [339200/756895]\n",
      "loss: 0.018416, accuracy: 93.090%, batch [340480/756895]\n",
      "loss: 0.018304, accuracy: 93.138%, batch [341760/756895]\n",
      "loss: 0.020690, accuracy: 93.181%, batch [343040/756895]\n",
      "loss: 0.020012, accuracy: 92.892%, batch [344320/756895]\n",
      "loss: 0.019401, accuracy: 92.987%, batch [345600/756895]\n",
      "loss: 0.016708, accuracy: 93.151%, batch [346880/756895]\n",
      "loss: 0.019728, accuracy: 93.176%, batch [348160/756895]\n",
      "loss: 0.023652, accuracy: 92.746%, batch [349440/756895]\n",
      "loss: 0.020157, accuracy: 93.050%, batch [350720/756895]\n",
      "loss: 0.021453, accuracy: 92.878%, batch [352000/756895]\n",
      "loss: 0.023892, accuracy: 92.934%, batch [353280/756895]\n",
      "loss: 0.017406, accuracy: 93.059%, batch [354560/756895]\n",
      "loss: 0.018854, accuracy: 93.084%, batch [355840/756895]\n",
      "loss: 0.018027, accuracy: 93.073%, batch [357120/756895]\n",
      "loss: 0.023019, accuracy: 92.895%, batch [358400/756895]\n",
      "loss: 0.023221, accuracy: 92.926%, batch [359680/756895]\n",
      "loss: 0.015966, accuracy: 93.097%, batch [360960/756895]\n",
      "loss: 0.018448, accuracy: 93.082%, batch [362240/756895]\n",
      "loss: 0.017317, accuracy: 93.093%, batch [363520/756895]\n",
      "loss: 0.017622, accuracy: 93.056%, batch [364800/756895]\n",
      "loss: 0.024466, accuracy: 92.861%, batch [366080/756895]\n",
      "loss: 0.017301, accuracy: 93.124%, batch [367360/756895]\n",
      "loss: 0.016845, accuracy: 93.133%, batch [368640/756895]\n",
      "loss: 0.016809, accuracy: 93.131%, batch [369920/756895]\n",
      "loss: 0.017986, accuracy: 93.027%, batch [371200/756895]\n",
      "loss: 0.018445, accuracy: 93.232%, batch [372480/756895]\n",
      "loss: 0.019503, accuracy: 93.091%, batch [373760/756895]\n",
      "loss: 0.018987, accuracy: 92.970%, batch [375040/756895]\n",
      "loss: 0.020114, accuracy: 93.283%, batch [376320/756895]\n",
      "loss: 0.017207, accuracy: 92.938%, batch [377600/756895]\n",
      "loss: 0.023954, accuracy: 92.990%, batch [378880/756895]\n",
      "loss: 0.018695, accuracy: 93.076%, batch [380160/756895]\n",
      "loss: 0.020628, accuracy: 92.998%, batch [381440/756895]\n",
      "loss: 0.018543, accuracy: 92.929%, batch [382720/756895]\n",
      "loss: 0.028635, accuracy: 92.764%, batch [384000/756895]\n",
      "loss: 0.016271, accuracy: 93.163%, batch [385280/756895]\n",
      "loss: 0.015428, accuracy: 93.176%, batch [386560/756895]\n",
      "loss: 0.017970, accuracy: 92.952%, batch [387840/756895]\n",
      "loss: 0.014117, accuracy: 93.280%, batch [389120/756895]\n",
      "loss: 0.021335, accuracy: 92.952%, batch [390400/756895]\n",
      "loss: 0.023179, accuracy: 92.932%, batch [391680/756895]\n",
      "loss: 0.017502, accuracy: 93.075%, batch [392960/756895]\n",
      "loss: 0.019664, accuracy: 92.998%, batch [394240/756895]\n",
      "loss: 0.018578, accuracy: 93.041%, batch [395520/756895]\n",
      "loss: 0.027736, accuracy: 92.739%, batch [396800/756895]\n",
      "loss: 0.019566, accuracy: 92.966%, batch [398080/756895]\n",
      "loss: 0.020877, accuracy: 93.000%, batch [399360/756895]\n",
      "loss: 0.019163, accuracy: 92.921%, batch [400640/756895]\n",
      "loss: 0.016151, accuracy: 93.238%, batch [401920/756895]\n",
      "loss: 0.022225, accuracy: 92.968%, batch [403200/756895]\n",
      "loss: 0.020694, accuracy: 92.912%, batch [404480/756895]\n",
      "loss: 0.015439, accuracy: 93.289%, batch [405760/756895]\n",
      "loss: 0.017796, accuracy: 93.058%, batch [407040/756895]\n",
      "loss: 0.018631, accuracy: 93.204%, batch [408320/756895]\n",
      "loss: 0.015581, accuracy: 93.147%, batch [409600/756895]\n",
      "loss: 0.016156, accuracy: 93.047%, batch [410880/756895]\n",
      "loss: 0.017583, accuracy: 93.066%, batch [412160/756895]\n",
      "loss: 0.017086, accuracy: 93.110%, batch [413440/756895]\n",
      "loss: 0.020434, accuracy: 93.044%, batch [414720/756895]\n",
      "loss: 0.017607, accuracy: 93.124%, batch [416000/756895]\n",
      "loss: 0.017559, accuracy: 93.223%, batch [417280/756895]\n",
      "loss: 0.018544, accuracy: 93.206%, batch [418560/756895]\n",
      "loss: 0.020039, accuracy: 93.028%, batch [419840/756895]\n",
      "loss: 0.021530, accuracy: 93.038%, batch [421120/756895]\n",
      "loss: 0.014677, accuracy: 93.189%, batch [422400/756895]\n",
      "loss: 0.023852, accuracy: 92.848%, batch [423680/756895]\n",
      "loss: 0.020319, accuracy: 92.973%, batch [424960/756895]\n",
      "loss: 0.018293, accuracy: 93.099%, batch [426240/756895]\n",
      "loss: 0.018847, accuracy: 93.044%, batch [427520/756895]\n",
      "loss: 0.019944, accuracy: 92.964%, batch [428800/756895]\n",
      "loss: 0.017369, accuracy: 93.045%, batch [430080/756895]\n",
      "loss: 0.014698, accuracy: 93.220%, batch [431360/756895]\n",
      "loss: 0.015943, accuracy: 93.202%, batch [432640/756895]\n",
      "loss: 0.019054, accuracy: 92.917%, batch [433920/756895]\n",
      "loss: 0.017970, accuracy: 93.047%, batch [435200/756895]\n",
      "loss: 0.015485, accuracy: 93.211%, batch [436480/756895]\n",
      "loss: 0.020182, accuracy: 92.999%, batch [437760/756895]\n",
      "loss: 0.015545, accuracy: 93.179%, batch [439040/756895]\n",
      "loss: 0.016061, accuracy: 93.258%, batch [440320/756895]\n",
      "loss: 0.020072, accuracy: 93.062%, batch [441600/756895]\n",
      "loss: 0.023110, accuracy: 93.096%, batch [442880/756895]\n",
      "loss: 0.017043, accuracy: 93.100%, batch [444160/756895]\n",
      "loss: 0.019670, accuracy: 93.135%, batch [445440/756895]\n",
      "loss: 0.015215, accuracy: 93.293%, batch [446720/756895]\n",
      "loss: 0.019060, accuracy: 92.956%, batch [448000/756895]\n",
      "loss: 0.019121, accuracy: 93.086%, batch [449280/756895]\n",
      "loss: 0.017671, accuracy: 93.204%, batch [450560/756895]\n",
      "loss: 0.021816, accuracy: 93.045%, batch [451840/756895]\n",
      "loss: 0.019332, accuracy: 93.046%, batch [453120/756895]\n",
      "loss: 0.017392, accuracy: 93.059%, batch [454400/756895]\n",
      "loss: 0.017524, accuracy: 93.154%, batch [455680/756895]\n",
      "loss: 0.015601, accuracy: 93.299%, batch [456960/756895]\n",
      "loss: 0.020380, accuracy: 92.975%, batch [458240/756895]\n",
      "loss: 0.020289, accuracy: 92.940%, batch [459520/756895]\n",
      "loss: 0.020113, accuracy: 93.180%, batch [460800/756895]\n",
      "loss: 0.013164, accuracy: 93.220%, batch [462080/756895]\n",
      "loss: 0.016212, accuracy: 93.336%, batch [463360/756895]\n",
      "loss: 0.023847, accuracy: 92.931%, batch [464640/756895]\n",
      "loss: 0.017178, accuracy: 93.109%, batch [465920/756895]\n",
      "loss: 0.020412, accuracy: 92.999%, batch [467200/756895]\n",
      "loss: 0.020885, accuracy: 92.929%, batch [468480/756895]\n",
      "loss: 0.018558, accuracy: 93.192%, batch [469760/756895]\n",
      "loss: 0.017301, accuracy: 93.087%, batch [471040/756895]\n",
      "loss: 0.016486, accuracy: 93.055%, batch [472320/756895]\n",
      "loss: 0.018737, accuracy: 93.055%, batch [473600/756895]\n",
      "loss: 0.019655, accuracy: 92.945%, batch [474880/756895]\n",
      "loss: 0.017143, accuracy: 93.089%, batch [476160/756895]\n",
      "loss: 0.017423, accuracy: 93.142%, batch [477440/756895]\n",
      "loss: 0.017607, accuracy: 93.219%, batch [478720/756895]\n",
      "loss: 0.019961, accuracy: 92.921%, batch [480000/756895]\n",
      "loss: 0.019387, accuracy: 93.000%, batch [481280/756895]\n",
      "loss: 0.019887, accuracy: 92.791%, batch [482560/756895]\n",
      "loss: 0.019461, accuracy: 93.075%, batch [483840/756895]\n",
      "loss: 0.019575, accuracy: 93.031%, batch [485120/756895]\n",
      "loss: 0.021214, accuracy: 92.813%, batch [486400/756895]\n",
      "loss: 0.022309, accuracy: 92.891%, batch [487680/756895]\n",
      "loss: 0.019871, accuracy: 92.898%, batch [488960/756895]\n",
      "loss: 0.020316, accuracy: 93.133%, batch [490240/756895]\n",
      "loss: 0.017078, accuracy: 93.113%, batch [491520/756895]\n",
      "loss: 0.018162, accuracy: 93.037%, batch [492800/756895]\n",
      "loss: 0.020339, accuracy: 92.936%, batch [494080/756895]\n",
      "loss: 0.019277, accuracy: 92.968%, batch [495360/756895]\n",
      "loss: 0.017469, accuracy: 93.115%, batch [496640/756895]\n",
      "loss: 0.018015, accuracy: 93.154%, batch [497920/756895]\n",
      "loss: 0.022303, accuracy: 92.983%, batch [499200/756895]\n",
      "loss: 0.019895, accuracy: 92.790%, batch [500480/756895]\n",
      "loss: 0.018266, accuracy: 93.053%, batch [501760/756895]\n",
      "loss: 0.019598, accuracy: 92.949%, batch [503040/756895]\n",
      "loss: 0.014775, accuracy: 93.094%, batch [504320/756895]\n",
      "loss: 0.021776, accuracy: 93.017%, batch [505600/756895]\n",
      "loss: 0.018371, accuracy: 93.002%, batch [506880/756895]\n",
      "loss: 0.019215, accuracy: 93.142%, batch [508160/756895]\n",
      "loss: 0.024619, accuracy: 92.718%, batch [509440/756895]\n",
      "loss: 0.019111, accuracy: 93.124%, batch [510720/756895]\n",
      "loss: 0.024111, accuracy: 92.889%, batch [512000/756895]\n",
      "loss: 0.016078, accuracy: 93.104%, batch [513280/756895]\n",
      "loss: 0.019017, accuracy: 93.051%, batch [514560/756895]\n",
      "loss: 0.015872, accuracy: 93.126%, batch [515840/756895]\n",
      "loss: 0.016142, accuracy: 93.181%, batch [517120/756895]\n",
      "loss: 0.018230, accuracy: 93.074%, batch [518400/756895]\n",
      "loss: 0.017604, accuracy: 93.132%, batch [519680/756895]\n",
      "loss: 0.016737, accuracy: 93.167%, batch [520960/756895]\n",
      "loss: 0.017256, accuracy: 93.034%, batch [522240/756895]\n",
      "loss: 0.018743, accuracy: 93.041%, batch [523520/756895]\n",
      "loss: 0.019704, accuracy: 92.961%, batch [524800/756895]\n",
      "loss: 0.020611, accuracy: 92.999%, batch [526080/756895]\n",
      "loss: 0.022763, accuracy: 92.745%, batch [527360/756895]\n",
      "loss: 0.020231, accuracy: 93.090%, batch [528640/756895]\n",
      "loss: 0.022982, accuracy: 93.065%, batch [529920/756895]\n",
      "loss: 0.020040, accuracy: 93.013%, batch [531200/756895]\n",
      "loss: 0.018884, accuracy: 93.118%, batch [532480/756895]\n",
      "loss: 0.020480, accuracy: 92.960%, batch [533760/756895]\n",
      "loss: 0.015644, accuracy: 92.959%, batch [535040/756895]\n",
      "loss: 0.015472, accuracy: 93.156%, batch [536320/756895]\n",
      "loss: 0.014509, accuracy: 93.274%, batch [537600/756895]\n",
      "loss: 0.022284, accuracy: 92.918%, batch [538880/756895]\n",
      "loss: 0.015783, accuracy: 93.172%, batch [540160/756895]\n",
      "loss: 0.020267, accuracy: 92.910%, batch [541440/756895]\n",
      "loss: 0.023525, accuracy: 92.779%, batch [542720/756895]\n",
      "loss: 0.017334, accuracy: 93.225%, batch [544000/756895]\n",
      "loss: 0.016646, accuracy: 93.094%, batch [545280/756895]\n",
      "loss: 0.018548, accuracy: 93.007%, batch [546560/756895]\n",
      "loss: 0.022231, accuracy: 92.997%, batch [547840/756895]\n",
      "loss: 0.016502, accuracy: 93.207%, batch [549120/756895]\n",
      "loss: 0.018598, accuracy: 92.977%, batch [550400/756895]\n",
      "loss: 0.016388, accuracy: 93.267%, batch [551680/756895]\n",
      "loss: 0.016590, accuracy: 93.263%, batch [552960/756895]\n",
      "loss: 0.018300, accuracy: 93.046%, batch [554240/756895]\n",
      "loss: 0.022072, accuracy: 92.971%, batch [555520/756895]\n",
      "loss: 0.018358, accuracy: 93.206%, batch [556800/756895]\n",
      "loss: 0.024570, accuracy: 92.658%, batch [558080/756895]\n",
      "loss: 0.016754, accuracy: 93.220%, batch [559360/756895]\n",
      "loss: 0.016384, accuracy: 92.982%, batch [560640/756895]\n",
      "loss: 0.015193, accuracy: 93.319%, batch [561920/756895]\n",
      "loss: 0.019300, accuracy: 93.007%, batch [563200/756895]\n",
      "loss: 0.018161, accuracy: 93.094%, batch [564480/756895]\n",
      "loss: 0.017596, accuracy: 93.115%, batch [565760/756895]\n",
      "loss: 0.018190, accuracy: 93.083%, batch [567040/756895]\n",
      "loss: 0.016679, accuracy: 93.093%, batch [568320/756895]\n",
      "loss: 0.017485, accuracy: 93.183%, batch [569600/756895]\n",
      "loss: 0.017740, accuracy: 93.167%, batch [570880/756895]\n",
      "loss: 0.017954, accuracy: 93.146%, batch [572160/756895]\n",
      "loss: 0.016512, accuracy: 93.055%, batch [573440/756895]\n",
      "loss: 0.022313, accuracy: 92.821%, batch [574720/756895]\n",
      "loss: 0.015609, accuracy: 93.111%, batch [576000/756895]\n",
      "loss: 0.018315, accuracy: 93.055%, batch [577280/756895]\n",
      "loss: 0.018075, accuracy: 93.130%, batch [578560/756895]\n",
      "loss: 0.017535, accuracy: 93.065%, batch [579840/756895]\n",
      "loss: 0.027073, accuracy: 92.687%, batch [581120/756895]\n",
      "loss: 0.017905, accuracy: 93.173%, batch [582400/756895]\n",
      "loss: 0.015954, accuracy: 93.059%, batch [583680/756895]\n",
      "loss: 0.016213, accuracy: 93.155%, batch [584960/756895]\n",
      "loss: 0.016352, accuracy: 93.169%, batch [586240/756895]\n",
      "loss: 0.021039, accuracy: 93.034%, batch [587520/756895]\n",
      "loss: 0.015988, accuracy: 93.155%, batch [588800/756895]\n",
      "loss: 0.015246, accuracy: 93.246%, batch [590080/756895]\n",
      "loss: 0.019125, accuracy: 93.060%, batch [591360/756895]\n",
      "loss: 0.016714, accuracy: 92.994%, batch [592640/756895]\n",
      "loss: 0.016305, accuracy: 93.211%, batch [593920/756895]\n",
      "loss: 0.015143, accuracy: 93.207%, batch [595200/756895]\n",
      "loss: 0.018683, accuracy: 92.971%, batch [596480/756895]\n",
      "loss: 0.021201, accuracy: 93.017%, batch [597760/756895]\n",
      "loss: 0.024831, accuracy: 93.161%, batch [599040/756895]\n",
      "loss: 0.026063, accuracy: 92.854%, batch [600320/756895]\n",
      "loss: 0.017380, accuracy: 93.224%, batch [601600/756895]\n",
      "loss: 0.021099, accuracy: 92.949%, batch [602880/756895]\n",
      "loss: 0.017769, accuracy: 93.220%, batch [604160/756895]\n",
      "loss: 0.023582, accuracy: 92.867%, batch [605440/756895]\n",
      "loss: 0.016795, accuracy: 93.158%, batch [606720/756895]\n",
      "loss: 0.019738, accuracy: 92.976%, batch [608000/756895]\n",
      "loss: 0.021177, accuracy: 93.021%, batch [609280/756895]\n",
      "loss: 0.020323, accuracy: 92.927%, batch [610560/756895]\n",
      "loss: 0.023179, accuracy: 92.976%, batch [611840/756895]\n",
      "loss: 0.017445, accuracy: 93.170%, batch [613120/756895]\n",
      "loss: 0.023244, accuracy: 92.956%, batch [614400/756895]\n",
      "loss: 0.023882, accuracy: 92.831%, batch [615680/756895]\n",
      "loss: 0.020788, accuracy: 92.986%, batch [616960/756895]\n",
      "loss: 0.016130, accuracy: 93.064%, batch [618240/756895]\n",
      "loss: 0.016529, accuracy: 93.167%, batch [619520/756895]\n",
      "loss: 0.018162, accuracy: 93.141%, batch [620800/756895]\n",
      "loss: 0.016543, accuracy: 92.999%, batch [622080/756895]\n",
      "loss: 0.018513, accuracy: 92.880%, batch [623360/756895]\n",
      "loss: 0.024224, accuracy: 92.902%, batch [624640/756895]\n",
      "loss: 0.015486, accuracy: 93.079%, batch [625920/756895]\n",
      "loss: 0.018395, accuracy: 93.116%, batch [627200/756895]\n",
      "loss: 0.019441, accuracy: 93.200%, batch [628480/756895]\n",
      "loss: 0.018967, accuracy: 93.200%, batch [629760/756895]\n",
      "loss: 0.022552, accuracy: 92.944%, batch [631040/756895]\n",
      "loss: 0.016752, accuracy: 92.993%, batch [632320/756895]\n",
      "loss: 0.020585, accuracy: 92.976%, batch [633600/756895]\n",
      "loss: 0.016980, accuracy: 93.127%, batch [634880/756895]\n",
      "loss: 0.020892, accuracy: 93.022%, batch [636160/756895]\n",
      "loss: 0.016621, accuracy: 93.122%, batch [637440/756895]\n",
      "loss: 0.015564, accuracy: 93.261%, batch [638720/756895]\n",
      "loss: 0.019870, accuracy: 93.134%, batch [640000/756895]\n",
      "loss: 0.016680, accuracy: 93.056%, batch [641280/756895]\n",
      "loss: 0.018986, accuracy: 93.083%, batch [642560/756895]\n",
      "loss: 0.015750, accuracy: 93.264%, batch [643840/756895]\n",
      "loss: 0.021348, accuracy: 92.992%, batch [645120/756895]\n",
      "loss: 0.025560, accuracy: 92.983%, batch [646400/756895]\n",
      "loss: 0.015582, accuracy: 93.234%, batch [647680/756895]\n",
      "loss: 0.016226, accuracy: 93.013%, batch [648960/756895]\n",
      "loss: 0.022113, accuracy: 92.808%, batch [650240/756895]\n",
      "loss: 0.019244, accuracy: 93.063%, batch [651520/756895]\n",
      "loss: 0.015532, accuracy: 93.407%, batch [652800/756895]\n",
      "loss: 0.021581, accuracy: 92.881%, batch [654080/756895]\n",
      "loss: 0.016375, accuracy: 93.107%, batch [655360/756895]\n",
      "loss: 0.016622, accuracy: 93.146%, batch [656640/756895]\n",
      "loss: 0.019776, accuracy: 93.093%, batch [657920/756895]\n",
      "loss: 0.015634, accuracy: 93.199%, batch [659200/756895]\n",
      "loss: 0.017367, accuracy: 92.985%, batch [660480/756895]\n",
      "loss: 0.016698, accuracy: 93.094%, batch [661760/756895]\n",
      "loss: 0.015193, accuracy: 93.265%, batch [663040/756895]\n",
      "loss: 0.016739, accuracy: 93.103%, batch [664320/756895]\n",
      "loss: 0.021936, accuracy: 93.082%, batch [665600/756895]\n",
      "loss: 0.017927, accuracy: 92.980%, batch [666880/756895]\n",
      "loss: 0.015869, accuracy: 93.149%, batch [668160/756895]\n",
      "loss: 0.016310, accuracy: 93.204%, batch [669440/756895]\n",
      "loss: 0.018289, accuracy: 93.248%, batch [670720/756895]\n",
      "loss: 0.019971, accuracy: 93.087%, batch [672000/756895]\n",
      "loss: 0.018673, accuracy: 93.017%, batch [673280/756895]\n",
      "loss: 0.015540, accuracy: 93.291%, batch [674560/756895]\n",
      "loss: 0.018980, accuracy: 92.986%, batch [675840/756895]\n",
      "loss: 0.018417, accuracy: 93.080%, batch [677120/756895]\n",
      "loss: 0.016365, accuracy: 93.155%, batch [678400/756895]\n",
      "loss: 0.016916, accuracy: 93.162%, batch [679680/756895]\n",
      "loss: 0.020006, accuracy: 92.925%, batch [680960/756895]\n",
      "loss: 0.015318, accuracy: 93.339%, batch [682240/756895]\n",
      "loss: 0.018914, accuracy: 93.135%, batch [683520/756895]\n",
      "loss: 0.021205, accuracy: 93.157%, batch [684800/756895]\n",
      "loss: 0.021717, accuracy: 92.856%, batch [686080/756895]\n",
      "loss: 0.022614, accuracy: 92.715%, batch [687360/756895]\n",
      "loss: 0.017887, accuracy: 93.113%, batch [688640/756895]\n",
      "loss: 0.019389, accuracy: 93.059%, batch [689920/756895]\n",
      "loss: 0.020918, accuracy: 92.932%, batch [691200/756895]\n",
      "loss: 0.020175, accuracy: 93.121%, batch [692480/756895]\n",
      "loss: 0.015487, accuracy: 93.283%, batch [693760/756895]\n",
      "loss: 0.019625, accuracy: 93.049%, batch [695040/756895]\n",
      "loss: 0.013839, accuracy: 93.333%, batch [696320/756895]\n",
      "loss: 0.019076, accuracy: 92.961%, batch [697600/756895]\n",
      "loss: 0.023787, accuracy: 92.920%, batch [698880/756895]\n",
      "loss: 0.020164, accuracy: 92.797%, batch [700160/756895]\n",
      "loss: 0.021696, accuracy: 92.919%, batch [701440/756895]\n",
      "loss: 0.016421, accuracy: 93.115%, batch [702720/756895]\n",
      "loss: 0.018104, accuracy: 92.946%, batch [704000/756895]\n",
      "loss: 0.020843, accuracy: 92.962%, batch [705280/756895]\n",
      "loss: 0.022170, accuracy: 92.878%, batch [706560/756895]\n",
      "loss: 0.019559, accuracy: 92.961%, batch [707840/756895]\n",
      "loss: 0.024724, accuracy: 92.862%, batch [709120/756895]\n",
      "loss: 0.021256, accuracy: 93.012%, batch [710400/756895]\n",
      "loss: 0.016256, accuracy: 93.174%, batch [711680/756895]\n",
      "loss: 0.022293, accuracy: 92.946%, batch [712960/756895]\n",
      "loss: 0.016540, accuracy: 93.153%, batch [714240/756895]\n",
      "loss: 0.024648, accuracy: 92.955%, batch [715520/756895]\n",
      "loss: 0.017668, accuracy: 93.005%, batch [716800/756895]\n",
      "loss: 0.021297, accuracy: 92.733%, batch [718080/756895]\n",
      "loss: 0.018844, accuracy: 93.120%, batch [719360/756895]\n",
      "loss: 0.021681, accuracy: 92.898%, batch [720640/756895]\n",
      "loss: 0.020660, accuracy: 92.897%, batch [721920/756895]\n",
      "loss: 0.018854, accuracy: 92.943%, batch [723200/756895]\n",
      "loss: 0.017813, accuracy: 93.028%, batch [724480/756895]\n",
      "loss: 0.022268, accuracy: 92.887%, batch [725760/756895]\n",
      "loss: 0.020499, accuracy: 93.025%, batch [727040/756895]\n",
      "loss: 0.016919, accuracy: 93.097%, batch [728320/756895]\n",
      "loss: 0.016352, accuracy: 93.235%, batch [729600/756895]\n",
      "loss: 0.016512, accuracy: 93.195%, batch [730880/756895]\n",
      "loss: 0.018027, accuracy: 93.025%, batch [732160/756895]\n",
      "loss: 0.022531, accuracy: 92.975%, batch [733440/756895]\n",
      "loss: 0.017634, accuracy: 93.048%, batch [734720/756895]\n",
      "loss: 0.017236, accuracy: 93.129%, batch [736000/756895]\n",
      "loss: 0.028068, accuracy: 92.785%, batch [737280/756895]\n",
      "loss: 0.018753, accuracy: 93.135%, batch [738560/756895]\n",
      "loss: 0.016743, accuracy: 93.161%, batch [739840/756895]\n",
      "loss: 0.017922, accuracy: 93.148%, batch [741120/756895]\n",
      "loss: 0.017888, accuracy: 92.952%, batch [742400/756895]\n",
      "loss: 0.020005, accuracy: 93.105%, batch [743680/756895]\n",
      "loss: 0.019918, accuracy: 93.095%, batch [744960/756895]\n",
      "loss: 0.022286, accuracy: 92.979%, batch [746240/756895]\n",
      "loss: 0.019936, accuracy: 93.018%, batch [747520/756895]\n",
      "loss: 0.018539, accuracy: 93.023%, batch [748800/756895]\n",
      "loss: 0.017112, accuracy: 93.117%, batch [750080/756895]\n",
      "loss: 0.023082, accuracy: 92.879%, batch [751360/756895]\n",
      "loss: 0.022164, accuracy: 92.872%, batch [752640/756895]\n",
      "loss: 0.017797, accuracy: 93.249%, batch [753920/756895]\n",
      "loss: 0.019973, accuracy: 92.951%, batch [755200/756895]\n",
      "loss: 0.018741, accuracy: 92.944%, batch [756480/756895]\n",
      "Test avg loss: 0.020496, test avg accuracy: 92.996% \n",
      "\n",
      "Test avg loss: 0.019942, test avg accuracy: 93.028% \n",
      "\n",
      "Epoch 108\n",
      "------------------------\n",
      "loss: 0.017742, accuracy: 93.016%, batch [    0/756895]\n",
      "loss: 0.018002, accuracy: 93.228%, batch [ 1280/756895]\n",
      "loss: 0.018477, accuracy: 93.006%, batch [ 2560/756895]\n",
      "loss: 0.021856, accuracy: 92.933%, batch [ 3840/756895]\n",
      "loss: 0.021946, accuracy: 92.874%, batch [ 5120/756895]\n",
      "loss: 0.023962, accuracy: 93.017%, batch [ 6400/756895]\n",
      "loss: 0.017912, accuracy: 93.033%, batch [ 7680/756895]\n",
      "loss: 0.016730, accuracy: 93.101%, batch [ 8960/756895]\n",
      "loss: 0.020831, accuracy: 92.914%, batch [10240/756895]\n",
      "loss: 0.026406, accuracy: 93.121%, batch [11520/756895]\n",
      "loss: 0.018674, accuracy: 93.000%, batch [12800/756895]\n",
      "loss: 0.018622, accuracy: 92.962%, batch [14080/756895]\n",
      "loss: 0.023583, accuracy: 93.034%, batch [15360/756895]\n",
      "loss: 0.022345, accuracy: 92.944%, batch [16640/756895]\n",
      "loss: 0.017158, accuracy: 93.117%, batch [17920/756895]\n",
      "loss: 0.016067, accuracy: 93.196%, batch [19200/756895]\n",
      "loss: 0.018526, accuracy: 93.127%, batch [20480/756895]\n",
      "loss: 0.023080, accuracy: 92.915%, batch [21760/756895]\n",
      "loss: 0.019216, accuracy: 93.035%, batch [23040/756895]\n",
      "loss: 0.016303, accuracy: 93.176%, batch [24320/756895]\n",
      "loss: 0.019489, accuracy: 93.051%, batch [25600/756895]\n",
      "loss: 0.016683, accuracy: 93.147%, batch [26880/756895]\n",
      "loss: 0.016797, accuracy: 93.132%, batch [28160/756895]\n",
      "loss: 0.018849, accuracy: 93.117%, batch [29440/756895]\n",
      "loss: 0.020971, accuracy: 93.059%, batch [30720/756895]\n",
      "loss: 0.021146, accuracy: 92.822%, batch [32000/756895]\n",
      "loss: 0.019017, accuracy: 93.070%, batch [33280/756895]\n",
      "loss: 0.019065, accuracy: 93.179%, batch [34560/756895]\n",
      "loss: 0.016637, accuracy: 93.251%, batch [35840/756895]\n",
      "loss: 0.016073, accuracy: 93.090%, batch [37120/756895]\n",
      "loss: 0.017064, accuracy: 93.062%, batch [38400/756895]\n",
      "loss: 0.018840, accuracy: 93.038%, batch [39680/756895]\n",
      "loss: 0.020935, accuracy: 93.039%, batch [40960/756895]\n",
      "loss: 0.020291, accuracy: 93.040%, batch [42240/756895]\n",
      "loss: 0.019710, accuracy: 92.966%, batch [43520/756895]\n",
      "loss: 0.018309, accuracy: 92.942%, batch [44800/756895]\n",
      "loss: 0.018783, accuracy: 93.022%, batch [46080/756895]\n",
      "loss: 0.022226, accuracy: 92.943%, batch [47360/756895]\n",
      "loss: 0.016940, accuracy: 93.065%, batch [48640/756895]\n",
      "loss: 0.018030, accuracy: 92.973%, batch [49920/756895]\n",
      "loss: 0.020684, accuracy: 93.222%, batch [51200/756895]\n",
      "loss: 0.028324, accuracy: 92.816%, batch [52480/756895]\n",
      "loss: 0.020061, accuracy: 92.926%, batch [53760/756895]\n",
      "loss: 0.017523, accuracy: 93.028%, batch [55040/756895]\n",
      "loss: 0.020577, accuracy: 93.045%, batch [56320/756895]\n",
      "loss: 0.017170, accuracy: 93.111%, batch [57600/756895]\n",
      "loss: 0.028805, accuracy: 92.713%, batch [58880/756895]\n",
      "loss: 0.018835, accuracy: 93.170%, batch [60160/756895]\n",
      "loss: 0.017368, accuracy: 93.119%, batch [61440/756895]\n",
      "loss: 0.018310, accuracy: 93.119%, batch [62720/756895]\n",
      "loss: 0.021573, accuracy: 93.064%, batch [64000/756895]\n",
      "loss: 0.021849, accuracy: 92.951%, batch [65280/756895]\n",
      "loss: 0.017770, accuracy: 93.237%, batch [66560/756895]\n",
      "loss: 0.021994, accuracy: 92.925%, batch [67840/756895]\n",
      "loss: 0.020326, accuracy: 93.211%, batch [69120/756895]\n",
      "loss: 0.016901, accuracy: 93.137%, batch [70400/756895]\n",
      "loss: 0.013559, accuracy: 93.271%, batch [71680/756895]\n",
      "loss: 0.018170, accuracy: 93.116%, batch [72960/756895]\n",
      "loss: 0.021902, accuracy: 92.983%, batch [74240/756895]\n",
      "loss: 0.017066, accuracy: 92.998%, batch [75520/756895]\n",
      "loss: 0.020752, accuracy: 93.165%, batch [76800/756895]\n",
      "loss: 0.020406, accuracy: 93.139%, batch [78080/756895]\n",
      "loss: 0.019264, accuracy: 92.920%, batch [79360/756895]\n",
      "loss: 0.016358, accuracy: 93.349%, batch [80640/756895]\n",
      "loss: 0.017907, accuracy: 92.879%, batch [81920/756895]\n",
      "loss: 0.018303, accuracy: 93.112%, batch [83200/756895]\n",
      "loss: 0.019880, accuracy: 93.120%, batch [84480/756895]\n",
      "loss: 0.017200, accuracy: 92.917%, batch [85760/756895]\n",
      "loss: 0.018018, accuracy: 93.167%, batch [87040/756895]\n",
      "loss: 0.025320, accuracy: 92.826%, batch [88320/756895]\n",
      "loss: 0.014715, accuracy: 93.383%, batch [89600/756895]\n",
      "loss: 0.020465, accuracy: 92.983%, batch [90880/756895]\n",
      "loss: 0.015396, accuracy: 93.277%, batch [92160/756895]\n",
      "loss: 0.018477, accuracy: 93.060%, batch [93440/756895]\n",
      "loss: 0.015041, accuracy: 93.133%, batch [94720/756895]\n",
      "loss: 0.020618, accuracy: 92.953%, batch [96000/756895]\n",
      "loss: 0.019261, accuracy: 93.057%, batch [97280/756895]\n",
      "loss: 0.014048, accuracy: 93.318%, batch [98560/756895]\n",
      "loss: 0.019231, accuracy: 92.928%, batch [99840/756895]\n",
      "loss: 0.015504, accuracy: 93.243%, batch [101120/756895]\n",
      "loss: 0.019970, accuracy: 93.106%, batch [102400/756895]\n",
      "loss: 0.020098, accuracy: 92.962%, batch [103680/756895]\n",
      "loss: 0.015908, accuracy: 93.219%, batch [104960/756895]\n",
      "loss: 0.016421, accuracy: 93.126%, batch [106240/756895]\n",
      "loss: 0.021319, accuracy: 92.853%, batch [107520/756895]\n",
      "loss: 0.018638, accuracy: 93.178%, batch [108800/756895]\n",
      "loss: 0.020003, accuracy: 93.118%, batch [110080/756895]\n",
      "loss: 0.023999, accuracy: 92.950%, batch [111360/756895]\n",
      "loss: 0.017356, accuracy: 92.978%, batch [112640/756895]\n",
      "loss: 0.016569, accuracy: 93.043%, batch [113920/756895]\n",
      "loss: 0.028978, accuracy: 92.653%, batch [115200/756895]\n",
      "loss: 0.017094, accuracy: 93.020%, batch [116480/756895]\n",
      "loss: 0.020237, accuracy: 93.077%, batch [117760/756895]\n",
      "loss: 0.021697, accuracy: 92.960%, batch [119040/756895]\n",
      "loss: 0.018740, accuracy: 92.996%, batch [120320/756895]\n",
      "loss: 0.027796, accuracy: 92.579%, batch [121600/756895]\n",
      "loss: 0.021220, accuracy: 92.908%, batch [122880/756895]\n",
      "loss: 0.016773, accuracy: 93.029%, batch [124160/756895]\n",
      "loss: 0.014931, accuracy: 93.256%, batch [125440/756895]\n",
      "loss: 0.017184, accuracy: 93.181%, batch [126720/756895]\n",
      "loss: 0.020050, accuracy: 93.137%, batch [128000/756895]\n",
      "loss: 0.021492, accuracy: 92.950%, batch [129280/756895]\n",
      "loss: 0.023710, accuracy: 92.822%, batch [130560/756895]\n",
      "loss: 0.019316, accuracy: 93.152%, batch [131840/756895]\n",
      "loss: 0.015021, accuracy: 93.142%, batch [133120/756895]\n",
      "loss: 0.017732, accuracy: 93.065%, batch [134400/756895]\n",
      "loss: 0.020525, accuracy: 93.117%, batch [135680/756895]\n",
      "loss: 0.021162, accuracy: 92.998%, batch [136960/756895]\n",
      "loss: 0.023385, accuracy: 93.018%, batch [138240/756895]\n",
      "loss: 0.018237, accuracy: 93.105%, batch [139520/756895]\n",
      "loss: 0.018232, accuracy: 93.273%, batch [140800/756895]\n",
      "loss: 0.017747, accuracy: 93.145%, batch [142080/756895]\n",
      "loss: 0.016094, accuracy: 93.208%, batch [143360/756895]\n",
      "loss: 0.018449, accuracy: 93.019%, batch [144640/756895]\n",
      "loss: 0.017072, accuracy: 93.166%, batch [145920/756895]\n",
      "loss: 0.017419, accuracy: 92.996%, batch [147200/756895]\n",
      "loss: 0.015410, accuracy: 93.226%, batch [148480/756895]\n",
      "loss: 0.018909, accuracy: 93.169%, batch [149760/756895]\n",
      "loss: 0.016181, accuracy: 92.954%, batch [151040/756895]\n",
      "loss: 0.018774, accuracy: 93.012%, batch [152320/756895]\n",
      "loss: 0.017459, accuracy: 93.233%, batch [153600/756895]\n",
      "loss: 0.019092, accuracy: 92.986%, batch [154880/756895]\n",
      "loss: 0.019254, accuracy: 93.037%, batch [156160/756895]\n",
      "loss: 0.013900, accuracy: 93.238%, batch [157440/756895]\n",
      "loss: 0.016583, accuracy: 93.122%, batch [158720/756895]\n",
      "loss: 0.017014, accuracy: 92.971%, batch [160000/756895]\n",
      "loss: 0.025526, accuracy: 93.070%, batch [161280/756895]\n",
      "loss: 0.029495, accuracy: 92.598%, batch [162560/756895]\n",
      "loss: 0.019057, accuracy: 93.096%, batch [163840/756895]\n",
      "loss: 0.017617, accuracy: 93.083%, batch [165120/756895]\n",
      "loss: 0.018502, accuracy: 93.090%, batch [166400/756895]\n",
      "loss: 0.018610, accuracy: 93.098%, batch [167680/756895]\n",
      "loss: 0.018017, accuracy: 93.312%, batch [168960/756895]\n",
      "loss: 0.020084, accuracy: 92.959%, batch [170240/756895]\n",
      "loss: 0.018887, accuracy: 93.025%, batch [171520/756895]\n",
      "loss: 0.023448, accuracy: 93.011%, batch [172800/756895]\n",
      "loss: 0.018035, accuracy: 92.890%, batch [174080/756895]\n",
      "loss: 0.018775, accuracy: 92.959%, batch [175360/756895]\n",
      "loss: 0.017346, accuracy: 93.244%, batch [176640/756895]\n",
      "loss: 0.015858, accuracy: 93.250%, batch [177920/756895]\n",
      "loss: 0.018003, accuracy: 93.108%, batch [179200/756895]\n",
      "loss: 0.016745, accuracy: 93.052%, batch [180480/756895]\n",
      "loss: 0.018688, accuracy: 92.966%, batch [181760/756895]\n",
      "loss: 0.020075, accuracy: 92.888%, batch [183040/756895]\n",
      "loss: 0.019932, accuracy: 93.036%, batch [184320/756895]\n",
      "loss: 0.021370, accuracy: 92.887%, batch [185600/756895]\n",
      "loss: 0.022281, accuracy: 92.892%, batch [186880/756895]\n",
      "loss: 0.016084, accuracy: 93.299%, batch [188160/756895]\n",
      "loss: 0.014232, accuracy: 93.178%, batch [189440/756895]\n",
      "loss: 0.021587, accuracy: 93.058%, batch [190720/756895]\n",
      "loss: 0.018498, accuracy: 92.957%, batch [192000/756895]\n",
      "loss: 0.017801, accuracy: 92.994%, batch [193280/756895]\n",
      "loss: 0.018787, accuracy: 93.122%, batch [194560/756895]\n",
      "loss: 0.024554, accuracy: 92.938%, batch [195840/756895]\n",
      "loss: 0.015172, accuracy: 93.357%, batch [197120/756895]\n",
      "loss: 0.017046, accuracy: 93.209%, batch [198400/756895]\n",
      "loss: 0.019793, accuracy: 93.270%, batch [199680/756895]\n",
      "loss: 0.018258, accuracy: 93.026%, batch [200960/756895]\n",
      "loss: 0.020492, accuracy: 92.977%, batch [202240/756895]\n",
      "loss: 0.020117, accuracy: 93.077%, batch [203520/756895]\n",
      "loss: 0.017964, accuracy: 93.027%, batch [204800/756895]\n",
      "loss: 0.018464, accuracy: 93.090%, batch [206080/756895]\n",
      "loss: 0.018999, accuracy: 93.078%, batch [207360/756895]\n",
      "loss: 0.019463, accuracy: 92.902%, batch [208640/756895]\n",
      "loss: 0.022231, accuracy: 92.648%, batch [209920/756895]\n",
      "loss: 0.018213, accuracy: 93.012%, batch [211200/756895]\n",
      "loss: 0.018212, accuracy: 93.026%, batch [212480/756895]\n",
      "loss: 0.020815, accuracy: 92.903%, batch [213760/756895]\n",
      "loss: 0.018080, accuracy: 93.034%, batch [215040/756895]\n",
      "loss: 0.017545, accuracy: 93.159%, batch [216320/756895]\n",
      "loss: 0.019979, accuracy: 92.912%, batch [217600/756895]\n",
      "loss: 0.017106, accuracy: 93.166%, batch [218880/756895]\n",
      "loss: 0.018465, accuracy: 93.130%, batch [220160/756895]\n",
      "loss: 0.017339, accuracy: 93.193%, batch [221440/756895]\n",
      "loss: 0.020410, accuracy: 92.870%, batch [222720/756895]\n",
      "loss: 0.016364, accuracy: 93.202%, batch [224000/756895]\n",
      "loss: 0.017792, accuracy: 93.284%, batch [225280/756895]\n",
      "loss: 0.021736, accuracy: 92.942%, batch [226560/756895]\n",
      "loss: 0.017003, accuracy: 93.069%, batch [227840/756895]\n",
      "loss: 0.017485, accuracy: 93.200%, batch [229120/756895]\n",
      "loss: 0.016450, accuracy: 93.205%, batch [230400/756895]\n",
      "loss: 0.016673, accuracy: 93.135%, batch [231680/756895]\n",
      "loss: 0.022007, accuracy: 92.860%, batch [232960/756895]\n",
      "loss: 0.020068, accuracy: 93.157%, batch [234240/756895]\n",
      "loss: 0.019419, accuracy: 93.077%, batch [235520/756895]\n",
      "loss: 0.016063, accuracy: 93.154%, batch [236800/756895]\n",
      "loss: 0.019813, accuracy: 92.963%, batch [238080/756895]\n",
      "loss: 0.017691, accuracy: 92.926%, batch [239360/756895]\n",
      "loss: 0.014524, accuracy: 93.270%, batch [240640/756895]\n",
      "loss: 0.020916, accuracy: 92.930%, batch [241920/756895]\n",
      "loss: 0.015582, accuracy: 93.158%, batch [243200/756895]\n",
      "loss: 0.019422, accuracy: 92.991%, batch [244480/756895]\n",
      "loss: 0.014397, accuracy: 93.193%, batch [245760/756895]\n",
      "loss: 0.016331, accuracy: 93.201%, batch [247040/756895]\n",
      "loss: 0.023391, accuracy: 92.903%, batch [248320/756895]\n",
      "loss: 0.019561, accuracy: 93.108%, batch [249600/756895]\n",
      "loss: 0.019498, accuracy: 92.995%, batch [250880/756895]\n",
      "loss: 0.021239, accuracy: 92.983%, batch [252160/756895]\n",
      "loss: 0.026822, accuracy: 92.650%, batch [253440/756895]\n",
      "loss: 0.020847, accuracy: 93.102%, batch [254720/756895]\n",
      "loss: 0.021650, accuracy: 93.063%, batch [256000/756895]\n",
      "loss: 0.019432, accuracy: 92.947%, batch [257280/756895]\n",
      "loss: 0.019235, accuracy: 93.042%, batch [258560/756895]\n",
      "loss: 0.017461, accuracy: 93.129%, batch [259840/756895]\n",
      "loss: 0.018148, accuracy: 93.158%, batch [261120/756895]\n",
      "loss: 0.015010, accuracy: 93.179%, batch [262400/756895]\n",
      "loss: 0.018206, accuracy: 93.033%, batch [263680/756895]\n",
      "loss: 0.021564, accuracy: 92.860%, batch [264960/756895]\n",
      "loss: 0.018214, accuracy: 93.116%, batch [266240/756895]\n",
      "loss: 0.018408, accuracy: 92.969%, batch [267520/756895]\n",
      "loss: 0.019005, accuracy: 92.986%, batch [268800/756895]\n",
      "loss: 0.017609, accuracy: 92.998%, batch [270080/756895]\n",
      "loss: 0.013915, accuracy: 93.431%, batch [271360/756895]\n",
      "loss: 0.018884, accuracy: 93.150%, batch [272640/756895]\n",
      "loss: 0.019709, accuracy: 92.919%, batch [273920/756895]\n",
      "loss: 0.018706, accuracy: 93.179%, batch [275200/756895]\n",
      "loss: 0.016701, accuracy: 93.092%, batch [276480/756895]\n",
      "loss: 0.015174, accuracy: 93.197%, batch [277760/756895]\n",
      "loss: 0.017093, accuracy: 93.149%, batch [279040/756895]\n",
      "loss: 0.020111, accuracy: 93.043%, batch [280320/756895]\n",
      "loss: 0.017301, accuracy: 93.084%, batch [281600/756895]\n",
      "loss: 0.017406, accuracy: 93.016%, batch [282880/756895]\n",
      "loss: 0.027094, accuracy: 92.814%, batch [284160/756895]\n",
      "loss: 0.020954, accuracy: 92.914%, batch [285440/756895]\n",
      "loss: 0.020710, accuracy: 93.040%, batch [286720/756895]\n",
      "loss: 0.022460, accuracy: 93.156%, batch [288000/756895]\n",
      "loss: 0.017759, accuracy: 93.060%, batch [289280/756895]\n",
      "loss: 0.020648, accuracy: 93.040%, batch [290560/756895]\n",
      "loss: 0.020935, accuracy: 93.104%, batch [291840/756895]\n",
      "loss: 0.019724, accuracy: 92.910%, batch [293120/756895]\n",
      "loss: 0.014665, accuracy: 93.231%, batch [294400/756895]\n",
      "loss: 0.016354, accuracy: 93.202%, batch [295680/756895]\n",
      "loss: 0.021110, accuracy: 93.127%, batch [296960/756895]\n",
      "loss: 0.020537, accuracy: 93.092%, batch [298240/756895]\n",
      "loss: 0.018117, accuracy: 93.049%, batch [299520/756895]\n",
      "loss: 0.021584, accuracy: 92.807%, batch [300800/756895]\n",
      "loss: 0.020054, accuracy: 93.003%, batch [302080/756895]\n",
      "loss: 0.023664, accuracy: 93.129%, batch [303360/756895]\n",
      "loss: 0.019166, accuracy: 93.011%, batch [304640/756895]\n",
      "loss: 0.020301, accuracy: 92.850%, batch [305920/756895]\n",
      "loss: 0.018871, accuracy: 93.214%, batch [307200/756895]\n",
      "loss: 0.017207, accuracy: 93.154%, batch [308480/756895]\n",
      "loss: 0.018598, accuracy: 92.959%, batch [309760/756895]\n",
      "loss: 0.020054, accuracy: 93.046%, batch [311040/756895]\n",
      "loss: 0.021649, accuracy: 92.959%, batch [312320/756895]\n",
      "loss: 0.021118, accuracy: 92.951%, batch [313600/756895]\n",
      "loss: 0.020724, accuracy: 93.182%, batch [314880/756895]\n",
      "loss: 0.016700, accuracy: 93.026%, batch [316160/756895]\n",
      "loss: 0.019043, accuracy: 93.112%, batch [317440/756895]\n",
      "loss: 0.023181, accuracy: 92.849%, batch [318720/756895]\n",
      "loss: 0.016462, accuracy: 93.153%, batch [320000/756895]\n",
      "loss: 0.019225, accuracy: 93.101%, batch [321280/756895]\n",
      "loss: 0.017471, accuracy: 93.238%, batch [322560/756895]\n",
      "loss: 0.019544, accuracy: 93.031%, batch [323840/756895]\n",
      "loss: 0.017900, accuracy: 93.021%, batch [325120/756895]\n",
      "loss: 0.020123, accuracy: 92.998%, batch [326400/756895]\n",
      "loss: 0.014354, accuracy: 93.313%, batch [327680/756895]\n",
      "loss: 0.020950, accuracy: 92.906%, batch [328960/756895]\n",
      "loss: 0.019824, accuracy: 92.965%, batch [330240/756895]\n",
      "loss: 0.017431, accuracy: 93.147%, batch [331520/756895]\n",
      "loss: 0.022195, accuracy: 92.953%, batch [332800/756895]\n",
      "loss: 0.014478, accuracy: 93.212%, batch [334080/756895]\n",
      "loss: 0.021489, accuracy: 93.085%, batch [335360/756895]\n",
      "loss: 0.022374, accuracy: 92.966%, batch [336640/756895]\n",
      "loss: 0.018045, accuracy: 93.193%, batch [337920/756895]\n",
      "loss: 0.018959, accuracy: 93.116%, batch [339200/756895]\n",
      "loss: 0.017659, accuracy: 93.222%, batch [340480/756895]\n",
      "loss: 0.017414, accuracy: 93.063%, batch [341760/756895]\n",
      "loss: 0.018164, accuracy: 93.258%, batch [343040/756895]\n",
      "loss: 0.018400, accuracy: 93.073%, batch [344320/756895]\n",
      "loss: 0.019148, accuracy: 93.065%, batch [345600/756895]\n",
      "loss: 0.019438, accuracy: 92.986%, batch [346880/756895]\n",
      "loss: 0.023472, accuracy: 92.927%, batch [348160/756895]\n",
      "loss: 0.016322, accuracy: 93.193%, batch [349440/756895]\n",
      "loss: 0.019167, accuracy: 92.953%, batch [350720/756895]\n",
      "loss: 0.017783, accuracy: 93.014%, batch [352000/756895]\n",
      "loss: 0.018005, accuracy: 93.050%, batch [353280/756895]\n",
      "loss: 0.021357, accuracy: 93.046%, batch [354560/756895]\n",
      "loss: 0.019185, accuracy: 92.980%, batch [355840/756895]\n",
      "loss: 0.023951, accuracy: 92.921%, batch [357120/756895]\n",
      "loss: 0.015518, accuracy: 93.132%, batch [358400/756895]\n",
      "loss: 0.020905, accuracy: 92.840%, batch [359680/756895]\n",
      "loss: 0.021298, accuracy: 92.869%, batch [360960/756895]\n",
      "loss: 0.016452, accuracy: 93.235%, batch [362240/756895]\n",
      "loss: 0.019384, accuracy: 93.121%, batch [363520/756895]\n",
      "loss: 0.021601, accuracy: 92.923%, batch [364800/756895]\n",
      "loss: 0.024056, accuracy: 93.013%, batch [366080/756895]\n",
      "loss: 0.026215, accuracy: 92.700%, batch [367360/756895]\n",
      "loss: 0.018843, accuracy: 93.066%, batch [368640/756895]\n",
      "loss: 0.020810, accuracy: 93.059%, batch [369920/756895]\n",
      "loss: 0.020341, accuracy: 93.057%, batch [371200/756895]\n",
      "loss: 0.018466, accuracy: 93.058%, batch [372480/756895]\n",
      "loss: 0.019264, accuracy: 93.032%, batch [373760/756895]\n",
      "loss: 0.019879, accuracy: 93.013%, batch [375040/756895]\n",
      "loss: 0.020549, accuracy: 92.910%, batch [376320/756895]\n",
      "loss: 0.014364, accuracy: 93.216%, batch [377600/756895]\n",
      "loss: 0.015897, accuracy: 93.218%, batch [378880/756895]\n",
      "loss: 0.018122, accuracy: 93.161%, batch [380160/756895]\n",
      "loss: 0.018786, accuracy: 93.095%, batch [381440/756895]\n",
      "loss: 0.014386, accuracy: 93.227%, batch [382720/756895]\n",
      "loss: 0.015752, accuracy: 93.019%, batch [384000/756895]\n",
      "loss: 0.015352, accuracy: 93.180%, batch [385280/756895]\n",
      "loss: 0.016202, accuracy: 93.129%, batch [386560/756895]\n",
      "loss: 0.017014, accuracy: 93.047%, batch [387840/756895]\n",
      "loss: 0.022865, accuracy: 92.935%, batch [389120/756895]\n",
      "loss: 0.022430, accuracy: 92.951%, batch [390400/756895]\n",
      "loss: 0.021365, accuracy: 92.826%, batch [391680/756895]\n",
      "loss: 0.017284, accuracy: 93.232%, batch [392960/756895]\n",
      "loss: 0.014820, accuracy: 93.336%, batch [394240/756895]\n",
      "loss: 0.022603, accuracy: 92.958%, batch [395520/756895]\n",
      "loss: 0.021038, accuracy: 93.008%, batch [396800/756895]\n",
      "loss: 0.015353, accuracy: 93.226%, batch [398080/756895]\n",
      "loss: 0.020465, accuracy: 92.990%, batch [399360/756895]\n",
      "loss: 0.022307, accuracy: 92.793%, batch [400640/756895]\n",
      "loss: 0.021669, accuracy: 92.979%, batch [401920/756895]\n",
      "loss: 0.018996, accuracy: 93.215%, batch [403200/756895]\n",
      "loss: 0.028071, accuracy: 92.829%, batch [404480/756895]\n",
      "loss: 0.018952, accuracy: 93.010%, batch [405760/756895]\n",
      "loss: 0.017997, accuracy: 92.952%, batch [407040/756895]\n",
      "loss: 0.019872, accuracy: 93.013%, batch [408320/756895]\n",
      "loss: 0.023818, accuracy: 92.791%, batch [409600/756895]\n",
      "loss: 0.018296, accuracy: 92.940%, batch [410880/756895]\n",
      "loss: 0.023985, accuracy: 92.773%, batch [412160/756895]\n",
      "loss: 0.019207, accuracy: 93.000%, batch [413440/756895]\n",
      "loss: 0.016452, accuracy: 93.083%, batch [414720/756895]\n",
      "loss: 0.020441, accuracy: 93.243%, batch [416000/756895]\n",
      "loss: 0.013954, accuracy: 93.269%, batch [417280/756895]\n",
      "loss: 0.015662, accuracy: 93.194%, batch [418560/756895]\n",
      "loss: 0.015897, accuracy: 93.018%, batch [419840/756895]\n",
      "loss: 0.015473, accuracy: 93.282%, batch [421120/756895]\n",
      "loss: 0.019020, accuracy: 93.168%, batch [422400/756895]\n",
      "loss: 0.020671, accuracy: 92.996%, batch [423680/756895]\n",
      "loss: 0.017902, accuracy: 93.074%, batch [424960/756895]\n",
      "loss: 0.020446, accuracy: 93.148%, batch [426240/756895]\n",
      "loss: 0.016254, accuracy: 93.187%, batch [427520/756895]\n",
      "loss: 0.019717, accuracy: 93.273%, batch [428800/756895]\n",
      "loss: 0.020604, accuracy: 93.017%, batch [430080/756895]\n",
      "loss: 0.017906, accuracy: 93.131%, batch [431360/756895]\n",
      "loss: 0.019368, accuracy: 93.149%, batch [432640/756895]\n",
      "loss: 0.015431, accuracy: 93.338%, batch [433920/756895]\n",
      "loss: 0.019970, accuracy: 93.142%, batch [435200/756895]\n",
      "loss: 0.017454, accuracy: 93.089%, batch [436480/756895]\n",
      "loss: 0.018423, accuracy: 93.130%, batch [437760/756895]\n",
      "loss: 0.017647, accuracy: 92.906%, batch [439040/756895]\n",
      "loss: 0.021112, accuracy: 92.995%, batch [440320/756895]\n",
      "loss: 0.021663, accuracy: 92.979%, batch [441600/756895]\n",
      "loss: 0.017682, accuracy: 93.041%, batch [442880/756895]\n",
      "loss: 0.025028, accuracy: 92.880%, batch [444160/756895]\n",
      "loss: 0.020747, accuracy: 93.080%, batch [445440/756895]\n",
      "loss: 0.018201, accuracy: 93.103%, batch [446720/756895]\n",
      "loss: 0.018565, accuracy: 93.180%, batch [448000/756895]\n",
      "loss: 0.018510, accuracy: 92.921%, batch [449280/756895]\n",
      "loss: 0.015443, accuracy: 93.276%, batch [450560/756895]\n",
      "loss: 0.021548, accuracy: 93.110%, batch [451840/756895]\n",
      "loss: 0.018823, accuracy: 92.992%, batch [453120/756895]\n",
      "loss: 0.018805, accuracy: 93.038%, batch [454400/756895]\n",
      "loss: 0.019478, accuracy: 93.115%, batch [455680/756895]\n",
      "loss: 0.019592, accuracy: 93.128%, batch [456960/756895]\n",
      "loss: 0.021500, accuracy: 93.081%, batch [458240/756895]\n",
      "loss: 0.020949, accuracy: 92.854%, batch [459520/756895]\n",
      "loss: 0.018560, accuracy: 93.062%, batch [460800/756895]\n",
      "loss: 0.019834, accuracy: 92.981%, batch [462080/756895]\n",
      "loss: 0.019256, accuracy: 92.998%, batch [463360/756895]\n",
      "loss: 0.022100, accuracy: 92.833%, batch [464640/756895]\n",
      "loss: 0.023671, accuracy: 92.822%, batch [465920/756895]\n",
      "loss: 0.017625, accuracy: 93.036%, batch [467200/756895]\n",
      "loss: 0.019259, accuracy: 93.125%, batch [468480/756895]\n",
      "loss: 0.016918, accuracy: 93.240%, batch [469760/756895]\n",
      "loss: 0.015274, accuracy: 93.199%, batch [471040/756895]\n",
      "loss: 0.017885, accuracy: 93.147%, batch [472320/756895]\n",
      "loss: 0.015873, accuracy: 93.126%, batch [473600/756895]\n",
      "loss: 0.018189, accuracy: 92.977%, batch [474880/756895]\n",
      "loss: 0.018954, accuracy: 93.047%, batch [476160/756895]\n",
      "loss: 0.015829, accuracy: 93.080%, batch [477440/756895]\n",
      "loss: 0.017508, accuracy: 93.051%, batch [478720/756895]\n",
      "loss: 0.015359, accuracy: 93.210%, batch [480000/756895]\n",
      "loss: 0.015416, accuracy: 93.115%, batch [481280/756895]\n",
      "loss: 0.017210, accuracy: 93.275%, batch [482560/756895]\n",
      "loss: 0.020858, accuracy: 93.198%, batch [483840/756895]\n",
      "loss: 0.016808, accuracy: 93.225%, batch [485120/756895]\n",
      "loss: 0.024220, accuracy: 92.895%, batch [486400/756895]\n",
      "loss: 0.016423, accuracy: 93.225%, batch [487680/756895]\n",
      "loss: 0.017132, accuracy: 93.188%, batch [488960/756895]\n",
      "loss: 0.018195, accuracy: 93.015%, batch [490240/756895]\n",
      "loss: 0.015710, accuracy: 93.205%, batch [491520/756895]\n",
      "loss: 0.015336, accuracy: 93.121%, batch [492800/756895]\n",
      "loss: 0.016843, accuracy: 93.197%, batch [494080/756895]\n",
      "loss: 0.024482, accuracy: 93.023%, batch [495360/756895]\n",
      "loss: 0.018635, accuracy: 93.078%, batch [496640/756895]\n",
      "loss: 0.018106, accuracy: 93.080%, batch [497920/756895]\n",
      "loss: 0.015084, accuracy: 93.240%, batch [499200/756895]\n",
      "loss: 0.015060, accuracy: 93.081%, batch [500480/756895]\n",
      "loss: 0.026756, accuracy: 92.732%, batch [501760/756895]\n",
      "loss: 0.019837, accuracy: 93.084%, batch [503040/756895]\n",
      "loss: 0.019288, accuracy: 92.992%, batch [504320/756895]\n",
      "loss: 0.017594, accuracy: 93.018%, batch [505600/756895]\n",
      "loss: 0.020040, accuracy: 93.115%, batch [506880/756895]\n",
      "loss: 0.014947, accuracy: 93.007%, batch [508160/756895]\n",
      "loss: 0.018195, accuracy: 92.985%, batch [509440/756895]\n",
      "loss: 0.019789, accuracy: 93.157%, batch [510720/756895]\n",
      "loss: 0.022250, accuracy: 92.896%, batch [512000/756895]\n",
      "loss: 0.024005, accuracy: 92.991%, batch [513280/756895]\n",
      "loss: 0.020531, accuracy: 93.122%, batch [514560/756895]\n",
      "loss: 0.022019, accuracy: 92.942%, batch [515840/756895]\n",
      "loss: 0.019918, accuracy: 92.919%, batch [517120/756895]\n",
      "loss: 0.019323, accuracy: 93.024%, batch [518400/756895]\n",
      "loss: 0.022805, accuracy: 92.939%, batch [519680/756895]\n",
      "loss: 0.019588, accuracy: 93.041%, batch [520960/756895]\n",
      "loss: 0.023132, accuracy: 93.011%, batch [522240/756895]\n",
      "loss: 0.019604, accuracy: 93.107%, batch [523520/756895]\n",
      "loss: 0.022096, accuracy: 92.912%, batch [524800/756895]\n",
      "loss: 0.019357, accuracy: 93.034%, batch [526080/756895]\n",
      "loss: 0.022283, accuracy: 92.972%, batch [527360/756895]\n",
      "loss: 0.014202, accuracy: 93.487%, batch [528640/756895]\n",
      "loss: 0.027298, accuracy: 92.709%, batch [529920/756895]\n",
      "loss: 0.017020, accuracy: 93.128%, batch [531200/756895]\n",
      "loss: 0.020475, accuracy: 92.954%, batch [532480/756895]\n",
      "loss: 0.017540, accuracy: 92.931%, batch [533760/756895]\n",
      "loss: 0.021316, accuracy: 92.905%, batch [535040/756895]\n",
      "loss: 0.016751, accuracy: 93.195%, batch [536320/756895]\n",
      "loss: 0.027281, accuracy: 92.896%, batch [537600/756895]\n",
      "loss: 0.019061, accuracy: 93.163%, batch [538880/756895]\n",
      "loss: 0.020599, accuracy: 93.014%, batch [540160/756895]\n",
      "loss: 0.015603, accuracy: 93.127%, batch [541440/756895]\n",
      "loss: 0.018856, accuracy: 93.172%, batch [542720/756895]\n",
      "loss: 0.014464, accuracy: 93.149%, batch [544000/756895]\n",
      "loss: 0.023650, accuracy: 92.891%, batch [545280/756895]\n",
      "loss: 0.020318, accuracy: 93.149%, batch [546560/756895]\n",
      "loss: 0.018889, accuracy: 92.998%, batch [547840/756895]\n",
      "loss: 0.015701, accuracy: 93.089%, batch [549120/756895]\n",
      "loss: 0.019878, accuracy: 93.163%, batch [550400/756895]\n",
      "loss: 0.023173, accuracy: 92.925%, batch [551680/756895]\n",
      "loss: 0.015273, accuracy: 93.190%, batch [552960/756895]\n",
      "loss: 0.018201, accuracy: 93.079%, batch [554240/756895]\n",
      "loss: 0.026075, accuracy: 92.798%, batch [555520/756895]\n",
      "loss: 0.015340, accuracy: 93.245%, batch [556800/756895]\n",
      "loss: 0.022205, accuracy: 92.948%, batch [558080/756895]\n",
      "loss: 0.018631, accuracy: 92.959%, batch [559360/756895]\n",
      "loss: 0.025631, accuracy: 92.996%, batch [560640/756895]\n",
      "loss: 0.019103, accuracy: 93.087%, batch [561920/756895]\n",
      "loss: 0.017423, accuracy: 93.173%, batch [563200/756895]\n",
      "loss: 0.017699, accuracy: 93.052%, batch [564480/756895]\n",
      "loss: 0.017228, accuracy: 93.154%, batch [565760/756895]\n",
      "loss: 0.015301, accuracy: 93.166%, batch [567040/756895]\n",
      "loss: 0.017711, accuracy: 93.115%, batch [568320/756895]\n",
      "loss: 0.022682, accuracy: 92.998%, batch [569600/756895]\n",
      "loss: 0.017443, accuracy: 93.165%, batch [570880/756895]\n",
      "loss: 0.020211, accuracy: 92.739%, batch [572160/756895]\n",
      "loss: 0.020426, accuracy: 92.726%, batch [573440/756895]\n",
      "loss: 0.019633, accuracy: 92.988%, batch [574720/756895]\n",
      "loss: 0.021019, accuracy: 93.018%, batch [576000/756895]\n",
      "loss: 0.021560, accuracy: 92.946%, batch [577280/756895]\n",
      "loss: 0.018419, accuracy: 92.997%, batch [578560/756895]\n",
      "loss: 0.019173, accuracy: 93.169%, batch [579840/756895]\n",
      "loss: 0.017624, accuracy: 93.259%, batch [581120/756895]\n",
      "loss: 0.018568, accuracy: 92.838%, batch [582400/756895]\n",
      "loss: 0.017895, accuracy: 92.942%, batch [583680/756895]\n",
      "loss: 0.018340, accuracy: 93.158%, batch [584960/756895]\n",
      "loss: 0.017956, accuracy: 93.094%, batch [586240/756895]\n",
      "loss: 0.023812, accuracy: 93.041%, batch [587520/756895]\n",
      "loss: 0.021446, accuracy: 93.040%, batch [588800/756895]\n",
      "loss: 0.015508, accuracy: 93.239%, batch [590080/756895]\n",
      "loss: 0.020682, accuracy: 92.914%, batch [591360/756895]\n",
      "loss: 0.018844, accuracy: 92.992%, batch [592640/756895]\n",
      "loss: 0.021635, accuracy: 93.051%, batch [593920/756895]\n",
      "loss: 0.019984, accuracy: 92.999%, batch [595200/756895]\n",
      "loss: 0.017475, accuracy: 92.951%, batch [596480/756895]\n",
      "loss: 0.015407, accuracy: 93.297%, batch [597760/756895]\n",
      "loss: 0.015861, accuracy: 93.162%, batch [599040/756895]\n",
      "loss: 0.020690, accuracy: 92.927%, batch [600320/756895]\n",
      "loss: 0.018471, accuracy: 92.716%, batch [601600/756895]\n",
      "loss: 0.018796, accuracy: 93.069%, batch [602880/756895]\n",
      "loss: 0.015872, accuracy: 93.039%, batch [604160/756895]\n",
      "loss: 0.016561, accuracy: 92.974%, batch [605440/756895]\n",
      "loss: 0.016842, accuracy: 93.233%, batch [606720/756895]\n",
      "loss: 0.017478, accuracy: 93.178%, batch [608000/756895]\n",
      "loss: 0.017646, accuracy: 93.062%, batch [609280/756895]\n",
      "loss: 0.019838, accuracy: 93.120%, batch [610560/756895]\n",
      "loss: 0.026580, accuracy: 92.946%, batch [611840/756895]\n",
      "loss: 0.015348, accuracy: 93.185%, batch [613120/756895]\n",
      "loss: 0.016002, accuracy: 93.384%, batch [614400/756895]\n",
      "loss: 0.022670, accuracy: 92.954%, batch [615680/756895]\n",
      "loss: 0.017785, accuracy: 92.953%, batch [616960/756895]\n",
      "loss: 0.024142, accuracy: 92.908%, batch [618240/756895]\n",
      "loss: 0.018769, accuracy: 92.911%, batch [619520/756895]\n",
      "loss: 0.025430, accuracy: 92.660%, batch [620800/756895]\n",
      "loss: 0.015741, accuracy: 93.247%, batch [622080/756895]\n",
      "loss: 0.017312, accuracy: 93.053%, batch [623360/756895]\n",
      "loss: 0.018159, accuracy: 93.127%, batch [624640/756895]\n",
      "loss: 0.017851, accuracy: 93.140%, batch [625920/756895]\n",
      "loss: 0.020789, accuracy: 92.923%, batch [627200/756895]\n",
      "loss: 0.017712, accuracy: 93.151%, batch [628480/756895]\n",
      "loss: 0.020232, accuracy: 93.146%, batch [629760/756895]\n",
      "loss: 0.024104, accuracy: 92.894%, batch [631040/756895]\n",
      "loss: 0.016331, accuracy: 93.167%, batch [632320/756895]\n",
      "loss: 0.023233, accuracy: 92.783%, batch [633600/756895]\n",
      "loss: 0.022130, accuracy: 93.000%, batch [634880/756895]\n",
      "loss: 0.015495, accuracy: 93.064%, batch [636160/756895]\n",
      "loss: 0.019688, accuracy: 93.209%, batch [637440/756895]\n",
      "loss: 0.016682, accuracy: 92.994%, batch [638720/756895]\n",
      "loss: 0.014107, accuracy: 93.248%, batch [640000/756895]\n",
      "loss: 0.014120, accuracy: 93.325%, batch [641280/756895]\n",
      "loss: 0.016753, accuracy: 93.020%, batch [642560/756895]\n",
      "loss: 0.018773, accuracy: 92.978%, batch [643840/756895]\n",
      "loss: 0.016677, accuracy: 93.026%, batch [645120/756895]\n",
      "loss: 0.019642, accuracy: 92.845%, batch [646400/756895]\n",
      "loss: 0.021675, accuracy: 92.951%, batch [647680/756895]\n",
      "loss: 0.020959, accuracy: 92.979%, batch [648960/756895]\n",
      "loss: 0.021014, accuracy: 93.217%, batch [650240/756895]\n",
      "loss: 0.016725, accuracy: 93.156%, batch [651520/756895]\n",
      "loss: 0.022585, accuracy: 92.923%, batch [652800/756895]\n",
      "loss: 0.022270, accuracy: 92.803%, batch [654080/756895]\n",
      "loss: 0.016726, accuracy: 93.249%, batch [655360/756895]\n",
      "loss: 0.019189, accuracy: 92.973%, batch [656640/756895]\n",
      "loss: 0.014032, accuracy: 93.161%, batch [657920/756895]\n",
      "loss: 0.026938, accuracy: 92.864%, batch [659200/756895]\n",
      "loss: 0.017247, accuracy: 93.216%, batch [660480/756895]\n",
      "loss: 0.019265, accuracy: 92.949%, batch [661760/756895]\n",
      "loss: 0.020722, accuracy: 92.994%, batch [663040/756895]\n",
      "loss: 0.022385, accuracy: 92.902%, batch [664320/756895]\n",
      "loss: 0.017301, accuracy: 93.125%, batch [665600/756895]\n",
      "loss: 0.016304, accuracy: 93.234%, batch [666880/756895]\n",
      "loss: 0.014241, accuracy: 93.344%, batch [668160/756895]\n",
      "loss: 0.016450, accuracy: 93.146%, batch [669440/756895]\n",
      "loss: 0.016731, accuracy: 93.067%, batch [670720/756895]\n",
      "loss: 0.016641, accuracy: 93.118%, batch [672000/756895]\n",
      "loss: 0.015155, accuracy: 93.131%, batch [673280/756895]\n",
      "loss: 0.018429, accuracy: 93.063%, batch [674560/756895]\n",
      "loss: 0.018530, accuracy: 93.050%, batch [675840/756895]\n",
      "loss: 0.015298, accuracy: 93.104%, batch [677120/756895]\n",
      "loss: 0.016557, accuracy: 93.391%, batch [678400/756895]\n",
      "loss: 0.023847, accuracy: 92.841%, batch [679680/756895]\n",
      "loss: 0.016183, accuracy: 93.122%, batch [680960/756895]\n",
      "loss: 0.017835, accuracy: 92.991%, batch [682240/756895]\n",
      "loss: 0.022564, accuracy: 92.970%, batch [683520/756895]\n",
      "loss: 0.020211, accuracy: 92.994%, batch [684800/756895]\n",
      "loss: 0.024229, accuracy: 92.780%, batch [686080/756895]\n",
      "loss: 0.019265, accuracy: 92.989%, batch [687360/756895]\n",
      "loss: 0.019801, accuracy: 92.945%, batch [688640/756895]\n",
      "loss: 0.018631, accuracy: 93.138%, batch [689920/756895]\n",
      "loss: 0.015497, accuracy: 93.127%, batch [691200/756895]\n",
      "loss: 0.025318, accuracy: 92.742%, batch [692480/756895]\n",
      "loss: 0.023037, accuracy: 93.054%, batch [693760/756895]\n",
      "loss: 0.016683, accuracy: 92.996%, batch [695040/756895]\n",
      "loss: 0.019215, accuracy: 93.152%, batch [696320/756895]\n",
      "loss: 0.023220, accuracy: 92.809%, batch [697600/756895]\n",
      "loss: 0.019143, accuracy: 93.090%, batch [698880/756895]\n",
      "loss: 0.017379, accuracy: 93.212%, batch [700160/756895]\n",
      "loss: 0.020140, accuracy: 93.137%, batch [701440/756895]\n",
      "loss: 0.019908, accuracy: 92.963%, batch [702720/756895]\n",
      "loss: 0.018659, accuracy: 93.110%, batch [704000/756895]\n",
      "loss: 0.019721, accuracy: 93.031%, batch [705280/756895]\n",
      "loss: 0.018319, accuracy: 93.054%, batch [706560/756895]\n",
      "loss: 0.018464, accuracy: 93.094%, batch [707840/756895]\n",
      "loss: 0.017770, accuracy: 92.899%, batch [709120/756895]\n",
      "loss: 0.017023, accuracy: 93.207%, batch [710400/756895]\n",
      "loss: 0.017575, accuracy: 92.975%, batch [711680/756895]\n",
      "loss: 0.020342, accuracy: 93.048%, batch [712960/756895]\n",
      "loss: 0.018347, accuracy: 92.982%, batch [714240/756895]\n",
      "loss: 0.015361, accuracy: 93.247%, batch [715520/756895]\n",
      "loss: 0.025551, accuracy: 92.860%, batch [716800/756895]\n",
      "loss: 0.018844, accuracy: 93.002%, batch [718080/756895]\n",
      "loss: 0.020849, accuracy: 92.796%, batch [719360/756895]\n",
      "loss: 0.019599, accuracy: 93.049%, batch [720640/756895]\n",
      "loss: 0.019313, accuracy: 93.027%, batch [721920/756895]\n",
      "loss: 0.017306, accuracy: 93.007%, batch [723200/756895]\n",
      "loss: 0.026446, accuracy: 93.010%, batch [724480/756895]\n",
      "loss: 0.017758, accuracy: 93.029%, batch [725760/756895]\n",
      "loss: 0.016624, accuracy: 92.945%, batch [727040/756895]\n",
      "loss: 0.022003, accuracy: 93.058%, batch [728320/756895]\n",
      "loss: 0.017188, accuracy: 93.254%, batch [729600/756895]\n",
      "loss: 0.021916, accuracy: 92.804%, batch [730880/756895]\n",
      "loss: 0.021478, accuracy: 93.099%, batch [732160/756895]\n",
      "loss: 0.020773, accuracy: 93.010%, batch [733440/756895]\n",
      "loss: 0.020772, accuracy: 92.961%, batch [734720/756895]\n",
      "loss: 0.023411, accuracy: 92.909%, batch [736000/756895]\n",
      "loss: 0.017558, accuracy: 93.103%, batch [737280/756895]\n",
      "loss: 0.018665, accuracy: 92.862%, batch [738560/756895]\n",
      "loss: 0.018410, accuracy: 93.092%, batch [739840/756895]\n",
      "loss: 0.019872, accuracy: 92.876%, batch [741120/756895]\n",
      "loss: 0.025344, accuracy: 92.973%, batch [742400/756895]\n",
      "loss: 0.019824, accuracy: 92.964%, batch [743680/756895]\n",
      "loss: 0.018281, accuracy: 93.087%, batch [744960/756895]\n",
      "loss: 0.028803, accuracy: 93.070%, batch [746240/756895]\n",
      "loss: 0.015837, accuracy: 93.317%, batch [747520/756895]\n",
      "loss: 0.022702, accuracy: 92.802%, batch [748800/756895]\n",
      "loss: 0.017069, accuracy: 93.305%, batch [750080/756895]\n",
      "loss: 0.019819, accuracy: 93.126%, batch [751360/756895]\n",
      "loss: 0.021302, accuracy: 92.990%, batch [752640/756895]\n",
      "loss: 0.014207, accuracy: 93.302%, batch [753920/756895]\n",
      "loss: 0.022404, accuracy: 93.009%, batch [755200/756895]\n",
      "loss: 0.021713, accuracy: 92.992%, batch [756480/756895]\n",
      "Test avg loss: 0.020327, test avg accuracy: 92.995% \n",
      "\n",
      "Test avg loss: 0.020060, test avg accuracy: 93.023% \n",
      "\n",
      "Epoch 109\n",
      "------------------------\n",
      "loss: 0.020418, accuracy: 92.833%, batch [    0/756895]\n",
      "loss: 0.017227, accuracy: 93.065%, batch [ 1280/756895]\n",
      "loss: 0.017647, accuracy: 93.063%, batch [ 2560/756895]\n",
      "loss: 0.017664, accuracy: 93.212%, batch [ 3840/756895]\n",
      "loss: 0.017511, accuracy: 93.088%, batch [ 5120/756895]\n",
      "loss: 0.014680, accuracy: 93.287%, batch [ 6400/756895]\n",
      "loss: 0.020064, accuracy: 92.906%, batch [ 7680/756895]\n",
      "loss: 0.023073, accuracy: 93.191%, batch [ 8960/756895]\n",
      "loss: 0.014777, accuracy: 93.267%, batch [10240/756895]\n",
      "loss: 0.021315, accuracy: 92.960%, batch [11520/756895]\n",
      "loss: 0.015036, accuracy: 93.228%, batch [12800/756895]\n",
      "loss: 0.023075, accuracy: 93.075%, batch [14080/756895]\n",
      "loss: 0.018416, accuracy: 93.055%, batch [15360/756895]\n",
      "loss: 0.016594, accuracy: 93.237%, batch [16640/756895]\n",
      "loss: 0.018181, accuracy: 92.991%, batch [17920/756895]\n",
      "loss: 0.014754, accuracy: 93.100%, batch [19200/756895]\n",
      "loss: 0.020593, accuracy: 93.084%, batch [20480/756895]\n",
      "loss: 0.022111, accuracy: 93.039%, batch [21760/756895]\n",
      "loss: 0.015524, accuracy: 93.265%, batch [23040/756895]\n",
      "loss: 0.015636, accuracy: 93.151%, batch [24320/756895]\n",
      "loss: 0.016137, accuracy: 93.205%, batch [25600/756895]\n",
      "loss: 0.014762, accuracy: 93.200%, batch [26880/756895]\n",
      "loss: 0.021011, accuracy: 93.012%, batch [28160/756895]\n",
      "loss: 0.020400, accuracy: 92.953%, batch [29440/756895]\n",
      "loss: 0.024773, accuracy: 92.959%, batch [30720/756895]\n",
      "loss: 0.016420, accuracy: 92.955%, batch [32000/756895]\n",
      "loss: 0.016647, accuracy: 93.030%, batch [33280/756895]\n",
      "loss: 0.016471, accuracy: 93.177%, batch [34560/756895]\n",
      "loss: 0.020649, accuracy: 92.989%, batch [35840/756895]\n",
      "loss: 0.018496, accuracy: 93.002%, batch [37120/756895]\n",
      "loss: 0.018476, accuracy: 93.053%, batch [38400/756895]\n",
      "loss: 0.020667, accuracy: 93.145%, batch [39680/756895]\n",
      "loss: 0.014532, accuracy: 93.280%, batch [40960/756895]\n",
      "loss: 0.016815, accuracy: 93.128%, batch [42240/756895]\n",
      "loss: 0.020894, accuracy: 92.997%, batch [43520/756895]\n",
      "loss: 0.017863, accuracy: 93.197%, batch [44800/756895]\n",
      "loss: 0.017325, accuracy: 93.227%, batch [46080/756895]\n",
      "loss: 0.022284, accuracy: 93.031%, batch [47360/756895]\n",
      "loss: 0.021708, accuracy: 92.927%, batch [48640/756895]\n",
      "loss: 0.017453, accuracy: 93.132%, batch [49920/756895]\n",
      "loss: 0.017355, accuracy: 93.155%, batch [51200/756895]\n",
      "loss: 0.014582, accuracy: 93.238%, batch [52480/756895]\n",
      "loss: 0.020462, accuracy: 92.965%, batch [53760/756895]\n",
      "loss: 0.016442, accuracy: 93.025%, batch [55040/756895]\n",
      "loss: 0.017879, accuracy: 92.946%, batch [56320/756895]\n",
      "loss: 0.018512, accuracy: 93.163%, batch [57600/756895]\n",
      "loss: 0.021641, accuracy: 92.931%, batch [58880/756895]\n",
      "loss: 0.017037, accuracy: 93.272%, batch [60160/756895]\n",
      "loss: 0.022517, accuracy: 92.876%, batch [61440/756895]\n",
      "loss: 0.021055, accuracy: 92.852%, batch [62720/756895]\n",
      "loss: 0.017614, accuracy: 93.027%, batch [64000/756895]\n",
      "loss: 0.021478, accuracy: 92.922%, batch [65280/756895]\n",
      "loss: 0.016874, accuracy: 93.206%, batch [66560/756895]\n",
      "loss: 0.018104, accuracy: 93.105%, batch [67840/756895]\n",
      "loss: 0.019523, accuracy: 92.910%, batch [69120/756895]\n",
      "loss: 0.018093, accuracy: 93.161%, batch [70400/756895]\n",
      "loss: 0.019158, accuracy: 92.922%, batch [71680/756895]\n",
      "loss: 0.026345, accuracy: 92.904%, batch [72960/756895]\n",
      "loss: 0.017269, accuracy: 93.156%, batch [74240/756895]\n",
      "loss: 0.021215, accuracy: 92.992%, batch [75520/756895]\n",
      "loss: 0.029228, accuracy: 92.650%, batch [76800/756895]\n",
      "loss: 0.022071, accuracy: 92.827%, batch [78080/756895]\n",
      "loss: 0.023218, accuracy: 92.960%, batch [79360/756895]\n",
      "loss: 0.015631, accuracy: 93.287%, batch [80640/756895]\n",
      "loss: 0.020382, accuracy: 93.075%, batch [81920/756895]\n",
      "loss: 0.018160, accuracy: 92.976%, batch [83200/756895]\n",
      "loss: 0.016166, accuracy: 93.209%, batch [84480/756895]\n",
      "loss: 0.021219, accuracy: 92.931%, batch [85760/756895]\n",
      "loss: 0.017937, accuracy: 93.137%, batch [87040/756895]\n",
      "loss: 0.019128, accuracy: 93.118%, batch [88320/756895]\n",
      "loss: 0.016906, accuracy: 92.993%, batch [89600/756895]\n",
      "loss: 0.015316, accuracy: 93.190%, batch [90880/756895]\n",
      "loss: 0.017523, accuracy: 93.059%, batch [92160/756895]\n",
      "loss: 0.019619, accuracy: 93.058%, batch [93440/756895]\n",
      "loss: 0.019889, accuracy: 93.022%, batch [94720/756895]\n",
      "loss: 0.015256, accuracy: 93.229%, batch [96000/756895]\n",
      "loss: 0.017759, accuracy: 92.997%, batch [97280/756895]\n",
      "loss: 0.027491, accuracy: 92.788%, batch [98560/756895]\n",
      "loss: 0.022683, accuracy: 93.028%, batch [99840/756895]\n",
      "loss: 0.016613, accuracy: 92.961%, batch [101120/756895]\n",
      "loss: 0.017035, accuracy: 93.098%, batch [102400/756895]\n",
      "loss: 0.017533, accuracy: 93.115%, batch [103680/756895]\n",
      "loss: 0.017673, accuracy: 93.186%, batch [104960/756895]\n",
      "loss: 0.018933, accuracy: 93.185%, batch [106240/756895]\n",
      "loss: 0.014893, accuracy: 93.221%, batch [107520/756895]\n",
      "loss: 0.023884, accuracy: 92.857%, batch [108800/756895]\n",
      "loss: 0.018243, accuracy: 93.035%, batch [110080/756895]\n",
      "loss: 0.019451, accuracy: 92.976%, batch [111360/756895]\n",
      "loss: 0.017778, accuracy: 93.011%, batch [112640/756895]\n",
      "loss: 0.015254, accuracy: 93.391%, batch [113920/756895]\n",
      "loss: 0.016272, accuracy: 93.242%, batch [115200/756895]\n",
      "loss: 0.019615, accuracy: 93.049%, batch [116480/756895]\n",
      "loss: 0.022008, accuracy: 92.935%, batch [117760/756895]\n",
      "loss: 0.018624, accuracy: 92.947%, batch [119040/756895]\n",
      "loss: 0.021306, accuracy: 92.998%, batch [120320/756895]\n",
      "loss: 0.021642, accuracy: 92.937%, batch [121600/756895]\n",
      "loss: 0.017998, accuracy: 92.878%, batch [122880/756895]\n",
      "loss: 0.022080, accuracy: 92.986%, batch [124160/756895]\n",
      "loss: 0.016156, accuracy: 93.137%, batch [125440/756895]\n",
      "loss: 0.021513, accuracy: 92.943%, batch [126720/756895]\n",
      "loss: 0.021110, accuracy: 93.011%, batch [128000/756895]\n",
      "loss: 0.021022, accuracy: 92.949%, batch [129280/756895]\n",
      "loss: 0.020960, accuracy: 92.988%, batch [130560/756895]\n",
      "loss: 0.019887, accuracy: 92.868%, batch [131840/756895]\n",
      "loss: 0.015237, accuracy: 93.076%, batch [133120/756895]\n",
      "loss: 0.017639, accuracy: 93.145%, batch [134400/756895]\n",
      "loss: 0.019249, accuracy: 92.952%, batch [135680/756895]\n",
      "loss: 0.024398, accuracy: 92.981%, batch [136960/756895]\n",
      "loss: 0.018580, accuracy: 93.013%, batch [138240/756895]\n",
      "loss: 0.016394, accuracy: 93.093%, batch [139520/756895]\n",
      "loss: 0.022296, accuracy: 92.884%, batch [140800/756895]\n",
      "loss: 0.016513, accuracy: 93.069%, batch [142080/756895]\n",
      "loss: 0.017816, accuracy: 93.219%, batch [143360/756895]\n",
      "loss: 0.016919, accuracy: 93.201%, batch [144640/756895]\n",
      "loss: 0.017430, accuracy: 93.135%, batch [145920/756895]\n",
      "loss: 0.015193, accuracy: 93.101%, batch [147200/756895]\n",
      "loss: 0.021995, accuracy: 92.872%, batch [148480/756895]\n",
      "loss: 0.018442, accuracy: 93.024%, batch [149760/756895]\n",
      "loss: 0.017099, accuracy: 93.102%, batch [151040/756895]\n",
      "loss: 0.017318, accuracy: 93.244%, batch [152320/756895]\n",
      "loss: 0.017974, accuracy: 93.034%, batch [153600/756895]\n",
      "loss: 0.015763, accuracy: 93.208%, batch [154880/756895]\n",
      "loss: 0.018649, accuracy: 93.017%, batch [156160/756895]\n",
      "loss: 0.018212, accuracy: 93.027%, batch [157440/756895]\n",
      "loss: 0.019969, accuracy: 92.985%, batch [158720/756895]\n",
      "loss: 0.016038, accuracy: 93.253%, batch [160000/756895]\n",
      "loss: 0.016003, accuracy: 93.249%, batch [161280/756895]\n",
      "loss: 0.018610, accuracy: 93.061%, batch [162560/756895]\n",
      "loss: 0.019979, accuracy: 93.056%, batch [163840/756895]\n",
      "loss: 0.017329, accuracy: 93.106%, batch [165120/756895]\n",
      "loss: 0.021114, accuracy: 93.044%, batch [166400/756895]\n",
      "loss: 0.019721, accuracy: 93.186%, batch [167680/756895]\n",
      "loss: 0.019957, accuracy: 92.920%, batch [168960/756895]\n",
      "loss: 0.017539, accuracy: 92.940%, batch [170240/756895]\n",
      "loss: 0.014838, accuracy: 93.175%, batch [171520/756895]\n",
      "loss: 0.016938, accuracy: 93.166%, batch [172800/756895]\n",
      "loss: 0.020125, accuracy: 93.006%, batch [174080/756895]\n",
      "loss: 0.017663, accuracy: 93.017%, batch [175360/756895]\n",
      "loss: 0.017087, accuracy: 93.092%, batch [176640/756895]\n",
      "loss: 0.021452, accuracy: 92.887%, batch [177920/756895]\n",
      "loss: 0.018727, accuracy: 93.008%, batch [179200/756895]\n",
      "loss: 0.017628, accuracy: 93.172%, batch [180480/756895]\n",
      "loss: 0.016703, accuracy: 93.124%, batch [181760/756895]\n",
      "loss: 0.017775, accuracy: 93.049%, batch [183040/756895]\n",
      "loss: 0.019401, accuracy: 93.284%, batch [184320/756895]\n",
      "loss: 0.018516, accuracy: 93.078%, batch [185600/756895]\n",
      "loss: 0.020156, accuracy: 92.967%, batch [186880/756895]\n",
      "loss: 0.016182, accuracy: 93.210%, batch [188160/756895]\n",
      "loss: 0.019598, accuracy: 93.233%, batch [189440/756895]\n",
      "loss: 0.020054, accuracy: 92.921%, batch [190720/756895]\n",
      "loss: 0.019794, accuracy: 93.157%, batch [192000/756895]\n",
      "loss: 0.022516, accuracy: 92.787%, batch [193280/756895]\n",
      "loss: 0.016579, accuracy: 93.207%, batch [194560/756895]\n",
      "loss: 0.025009, accuracy: 92.812%, batch [195840/756895]\n",
      "loss: 0.016666, accuracy: 93.172%, batch [197120/756895]\n",
      "loss: 0.016253, accuracy: 93.095%, batch [198400/756895]\n",
      "loss: 0.023111, accuracy: 92.935%, batch [199680/756895]\n",
      "loss: 0.022229, accuracy: 92.996%, batch [200960/756895]\n",
      "loss: 0.022817, accuracy: 92.871%, batch [202240/756895]\n",
      "loss: 0.022893, accuracy: 92.928%, batch [203520/756895]\n",
      "loss: 0.024065, accuracy: 93.081%, batch [204800/756895]\n",
      "loss: 0.017923, accuracy: 93.146%, batch [206080/756895]\n",
      "loss: 0.015941, accuracy: 93.074%, batch [207360/756895]\n",
      "loss: 0.014904, accuracy: 93.198%, batch [208640/756895]\n",
      "loss: 0.018393, accuracy: 92.881%, batch [209920/756895]\n",
      "loss: 0.018945, accuracy: 93.052%, batch [211200/756895]\n",
      "loss: 0.021623, accuracy: 92.979%, batch [212480/756895]\n",
      "loss: 0.020994, accuracy: 92.995%, batch [213760/756895]\n",
      "loss: 0.015653, accuracy: 93.207%, batch [215040/756895]\n",
      "loss: 0.016353, accuracy: 93.064%, batch [216320/756895]\n",
      "loss: 0.016865, accuracy: 93.175%, batch [217600/756895]\n",
      "loss: 0.016073, accuracy: 93.158%, batch [218880/756895]\n",
      "loss: 0.016026, accuracy: 93.191%, batch [220160/756895]\n",
      "loss: 0.017953, accuracy: 92.988%, batch [221440/756895]\n",
      "loss: 0.018136, accuracy: 93.162%, batch [222720/756895]\n",
      "loss: 0.021200, accuracy: 93.033%, batch [224000/756895]\n",
      "loss: 0.014693, accuracy: 93.355%, batch [225280/756895]\n",
      "loss: 0.022847, accuracy: 92.760%, batch [226560/756895]\n",
      "loss: 0.017317, accuracy: 93.142%, batch [227840/756895]\n",
      "loss: 0.022353, accuracy: 93.046%, batch [229120/756895]\n",
      "loss: 0.019627, accuracy: 92.872%, batch [230400/756895]\n",
      "loss: 0.019298, accuracy: 93.072%, batch [231680/756895]\n",
      "loss: 0.017028, accuracy: 93.101%, batch [232960/756895]\n",
      "loss: 0.017087, accuracy: 93.228%, batch [234240/756895]\n",
      "loss: 0.018494, accuracy: 93.040%, batch [235520/756895]\n",
      "loss: 0.019065, accuracy: 93.202%, batch [236800/756895]\n",
      "loss: 0.016229, accuracy: 93.204%, batch [238080/756895]\n",
      "loss: 0.015805, accuracy: 93.077%, batch [239360/756895]\n",
      "loss: 0.022501, accuracy: 93.006%, batch [240640/756895]\n",
      "loss: 0.021397, accuracy: 93.045%, batch [241920/756895]\n",
      "loss: 0.019711, accuracy: 92.914%, batch [243200/756895]\n",
      "loss: 0.018462, accuracy: 92.900%, batch [244480/756895]\n",
      "loss: 0.013185, accuracy: 93.249%, batch [245760/756895]\n",
      "loss: 0.024611, accuracy: 92.866%, batch [247040/756895]\n",
      "loss: 0.016946, accuracy: 93.121%, batch [248320/756895]\n",
      "loss: 0.023040, accuracy: 92.824%, batch [249600/756895]\n",
      "loss: 0.018226, accuracy: 93.190%, batch [250880/756895]\n",
      "loss: 0.018329, accuracy: 93.083%, batch [252160/756895]\n",
      "loss: 0.019787, accuracy: 93.198%, batch [253440/756895]\n",
      "loss: 0.023721, accuracy: 93.061%, batch [254720/756895]\n",
      "loss: 0.021284, accuracy: 92.877%, batch [256000/756895]\n",
      "loss: 0.015328, accuracy: 93.134%, batch [257280/756895]\n",
      "loss: 0.020999, accuracy: 92.935%, batch [258560/756895]\n",
      "loss: 0.018508, accuracy: 93.152%, batch [259840/756895]\n",
      "loss: 0.016985, accuracy: 93.190%, batch [261120/756895]\n",
      "loss: 0.028003, accuracy: 92.919%, batch [262400/756895]\n",
      "loss: 0.018009, accuracy: 92.909%, batch [263680/756895]\n",
      "loss: 0.025560, accuracy: 93.046%, batch [264960/756895]\n",
      "loss: 0.021164, accuracy: 92.983%, batch [266240/756895]\n",
      "loss: 0.023359, accuracy: 92.949%, batch [267520/756895]\n",
      "loss: 0.015168, accuracy: 93.091%, batch [268800/756895]\n",
      "loss: 0.016569, accuracy: 93.363%, batch [270080/756895]\n",
      "loss: 0.028071, accuracy: 92.953%, batch [271360/756895]\n",
      "loss: 0.018596, accuracy: 93.090%, batch [272640/756895]\n",
      "loss: 0.019425, accuracy: 92.878%, batch [273920/756895]\n",
      "loss: 0.016455, accuracy: 93.134%, batch [275200/756895]\n",
      "loss: 0.024634, accuracy: 92.803%, batch [276480/756895]\n",
      "loss: 0.014807, accuracy: 93.225%, batch [277760/756895]\n",
      "loss: 0.018745, accuracy: 92.989%, batch [279040/756895]\n",
      "loss: 0.020254, accuracy: 93.144%, batch [280320/756895]\n",
      "loss: 0.020314, accuracy: 92.899%, batch [281600/756895]\n",
      "loss: 0.020066, accuracy: 92.884%, batch [282880/756895]\n",
      "loss: 0.017605, accuracy: 92.879%, batch [284160/756895]\n",
      "loss: 0.018589, accuracy: 93.051%, batch [285440/756895]\n",
      "loss: 0.019284, accuracy: 93.114%, batch [286720/756895]\n",
      "loss: 0.015918, accuracy: 93.108%, batch [288000/756895]\n",
      "loss: 0.019811, accuracy: 92.875%, batch [289280/756895]\n",
      "loss: 0.016017, accuracy: 93.007%, batch [290560/756895]\n",
      "loss: 0.021550, accuracy: 92.982%, batch [291840/756895]\n",
      "loss: 0.019878, accuracy: 93.083%, batch [293120/756895]\n",
      "loss: 0.017439, accuracy: 93.148%, batch [294400/756895]\n",
      "loss: 0.020373, accuracy: 93.074%, batch [295680/756895]\n",
      "loss: 0.021839, accuracy: 93.008%, batch [296960/756895]\n",
      "loss: 0.017113, accuracy: 93.252%, batch [298240/756895]\n",
      "loss: 0.019172, accuracy: 93.068%, batch [299520/756895]\n",
      "loss: 0.023463, accuracy: 92.835%, batch [300800/756895]\n",
      "loss: 0.016570, accuracy: 93.226%, batch [302080/756895]\n",
      "loss: 0.017224, accuracy: 93.107%, batch [303360/756895]\n",
      "loss: 0.016432, accuracy: 93.179%, batch [304640/756895]\n",
      "loss: 0.020223, accuracy: 93.016%, batch [305920/756895]\n",
      "loss: 0.021680, accuracy: 92.951%, batch [307200/756895]\n",
      "loss: 0.029113, accuracy: 92.863%, batch [308480/756895]\n",
      "loss: 0.015059, accuracy: 93.323%, batch [309760/756895]\n",
      "loss: 0.020102, accuracy: 93.120%, batch [311040/756895]\n",
      "loss: 0.016978, accuracy: 93.196%, batch [312320/756895]\n",
      "loss: 0.014413, accuracy: 93.257%, batch [313600/756895]\n",
      "loss: 0.023237, accuracy: 92.893%, batch [314880/756895]\n",
      "loss: 0.019822, accuracy: 93.192%, batch [316160/756895]\n",
      "loss: 0.019323, accuracy: 93.047%, batch [317440/756895]\n",
      "loss: 0.017085, accuracy: 93.249%, batch [318720/756895]\n",
      "loss: 0.015471, accuracy: 93.116%, batch [320000/756895]\n",
      "loss: 0.020054, accuracy: 93.069%, batch [321280/756895]\n",
      "loss: 0.015869, accuracy: 93.197%, batch [322560/756895]\n",
      "loss: 0.018279, accuracy: 93.036%, batch [323840/756895]\n",
      "loss: 0.017391, accuracy: 93.174%, batch [325120/756895]\n",
      "loss: 0.019035, accuracy: 93.113%, batch [326400/756895]\n",
      "loss: 0.020779, accuracy: 92.988%, batch [327680/756895]\n",
      "loss: 0.017467, accuracy: 93.072%, batch [328960/756895]\n",
      "loss: 0.023450, accuracy: 92.925%, batch [330240/756895]\n",
      "loss: 0.021630, accuracy: 93.118%, batch [331520/756895]\n",
      "loss: 0.022227, accuracy: 93.052%, batch [332800/756895]\n",
      "loss: 0.020748, accuracy: 92.972%, batch [334080/756895]\n",
      "loss: 0.017422, accuracy: 92.969%, batch [335360/756895]\n",
      "loss: 0.017147, accuracy: 93.368%, batch [336640/756895]\n",
      "loss: 0.017850, accuracy: 93.078%, batch [337920/756895]\n",
      "loss: 0.025749, accuracy: 92.892%, batch [339200/756895]\n",
      "loss: 0.016956, accuracy: 93.194%, batch [340480/756895]\n",
      "loss: 0.019612, accuracy: 92.777%, batch [341760/756895]\n",
      "loss: 0.019528, accuracy: 92.923%, batch [343040/756895]\n",
      "loss: 0.018010, accuracy: 93.196%, batch [344320/756895]\n",
      "loss: 0.026236, accuracy: 92.632%, batch [345600/756895]\n",
      "loss: 0.022923, accuracy: 92.917%, batch [346880/756895]\n",
      "loss: 0.022530, accuracy: 92.847%, batch [348160/756895]\n",
      "loss: 0.020115, accuracy: 93.135%, batch [349440/756895]\n",
      "loss: 0.018558, accuracy: 92.978%, batch [350720/756895]\n",
      "loss: 0.017429, accuracy: 93.234%, batch [352000/756895]\n",
      "loss: 0.021083, accuracy: 93.058%, batch [353280/756895]\n",
      "loss: 0.018919, accuracy: 93.014%, batch [354560/756895]\n",
      "loss: 0.023150, accuracy: 92.905%, batch [355840/756895]\n",
      "loss: 0.015254, accuracy: 93.283%, batch [357120/756895]\n",
      "loss: 0.021409, accuracy: 92.928%, batch [358400/756895]\n",
      "loss: 0.016884, accuracy: 93.271%, batch [359680/756895]\n",
      "loss: 0.017230, accuracy: 93.099%, batch [360960/756895]\n",
      "loss: 0.017590, accuracy: 93.192%, batch [362240/756895]\n",
      "loss: 0.017914, accuracy: 93.063%, batch [363520/756895]\n",
      "loss: 0.016044, accuracy: 93.160%, batch [364800/756895]\n",
      "loss: 0.021827, accuracy: 93.141%, batch [366080/756895]\n",
      "loss: 0.016088, accuracy: 93.112%, batch [367360/756895]\n",
      "loss: 0.020656, accuracy: 92.947%, batch [368640/756895]\n",
      "loss: 0.015896, accuracy: 93.089%, batch [369920/756895]\n",
      "loss: 0.019062, accuracy: 93.114%, batch [371200/756895]\n",
      "loss: 0.017315, accuracy: 93.100%, batch [372480/756895]\n",
      "loss: 0.025676, accuracy: 92.797%, batch [373760/756895]\n",
      "loss: 0.022520, accuracy: 93.102%, batch [375040/756895]\n",
      "loss: 0.015773, accuracy: 93.115%, batch [376320/756895]\n",
      "loss: 0.019344, accuracy: 93.174%, batch [377600/756895]\n",
      "loss: 0.024643, accuracy: 92.932%, batch [378880/756895]\n",
      "loss: 0.017711, accuracy: 93.085%, batch [380160/756895]\n",
      "loss: 0.022217, accuracy: 92.965%, batch [381440/756895]\n",
      "loss: 0.017798, accuracy: 93.163%, batch [382720/756895]\n",
      "loss: 0.023037, accuracy: 93.146%, batch [384000/756895]\n",
      "loss: 0.024347, accuracy: 92.854%, batch [385280/756895]\n",
      "loss: 0.017415, accuracy: 93.130%, batch [386560/756895]\n",
      "loss: 0.018228, accuracy: 93.131%, batch [387840/756895]\n",
      "loss: 0.019803, accuracy: 93.117%, batch [389120/756895]\n",
      "loss: 0.017246, accuracy: 92.971%, batch [390400/756895]\n",
      "loss: 0.017807, accuracy: 93.107%, batch [391680/756895]\n",
      "loss: 0.017931, accuracy: 93.330%, batch [392960/756895]\n",
      "loss: 0.022950, accuracy: 92.995%, batch [394240/756895]\n",
      "loss: 0.024761, accuracy: 92.962%, batch [395520/756895]\n",
      "loss: 0.023033, accuracy: 92.833%, batch [396800/756895]\n",
      "loss: 0.021547, accuracy: 93.124%, batch [398080/756895]\n",
      "loss: 0.022709, accuracy: 93.036%, batch [399360/756895]\n",
      "loss: 0.021974, accuracy: 92.981%, batch [400640/756895]\n",
      "loss: 0.017459, accuracy: 92.999%, batch [401920/756895]\n",
      "loss: 0.027459, accuracy: 92.950%, batch [403200/756895]\n",
      "loss: 0.019266, accuracy: 92.916%, batch [404480/756895]\n",
      "loss: 0.018493, accuracy: 93.095%, batch [405760/756895]\n",
      "loss: 0.016961, accuracy: 93.168%, batch [407040/756895]\n",
      "loss: 0.019525, accuracy: 92.975%, batch [408320/756895]\n",
      "loss: 0.020135, accuracy: 92.876%, batch [409600/756895]\n",
      "loss: 0.018716, accuracy: 93.006%, batch [410880/756895]\n",
      "loss: 0.019579, accuracy: 93.122%, batch [412160/756895]\n",
      "loss: 0.022998, accuracy: 92.929%, batch [413440/756895]\n",
      "loss: 0.018014, accuracy: 92.986%, batch [414720/756895]\n",
      "loss: 0.018562, accuracy: 92.980%, batch [416000/756895]\n",
      "loss: 0.021315, accuracy: 93.020%, batch [417280/756895]\n",
      "loss: 0.014837, accuracy: 93.175%, batch [418560/756895]\n",
      "loss: 0.018111, accuracy: 93.186%, batch [419840/756895]\n",
      "loss: 0.019156, accuracy: 93.085%, batch [421120/756895]\n",
      "loss: 0.015765, accuracy: 93.370%, batch [422400/756895]\n",
      "loss: 0.019283, accuracy: 93.047%, batch [423680/756895]\n",
      "loss: 0.019791, accuracy: 93.158%, batch [424960/756895]\n",
      "loss: 0.018754, accuracy: 93.000%, batch [426240/756895]\n",
      "loss: 0.019441, accuracy: 92.989%, batch [427520/756895]\n",
      "loss: 0.017782, accuracy: 93.063%, batch [428800/756895]\n",
      "loss: 0.019470, accuracy: 93.125%, batch [430080/756895]\n",
      "loss: 0.022227, accuracy: 93.042%, batch [431360/756895]\n",
      "loss: 0.021928, accuracy: 93.156%, batch [432640/756895]\n",
      "loss: 0.018994, accuracy: 92.934%, batch [433920/756895]\n",
      "loss: 0.017817, accuracy: 93.046%, batch [435200/756895]\n",
      "loss: 0.017033, accuracy: 93.200%, batch [436480/756895]\n",
      "loss: 0.018061, accuracy: 93.067%, batch [437760/756895]\n",
      "loss: 0.017198, accuracy: 93.127%, batch [439040/756895]\n",
      "loss: 0.020459, accuracy: 92.941%, batch [440320/756895]\n",
      "loss: 0.017964, accuracy: 93.221%, batch [441600/756895]\n",
      "loss: 0.022243, accuracy: 92.955%, batch [442880/756895]\n",
      "loss: 0.015690, accuracy: 93.087%, batch [444160/756895]\n",
      "loss: 0.014742, accuracy: 92.938%, batch [445440/756895]\n",
      "loss: 0.023352, accuracy: 92.908%, batch [446720/756895]\n",
      "loss: 0.026266, accuracy: 92.947%, batch [448000/756895]\n",
      "loss: 0.021451, accuracy: 92.961%, batch [449280/756895]\n",
      "loss: 0.019985, accuracy: 93.072%, batch [450560/756895]\n",
      "loss: 0.025794, accuracy: 92.819%, batch [451840/756895]\n",
      "loss: 0.021984, accuracy: 92.993%, batch [453120/756895]\n",
      "loss: 0.015161, accuracy: 93.498%, batch [454400/756895]\n",
      "loss: 0.017177, accuracy: 93.035%, batch [455680/756895]\n",
      "loss: 0.017460, accuracy: 93.103%, batch [456960/756895]\n",
      "loss: 0.020519, accuracy: 93.054%, batch [458240/756895]\n",
      "loss: 0.018861, accuracy: 93.121%, batch [459520/756895]\n",
      "loss: 0.019110, accuracy: 92.874%, batch [460800/756895]\n",
      "loss: 0.019789, accuracy: 93.016%, batch [462080/756895]\n",
      "loss: 0.015127, accuracy: 93.220%, batch [463360/756895]\n",
      "loss: 0.019089, accuracy: 93.007%, batch [464640/756895]\n",
      "loss: 0.024237, accuracy: 92.973%, batch [465920/756895]\n",
      "loss: 0.018990, accuracy: 93.096%, batch [467200/756895]\n",
      "loss: 0.017522, accuracy: 93.378%, batch [468480/756895]\n",
      "loss: 0.018983, accuracy: 93.059%, batch [469760/756895]\n",
      "loss: 0.023806, accuracy: 92.854%, batch [471040/756895]\n",
      "loss: 0.020810, accuracy: 93.036%, batch [472320/756895]\n",
      "loss: 0.017807, accuracy: 93.079%, batch [473600/756895]\n",
      "loss: 0.017452, accuracy: 93.142%, batch [474880/756895]\n",
      "loss: 0.020463, accuracy: 93.203%, batch [476160/756895]\n",
      "loss: 0.019237, accuracy: 93.077%, batch [477440/756895]\n",
      "loss: 0.018739, accuracy: 93.168%, batch [478720/756895]\n",
      "loss: 0.021399, accuracy: 93.072%, batch [480000/756895]\n",
      "loss: 0.017854, accuracy: 92.915%, batch [481280/756895]\n",
      "loss: 0.014921, accuracy: 93.107%, batch [482560/756895]\n",
      "loss: 0.017056, accuracy: 93.239%, batch [483840/756895]\n",
      "loss: 0.019903, accuracy: 93.185%, batch [485120/756895]\n",
      "loss: 0.017972, accuracy: 93.033%, batch [486400/756895]\n",
      "loss: 0.021743, accuracy: 93.092%, batch [487680/756895]\n",
      "loss: 0.019326, accuracy: 92.904%, batch [488960/756895]\n",
      "loss: 0.018365, accuracy: 93.179%, batch [490240/756895]\n",
      "loss: 0.019347, accuracy: 93.031%, batch [491520/756895]\n",
      "loss: 0.017711, accuracy: 93.043%, batch [492800/756895]\n",
      "loss: 0.024738, accuracy: 92.810%, batch [494080/756895]\n",
      "loss: 0.018844, accuracy: 93.143%, batch [495360/756895]\n",
      "loss: 0.018349, accuracy: 93.173%, batch [496640/756895]\n",
      "loss: 0.020479, accuracy: 93.197%, batch [497920/756895]\n",
      "loss: 0.019198, accuracy: 93.041%, batch [499200/756895]\n",
      "loss: 0.018256, accuracy: 92.960%, batch [500480/756895]\n",
      "loss: 0.018393, accuracy: 92.972%, batch [501760/756895]\n",
      "loss: 0.022836, accuracy: 92.862%, batch [503040/756895]\n",
      "loss: 0.022172, accuracy: 92.965%, batch [504320/756895]\n",
      "loss: 0.015053, accuracy: 93.240%, batch [505600/756895]\n",
      "loss: 0.016161, accuracy: 93.184%, batch [506880/756895]\n",
      "loss: 0.020080, accuracy: 93.015%, batch [508160/756895]\n",
      "loss: 0.016665, accuracy: 93.204%, batch [509440/756895]\n",
      "loss: 0.021778, accuracy: 93.019%, batch [510720/756895]\n",
      "loss: 0.016627, accuracy: 93.117%, batch [512000/756895]\n",
      "loss: 0.018571, accuracy: 93.019%, batch [513280/756895]\n",
      "loss: 0.019515, accuracy: 93.199%, batch [514560/756895]\n",
      "loss: 0.021524, accuracy: 92.987%, batch [515840/756895]\n",
      "loss: 0.016717, accuracy: 93.058%, batch [517120/756895]\n",
      "loss: 0.017673, accuracy: 93.198%, batch [518400/756895]\n",
      "loss: 0.017771, accuracy: 92.963%, batch [519680/756895]\n",
      "loss: 0.021133, accuracy: 93.061%, batch [520960/756895]\n",
      "loss: 0.020099, accuracy: 92.953%, batch [522240/756895]\n",
      "loss: 0.020328, accuracy: 93.107%, batch [523520/756895]\n",
      "loss: 0.021145, accuracy: 92.992%, batch [524800/756895]\n",
      "loss: 0.020913, accuracy: 93.014%, batch [526080/756895]\n",
      "loss: 0.017545, accuracy: 93.135%, batch [527360/756895]\n",
      "loss: 0.014981, accuracy: 93.214%, batch [528640/756895]\n",
      "loss: 0.021478, accuracy: 93.003%, batch [529920/756895]\n",
      "loss: 0.021118, accuracy: 92.991%, batch [531200/756895]\n",
      "loss: 0.015991, accuracy: 93.304%, batch [532480/756895]\n",
      "loss: 0.018272, accuracy: 93.041%, batch [533760/756895]\n",
      "loss: 0.016263, accuracy: 93.200%, batch [535040/756895]\n",
      "loss: 0.018752, accuracy: 93.038%, batch [536320/756895]\n",
      "loss: 0.021068, accuracy: 92.918%, batch [537600/756895]\n",
      "loss: 0.016721, accuracy: 93.200%, batch [538880/756895]\n",
      "loss: 0.018457, accuracy: 93.039%, batch [540160/756895]\n",
      "loss: 0.021636, accuracy: 93.017%, batch [541440/756895]\n",
      "loss: 0.023918, accuracy: 92.931%, batch [542720/756895]\n",
      "loss: 0.019131, accuracy: 93.118%, batch [544000/756895]\n",
      "loss: 0.016687, accuracy: 93.117%, batch [545280/756895]\n",
      "loss: 0.014740, accuracy: 93.201%, batch [546560/756895]\n",
      "loss: 0.022722, accuracy: 93.160%, batch [547840/756895]\n",
      "loss: 0.016516, accuracy: 93.061%, batch [549120/756895]\n",
      "loss: 0.019568, accuracy: 93.061%, batch [550400/756895]\n",
      "loss: 0.017227, accuracy: 93.023%, batch [551680/756895]\n",
      "loss: 0.018272, accuracy: 92.996%, batch [552960/756895]\n",
      "loss: 0.014726, accuracy: 93.307%, batch [554240/756895]\n",
      "loss: 0.017948, accuracy: 93.171%, batch [555520/756895]\n",
      "loss: 0.015585, accuracy: 93.242%, batch [556800/756895]\n",
      "loss: 0.019135, accuracy: 93.119%, batch [558080/756895]\n",
      "loss: 0.024011, accuracy: 92.980%, batch [559360/756895]\n",
      "loss: 0.023320, accuracy: 92.737%, batch [560640/756895]\n",
      "loss: 0.023047, accuracy: 92.838%, batch [561920/756895]\n",
      "loss: 0.016232, accuracy: 93.101%, batch [563200/756895]\n",
      "loss: 0.023375, accuracy: 92.918%, batch [564480/756895]\n",
      "loss: 0.016848, accuracy: 93.317%, batch [565760/756895]\n",
      "loss: 0.017670, accuracy: 92.935%, batch [567040/756895]\n",
      "loss: 0.016517, accuracy: 93.376%, batch [568320/756895]\n",
      "loss: 0.026156, accuracy: 92.936%, batch [569600/756895]\n",
      "loss: 0.019774, accuracy: 93.084%, batch [570880/756895]\n",
      "loss: 0.019868, accuracy: 93.167%, batch [572160/756895]\n",
      "loss: 0.019298, accuracy: 93.048%, batch [573440/756895]\n",
      "loss: 0.018376, accuracy: 93.106%, batch [574720/756895]\n",
      "loss: 0.016845, accuracy: 93.128%, batch [576000/756895]\n",
      "loss: 0.018265, accuracy: 93.018%, batch [577280/756895]\n",
      "loss: 0.018947, accuracy: 92.925%, batch [578560/756895]\n",
      "loss: 0.019971, accuracy: 93.168%, batch [579840/756895]\n",
      "loss: 0.017056, accuracy: 93.191%, batch [581120/756895]\n",
      "loss: 0.015427, accuracy: 93.031%, batch [582400/756895]\n",
      "loss: 0.025640, accuracy: 92.941%, batch [583680/756895]\n",
      "loss: 0.015060, accuracy: 93.201%, batch [584960/756895]\n",
      "loss: 0.021323, accuracy: 93.113%, batch [586240/756895]\n",
      "loss: 0.016543, accuracy: 93.164%, batch [587520/756895]\n",
      "loss: 0.020603, accuracy: 93.072%, batch [588800/756895]\n",
      "loss: 0.021231, accuracy: 93.168%, batch [590080/756895]\n",
      "loss: 0.019061, accuracy: 93.140%, batch [591360/756895]\n",
      "loss: 0.021926, accuracy: 92.937%, batch [592640/756895]\n",
      "loss: 0.016551, accuracy: 93.220%, batch [593920/756895]\n",
      "loss: 0.020682, accuracy: 92.982%, batch [595200/756895]\n",
      "loss: 0.015740, accuracy: 93.055%, batch [596480/756895]\n",
      "loss: 0.019247, accuracy: 92.985%, batch [597760/756895]\n",
      "loss: 0.018446, accuracy: 93.038%, batch [599040/756895]\n",
      "loss: 0.023586, accuracy: 92.852%, batch [600320/756895]\n",
      "loss: 0.014412, accuracy: 93.264%, batch [601600/756895]\n",
      "loss: 0.019968, accuracy: 93.049%, batch [602880/756895]\n",
      "loss: 0.021131, accuracy: 93.083%, batch [604160/756895]\n",
      "loss: 0.023304, accuracy: 92.793%, batch [605440/756895]\n",
      "loss: 0.019595, accuracy: 93.202%, batch [606720/756895]\n",
      "loss: 0.017568, accuracy: 92.987%, batch [608000/756895]\n",
      "loss: 0.019644, accuracy: 93.065%, batch [609280/756895]\n",
      "loss: 0.018382, accuracy: 93.036%, batch [610560/756895]\n",
      "loss: 0.016950, accuracy: 93.240%, batch [611840/756895]\n",
      "loss: 0.015626, accuracy: 93.056%, batch [613120/756895]\n",
      "loss: 0.020446, accuracy: 92.984%, batch [614400/756895]\n",
      "loss: 0.020864, accuracy: 92.817%, batch [615680/756895]\n",
      "loss: 0.016708, accuracy: 93.216%, batch [616960/756895]\n",
      "loss: 0.016473, accuracy: 93.173%, batch [618240/756895]\n",
      "loss: 0.018112, accuracy: 92.998%, batch [619520/756895]\n",
      "loss: 0.022974, accuracy: 92.962%, batch [620800/756895]\n",
      "loss: 0.017819, accuracy: 93.088%, batch [622080/756895]\n",
      "loss: 0.020025, accuracy: 93.024%, batch [623360/756895]\n",
      "loss: 0.022510, accuracy: 92.897%, batch [624640/756895]\n",
      "loss: 0.024680, accuracy: 92.889%, batch [625920/756895]\n",
      "loss: 0.015950, accuracy: 93.324%, batch [627200/756895]\n",
      "loss: 0.016067, accuracy: 93.173%, batch [628480/756895]\n",
      "loss: 0.032813, accuracy: 92.713%, batch [629760/756895]\n",
      "loss: 0.017010, accuracy: 93.148%, batch [631040/756895]\n",
      "loss: 0.018539, accuracy: 93.059%, batch [632320/756895]\n",
      "loss: 0.018583, accuracy: 93.057%, batch [633600/756895]\n",
      "loss: 0.017605, accuracy: 93.274%, batch [634880/756895]\n",
      "loss: 0.019158, accuracy: 93.168%, batch [636160/756895]\n",
      "loss: 0.019129, accuracy: 93.023%, batch [637440/756895]\n",
      "loss: 0.020517, accuracy: 92.971%, batch [638720/756895]\n",
      "loss: 0.018385, accuracy: 93.080%, batch [640000/756895]\n",
      "loss: 0.017404, accuracy: 93.106%, batch [641280/756895]\n",
      "loss: 0.019490, accuracy: 93.078%, batch [642560/756895]\n",
      "loss: 0.018242, accuracy: 92.999%, batch [643840/756895]\n",
      "loss: 0.018759, accuracy: 93.123%, batch [645120/756895]\n",
      "loss: 0.022125, accuracy: 93.025%, batch [646400/756895]\n",
      "loss: 0.020911, accuracy: 93.094%, batch [647680/756895]\n",
      "loss: 0.020027, accuracy: 93.077%, batch [648960/756895]\n",
      "loss: 0.017732, accuracy: 93.005%, batch [650240/756895]\n",
      "loss: 0.017861, accuracy: 93.135%, batch [651520/756895]\n",
      "loss: 0.017978, accuracy: 93.077%, batch [652800/756895]\n",
      "loss: 0.019476, accuracy: 92.965%, batch [654080/756895]\n",
      "loss: 0.025919, accuracy: 92.823%, batch [655360/756895]\n",
      "loss: 0.015617, accuracy: 93.185%, batch [656640/756895]\n",
      "loss: 0.020995, accuracy: 92.838%, batch [657920/756895]\n",
      "loss: 0.015155, accuracy: 93.158%, batch [659200/756895]\n",
      "loss: 0.014280, accuracy: 93.084%, batch [660480/756895]\n",
      "loss: 0.018825, accuracy: 93.064%, batch [661760/756895]\n",
      "loss: 0.018538, accuracy: 93.039%, batch [663040/756895]\n",
      "loss: 0.019622, accuracy: 93.035%, batch [664320/756895]\n",
      "loss: 0.017847, accuracy: 92.806%, batch [665600/756895]\n",
      "loss: 0.013535, accuracy: 93.283%, batch [666880/756895]\n",
      "loss: 0.016372, accuracy: 93.156%, batch [668160/756895]\n",
      "loss: 0.018829, accuracy: 92.907%, batch [669440/756895]\n",
      "loss: 0.016321, accuracy: 93.126%, batch [670720/756895]\n",
      "loss: 0.015497, accuracy: 93.164%, batch [672000/756895]\n",
      "loss: 0.014820, accuracy: 93.364%, batch [673280/756895]\n",
      "loss: 0.019069, accuracy: 93.012%, batch [674560/756895]\n",
      "loss: 0.018559, accuracy: 93.126%, batch [675840/756895]\n",
      "loss: 0.021212, accuracy: 92.978%, batch [677120/756895]\n",
      "loss: 0.018816, accuracy: 93.107%, batch [678400/756895]\n",
      "loss: 0.023349, accuracy: 92.868%, batch [679680/756895]\n",
      "loss: 0.020326, accuracy: 92.953%, batch [680960/756895]\n",
      "loss: 0.017777, accuracy: 93.093%, batch [682240/756895]\n",
      "loss: 0.015686, accuracy: 93.278%, batch [683520/756895]\n",
      "loss: 0.018608, accuracy: 93.062%, batch [684800/756895]\n",
      "loss: 0.026150, accuracy: 92.847%, batch [686080/756895]\n",
      "loss: 0.018642, accuracy: 93.037%, batch [687360/756895]\n",
      "loss: 0.016212, accuracy: 93.093%, batch [688640/756895]\n",
      "loss: 0.020472, accuracy: 93.118%, batch [689920/756895]\n",
      "loss: 0.017306, accuracy: 93.170%, batch [691200/756895]\n",
      "loss: 0.021775, accuracy: 93.097%, batch [692480/756895]\n",
      "loss: 0.019569, accuracy: 92.963%, batch [693760/756895]\n",
      "loss: 0.017125, accuracy: 93.200%, batch [695040/756895]\n",
      "loss: 0.021647, accuracy: 92.910%, batch [696320/756895]\n",
      "loss: 0.021694, accuracy: 92.863%, batch [697600/756895]\n",
      "loss: 0.026419, accuracy: 92.750%, batch [698880/756895]\n",
      "loss: 0.017908, accuracy: 93.038%, batch [700160/756895]\n",
      "loss: 0.023670, accuracy: 92.840%, batch [701440/756895]\n",
      "loss: 0.023555, accuracy: 93.006%, batch [702720/756895]\n",
      "loss: 0.019307, accuracy: 93.164%, batch [704000/756895]\n",
      "loss: 0.022056, accuracy: 92.978%, batch [705280/756895]\n",
      "loss: 0.019466, accuracy: 93.031%, batch [706560/756895]\n",
      "loss: 0.018286, accuracy: 92.987%, batch [707840/756895]\n",
      "loss: 0.020275, accuracy: 92.787%, batch [709120/756895]\n",
      "loss: 0.015025, accuracy: 93.229%, batch [710400/756895]\n",
      "loss: 0.025744, accuracy: 92.855%, batch [711680/756895]\n",
      "loss: 0.024617, accuracy: 92.932%, batch [712960/756895]\n",
      "loss: 0.014911, accuracy: 93.119%, batch [714240/756895]\n",
      "loss: 0.019510, accuracy: 92.945%, batch [715520/756895]\n",
      "loss: 0.017159, accuracy: 93.079%, batch [716800/756895]\n",
      "loss: 0.016948, accuracy: 93.096%, batch [718080/756895]\n",
      "loss: 0.019991, accuracy: 93.002%, batch [719360/756895]\n",
      "loss: 0.022017, accuracy: 93.093%, batch [720640/756895]\n",
      "loss: 0.018030, accuracy: 93.081%, batch [721920/756895]\n",
      "loss: 0.017308, accuracy: 93.058%, batch [723200/756895]\n",
      "loss: 0.017834, accuracy: 93.320%, batch [724480/756895]\n",
      "loss: 0.018009, accuracy: 93.000%, batch [725760/756895]\n",
      "loss: 0.019746, accuracy: 93.013%, batch [727040/756895]\n",
      "loss: 0.019142, accuracy: 93.154%, batch [728320/756895]\n",
      "loss: 0.020941, accuracy: 92.958%, batch [729600/756895]\n",
      "loss: 0.017598, accuracy: 93.040%, batch [730880/756895]\n",
      "loss: 0.019467, accuracy: 92.764%, batch [732160/756895]\n",
      "loss: 0.018161, accuracy: 93.141%, batch [733440/756895]\n",
      "loss: 0.018351, accuracy: 93.013%, batch [734720/756895]\n",
      "loss: 0.020911, accuracy: 92.964%, batch [736000/756895]\n",
      "loss: 0.017715, accuracy: 93.121%, batch [737280/756895]\n",
      "loss: 0.020163, accuracy: 92.929%, batch [738560/756895]\n",
      "loss: 0.016858, accuracy: 93.245%, batch [739840/756895]\n",
      "loss: 0.023418, accuracy: 93.109%, batch [741120/756895]\n",
      "loss: 0.019287, accuracy: 93.151%, batch [742400/756895]\n",
      "loss: 0.019057, accuracy: 93.096%, batch [743680/756895]\n",
      "loss: 0.021903, accuracy: 93.035%, batch [744960/756895]\n",
      "loss: 0.019239, accuracy: 93.224%, batch [746240/756895]\n",
      "loss: 0.023140, accuracy: 92.881%, batch [747520/756895]\n",
      "loss: 0.021554, accuracy: 92.879%, batch [748800/756895]\n",
      "loss: 0.017812, accuracy: 93.166%, batch [750080/756895]\n",
      "loss: 0.023900, accuracy: 92.998%, batch [751360/756895]\n",
      "loss: 0.021315, accuracy: 93.094%, batch [752640/756895]\n",
      "loss: 0.018357, accuracy: 93.137%, batch [753920/756895]\n",
      "loss: 0.017448, accuracy: 92.949%, batch [755200/756895]\n",
      "loss: 0.018305, accuracy: 93.101%, batch [756480/756895]\n",
      "Test avg loss: 0.020438, test avg accuracy: 92.997% \n",
      "\n",
      "Test avg loss: 0.019627, test avg accuracy: 93.042% \n",
      "\n",
      "Epoch 110\n",
      "------------------------\n",
      "loss: 0.020740, accuracy: 92.825%, batch [    0/756895]\n",
      "loss: 0.019713, accuracy: 93.094%, batch [ 1280/756895]\n",
      "loss: 0.022167, accuracy: 92.983%, batch [ 2560/756895]\n",
      "loss: 0.018754, accuracy: 93.135%, batch [ 3840/756895]\n",
      "loss: 0.023022, accuracy: 92.773%, batch [ 5120/756895]\n",
      "loss: 0.019275, accuracy: 92.988%, batch [ 6400/756895]\n",
      "loss: 0.015694, accuracy: 93.201%, batch [ 7680/756895]\n",
      "loss: 0.023007, accuracy: 93.100%, batch [ 8960/756895]\n",
      "loss: 0.018611, accuracy: 92.831%, batch [10240/756895]\n",
      "loss: 0.017561, accuracy: 93.183%, batch [11520/756895]\n",
      "loss: 0.022150, accuracy: 92.903%, batch [12800/756895]\n",
      "loss: 0.025488, accuracy: 92.837%, batch [14080/756895]\n",
      "loss: 0.018115, accuracy: 93.213%, batch [15360/756895]\n",
      "loss: 0.018238, accuracy: 93.123%, batch [16640/756895]\n",
      "loss: 0.013453, accuracy: 93.106%, batch [17920/756895]\n",
      "loss: 0.017806, accuracy: 92.955%, batch [19200/756895]\n",
      "loss: 0.018259, accuracy: 92.907%, batch [20480/756895]\n",
      "loss: 0.017194, accuracy: 93.025%, batch [21760/756895]\n",
      "loss: 0.033226, accuracy: 92.730%, batch [23040/756895]\n",
      "loss: 0.015953, accuracy: 93.083%, batch [24320/756895]\n",
      "loss: 0.016604, accuracy: 93.147%, batch [25600/756895]\n",
      "loss: 0.023158, accuracy: 93.084%, batch [26880/756895]\n",
      "loss: 0.031072, accuracy: 92.819%, batch [28160/756895]\n",
      "loss: 0.020529, accuracy: 92.855%, batch [29440/756895]\n",
      "loss: 0.021743, accuracy: 93.038%, batch [30720/756895]\n",
      "loss: 0.019104, accuracy: 92.877%, batch [32000/756895]\n",
      "loss: 0.019478, accuracy: 93.016%, batch [33280/756895]\n",
      "loss: 0.021459, accuracy: 92.677%, batch [34560/756895]\n",
      "loss: 0.022074, accuracy: 93.043%, batch [35840/756895]\n",
      "loss: 0.019697, accuracy: 93.116%, batch [37120/756895]\n",
      "loss: 0.022101, accuracy: 93.170%, batch [38400/756895]\n",
      "loss: 0.019785, accuracy: 92.952%, batch [39680/756895]\n",
      "loss: 0.020450, accuracy: 92.970%, batch [40960/756895]\n",
      "loss: 0.020831, accuracy: 92.882%, batch [42240/756895]\n",
      "loss: 0.014848, accuracy: 93.314%, batch [43520/756895]\n",
      "loss: 0.020761, accuracy: 92.991%, batch [44800/756895]\n",
      "loss: 0.019575, accuracy: 92.981%, batch [46080/756895]\n",
      "loss: 0.022571, accuracy: 92.947%, batch [47360/756895]\n",
      "loss: 0.020473, accuracy: 92.948%, batch [48640/756895]\n",
      "loss: 0.020854, accuracy: 92.940%, batch [49920/756895]\n",
      "loss: 0.020688, accuracy: 92.662%, batch [51200/756895]\n",
      "loss: 0.015912, accuracy: 93.218%, batch [52480/756895]\n",
      "loss: 0.020670, accuracy: 93.062%, batch [53760/756895]\n",
      "loss: 0.017154, accuracy: 93.202%, batch [55040/756895]\n",
      "loss: 0.020592, accuracy: 93.073%, batch [56320/756895]\n",
      "loss: 0.021152, accuracy: 93.001%, batch [57600/756895]\n",
      "loss: 0.016987, accuracy: 93.225%, batch [58880/756895]\n",
      "loss: 0.016575, accuracy: 93.158%, batch [60160/756895]\n",
      "loss: 0.020308, accuracy: 92.976%, batch [61440/756895]\n",
      "loss: 0.016922, accuracy: 93.085%, batch [62720/756895]\n",
      "loss: 0.017297, accuracy: 92.986%, batch [64000/756895]\n",
      "loss: 0.017534, accuracy: 93.173%, batch [65280/756895]\n",
      "loss: 0.024058, accuracy: 92.841%, batch [66560/756895]\n",
      "loss: 0.015641, accuracy: 93.216%, batch [67840/756895]\n",
      "loss: 0.020354, accuracy: 93.116%, batch [69120/756895]\n",
      "loss: 0.015646, accuracy: 93.044%, batch [70400/756895]\n",
      "loss: 0.016168, accuracy: 93.086%, batch [71680/756895]\n",
      "loss: 0.018053, accuracy: 93.123%, batch [72960/756895]\n",
      "loss: 0.015525, accuracy: 93.174%, batch [74240/756895]\n",
      "loss: 0.021021, accuracy: 92.959%, batch [75520/756895]\n",
      "loss: 0.019556, accuracy: 93.094%, batch [76800/756895]\n",
      "loss: 0.015408, accuracy: 93.321%, batch [78080/756895]\n",
      "loss: 0.017464, accuracy: 92.996%, batch [79360/756895]\n",
      "loss: 0.020445, accuracy: 92.858%, batch [80640/756895]\n",
      "loss: 0.019591, accuracy: 93.189%, batch [81920/756895]\n",
      "loss: 0.020724, accuracy: 92.918%, batch [83200/756895]\n",
      "loss: 0.023630, accuracy: 92.990%, batch [84480/756895]\n",
      "loss: 0.021736, accuracy: 93.228%, batch [85760/756895]\n",
      "loss: 0.019224, accuracy: 93.009%, batch [87040/756895]\n",
      "loss: 0.021949, accuracy: 92.855%, batch [88320/756895]\n",
      "loss: 0.016839, accuracy: 93.121%, batch [89600/756895]\n",
      "loss: 0.016913, accuracy: 93.084%, batch [90880/756895]\n",
      "loss: 0.022270, accuracy: 93.129%, batch [92160/756895]\n",
      "loss: 0.019133, accuracy: 92.935%, batch [93440/756895]\n",
      "loss: 0.016595, accuracy: 93.223%, batch [94720/756895]\n",
      "loss: 0.015713, accuracy: 93.298%, batch [96000/756895]\n",
      "loss: 0.018096, accuracy: 93.094%, batch [97280/756895]\n",
      "loss: 0.023861, accuracy: 93.060%, batch [98560/756895]\n",
      "loss: 0.019074, accuracy: 93.140%, batch [99840/756895]\n",
      "loss: 0.021472, accuracy: 92.962%, batch [101120/756895]\n",
      "loss: 0.015864, accuracy: 93.280%, batch [102400/756895]\n",
      "loss: 0.022461, accuracy: 93.005%, batch [103680/756895]\n",
      "loss: 0.019980, accuracy: 93.154%, batch [104960/756895]\n",
      "loss: 0.015942, accuracy: 93.297%, batch [106240/756895]\n",
      "loss: 0.020073, accuracy: 92.972%, batch [107520/756895]\n",
      "loss: 0.017033, accuracy: 93.093%, batch [108800/756895]\n",
      "loss: 0.017935, accuracy: 93.099%, batch [110080/756895]\n",
      "loss: 0.016674, accuracy: 93.031%, batch [111360/756895]\n",
      "loss: 0.018225, accuracy: 93.209%, batch [112640/756895]\n",
      "loss: 0.018999, accuracy: 92.906%, batch [113920/756895]\n",
      "loss: 0.020387, accuracy: 92.983%, batch [115200/756895]\n",
      "loss: 0.017130, accuracy: 93.247%, batch [116480/756895]\n",
      "loss: 0.013079, accuracy: 93.302%, batch [117760/756895]\n",
      "loss: 0.024813, accuracy: 92.863%, batch [119040/756895]\n",
      "loss: 0.016758, accuracy: 93.065%, batch [120320/756895]\n",
      "loss: 0.015981, accuracy: 93.307%, batch [121600/756895]\n",
      "loss: 0.014809, accuracy: 93.255%, batch [122880/756895]\n",
      "loss: 0.017841, accuracy: 92.959%, batch [124160/756895]\n",
      "loss: 0.020200, accuracy: 93.100%, batch [125440/756895]\n",
      "loss: 0.016880, accuracy: 92.939%, batch [126720/756895]\n",
      "loss: 0.018417, accuracy: 93.056%, batch [128000/756895]\n",
      "loss: 0.019262, accuracy: 93.087%, batch [129280/756895]\n",
      "loss: 0.018892, accuracy: 92.936%, batch [130560/756895]\n",
      "loss: 0.015204, accuracy: 93.277%, batch [131840/756895]\n",
      "loss: 0.017819, accuracy: 93.098%, batch [133120/756895]\n",
      "loss: 0.022298, accuracy: 92.894%, batch [134400/756895]\n",
      "loss: 0.018230, accuracy: 93.231%, batch [135680/756895]\n",
      "loss: 0.017950, accuracy: 93.196%, batch [136960/756895]\n",
      "loss: 0.019295, accuracy: 93.246%, batch [138240/756895]\n",
      "loss: 0.016783, accuracy: 93.127%, batch [139520/756895]\n",
      "loss: 0.019160, accuracy: 92.903%, batch [140800/756895]\n",
      "loss: 0.016730, accuracy: 93.171%, batch [142080/756895]\n",
      "loss: 0.015940, accuracy: 93.160%, batch [143360/756895]\n",
      "loss: 0.021261, accuracy: 93.102%, batch [144640/756895]\n",
      "loss: 0.019584, accuracy: 93.093%, batch [145920/756895]\n",
      "loss: 0.016521, accuracy: 93.197%, batch [147200/756895]\n",
      "loss: 0.016651, accuracy: 93.093%, batch [148480/756895]\n",
      "loss: 0.018471, accuracy: 93.067%, batch [149760/756895]\n",
      "loss: 0.016739, accuracy: 93.028%, batch [151040/756895]\n",
      "loss: 0.017664, accuracy: 93.170%, batch [152320/756895]\n",
      "loss: 0.019287, accuracy: 92.799%, batch [153600/756895]\n",
      "loss: 0.018273, accuracy: 93.140%, batch [154880/756895]\n",
      "loss: 0.018421, accuracy: 93.233%, batch [156160/756895]\n",
      "loss: 0.016392, accuracy: 92.970%, batch [157440/756895]\n",
      "loss: 0.021046, accuracy: 92.959%, batch [158720/756895]\n",
      "loss: 0.020707, accuracy: 93.022%, batch [160000/756895]\n",
      "loss: 0.015193, accuracy: 93.131%, batch [161280/756895]\n",
      "loss: 0.017620, accuracy: 92.989%, batch [162560/756895]\n",
      "loss: 0.018385, accuracy: 93.100%, batch [163840/756895]\n",
      "loss: 0.017722, accuracy: 93.258%, batch [165120/756895]\n",
      "loss: 0.018858, accuracy: 93.101%, batch [166400/756895]\n",
      "loss: 0.017152, accuracy: 93.107%, batch [167680/756895]\n",
      "loss: 0.017733, accuracy: 93.072%, batch [168960/756895]\n",
      "loss: 0.016429, accuracy: 93.304%, batch [170240/756895]\n",
      "loss: 0.021025, accuracy: 92.924%, batch [171520/756895]\n",
      "loss: 0.023906, accuracy: 92.771%, batch [172800/756895]\n",
      "loss: 0.018791, accuracy: 93.089%, batch [174080/756895]\n",
      "loss: 0.027419, accuracy: 92.909%, batch [175360/756895]\n",
      "loss: 0.018212, accuracy: 93.067%, batch [176640/756895]\n",
      "loss: 0.020494, accuracy: 93.054%, batch [177920/756895]\n",
      "loss: 0.028353, accuracy: 92.795%, batch [179200/756895]\n",
      "loss: 0.017530, accuracy: 93.104%, batch [180480/756895]\n",
      "loss: 0.018816, accuracy: 93.149%, batch [181760/756895]\n",
      "loss: 0.015892, accuracy: 93.117%, batch [183040/756895]\n",
      "loss: 0.014707, accuracy: 93.186%, batch [184320/756895]\n",
      "loss: 0.017392, accuracy: 92.936%, batch [185600/756895]\n",
      "loss: 0.023450, accuracy: 93.086%, batch [186880/756895]\n",
      "loss: 0.031474, accuracy: 93.196%, batch [188160/756895]\n",
      "loss: 0.025513, accuracy: 92.701%, batch [189440/756895]\n",
      "loss: 0.019893, accuracy: 93.053%, batch [190720/756895]\n",
      "loss: 0.025338, accuracy: 92.964%, batch [192000/756895]\n",
      "loss: 0.017117, accuracy: 93.017%, batch [193280/756895]\n",
      "loss: 0.016860, accuracy: 92.963%, batch [194560/756895]\n",
      "loss: 0.024449, accuracy: 92.878%, batch [195840/756895]\n",
      "loss: 0.019353, accuracy: 93.204%, batch [197120/756895]\n",
      "loss: 0.016982, accuracy: 93.078%, batch [198400/756895]\n",
      "loss: 0.018511, accuracy: 93.064%, batch [199680/756895]\n",
      "loss: 0.017805, accuracy: 93.127%, batch [200960/756895]\n",
      "loss: 0.017568, accuracy: 93.060%, batch [202240/756895]\n",
      "loss: 0.017279, accuracy: 93.158%, batch [203520/756895]\n",
      "loss: 0.016143, accuracy: 93.217%, batch [204800/756895]\n",
      "loss: 0.017543, accuracy: 93.171%, batch [206080/756895]\n",
      "loss: 0.017144, accuracy: 92.985%, batch [207360/756895]\n",
      "loss: 0.023391, accuracy: 92.976%, batch [208640/756895]\n",
      "loss: 0.020354, accuracy: 92.962%, batch [209920/756895]\n",
      "loss: 0.019199, accuracy: 93.089%, batch [211200/756895]\n",
      "loss: 0.017561, accuracy: 93.022%, batch [212480/756895]\n",
      "loss: 0.017323, accuracy: 93.254%, batch [213760/756895]\n",
      "loss: 0.016074, accuracy: 93.180%, batch [215040/756895]\n",
      "loss: 0.016397, accuracy: 93.101%, batch [216320/756895]\n",
      "loss: 0.019697, accuracy: 93.258%, batch [217600/756895]\n",
      "loss: 0.018406, accuracy: 93.148%, batch [218880/756895]\n",
      "loss: 0.019031, accuracy: 93.115%, batch [220160/756895]\n",
      "loss: 0.022734, accuracy: 93.031%, batch [221440/756895]\n",
      "loss: 0.023640, accuracy: 92.946%, batch [222720/756895]\n",
      "loss: 0.018439, accuracy: 92.903%, batch [224000/756895]\n",
      "loss: 0.017960, accuracy: 93.017%, batch [225280/756895]\n",
      "loss: 0.017333, accuracy: 93.122%, batch [226560/756895]\n",
      "loss: 0.015162, accuracy: 93.169%, batch [227840/756895]\n",
      "loss: 0.021613, accuracy: 92.987%, batch [229120/756895]\n",
      "loss: 0.020569, accuracy: 92.966%, batch [230400/756895]\n",
      "loss: 0.017814, accuracy: 93.085%, batch [231680/756895]\n",
      "loss: 0.016738, accuracy: 93.140%, batch [232960/756895]\n",
      "loss: 0.017523, accuracy: 93.186%, batch [234240/756895]\n",
      "loss: 0.016509, accuracy: 93.181%, batch [235520/756895]\n",
      "loss: 0.016203, accuracy: 93.043%, batch [236800/756895]\n",
      "loss: 0.018789, accuracy: 92.847%, batch [238080/756895]\n",
      "loss: 0.017204, accuracy: 93.232%, batch [239360/756895]\n",
      "loss: 0.016645, accuracy: 93.161%, batch [240640/756895]\n",
      "loss: 0.019055, accuracy: 93.082%, batch [241920/756895]\n",
      "loss: 0.022284, accuracy: 92.864%, batch [243200/756895]\n",
      "loss: 0.022435, accuracy: 92.918%, batch [244480/756895]\n",
      "loss: 0.016400, accuracy: 93.023%, batch [245760/756895]\n",
      "loss: 0.017166, accuracy: 93.177%, batch [247040/756895]\n",
      "loss: 0.015787, accuracy: 93.278%, batch [248320/756895]\n",
      "loss: 0.017202, accuracy: 93.004%, batch [249600/756895]\n",
      "loss: 0.019032, accuracy: 93.321%, batch [250880/756895]\n",
      "loss: 0.017285, accuracy: 93.152%, batch [252160/756895]\n",
      "loss: 0.023896, accuracy: 92.777%, batch [253440/756895]\n",
      "loss: 0.018389, accuracy: 93.126%, batch [254720/756895]\n",
      "loss: 0.015911, accuracy: 93.143%, batch [256000/756895]\n",
      "loss: 0.015415, accuracy: 93.171%, batch [257280/756895]\n",
      "loss: 0.018419, accuracy: 93.124%, batch [258560/756895]\n",
      "loss: 0.018725, accuracy: 93.193%, batch [259840/756895]\n",
      "loss: 0.017533, accuracy: 93.050%, batch [261120/756895]\n",
      "loss: 0.019176, accuracy: 93.024%, batch [262400/756895]\n",
      "loss: 0.017298, accuracy: 93.155%, batch [263680/756895]\n",
      "loss: 0.017776, accuracy: 93.143%, batch [264960/756895]\n",
      "loss: 0.017786, accuracy: 93.084%, batch [266240/756895]\n",
      "loss: 0.018605, accuracy: 93.157%, batch [267520/756895]\n",
      "loss: 0.020252, accuracy: 92.730%, batch [268800/756895]\n",
      "loss: 0.020541, accuracy: 92.946%, batch [270080/756895]\n",
      "loss: 0.014948, accuracy: 93.240%, batch [271360/756895]\n",
      "loss: 0.018423, accuracy: 93.047%, batch [272640/756895]\n",
      "loss: 0.021213, accuracy: 92.859%, batch [273920/756895]\n",
      "loss: 0.024744, accuracy: 92.870%, batch [275200/756895]\n",
      "loss: 0.020761, accuracy: 92.949%, batch [276480/756895]\n",
      "loss: 0.018736, accuracy: 93.109%, batch [277760/756895]\n",
      "loss: 0.022490, accuracy: 92.973%, batch [279040/756895]\n",
      "loss: 0.018429, accuracy: 93.070%, batch [280320/756895]\n",
      "loss: 0.018638, accuracy: 92.989%, batch [281600/756895]\n",
      "loss: 0.015275, accuracy: 93.240%, batch [282880/756895]\n",
      "loss: 0.025574, accuracy: 92.942%, batch [284160/756895]\n",
      "loss: 0.015269, accuracy: 93.260%, batch [285440/756895]\n",
      "loss: 0.014944, accuracy: 93.168%, batch [286720/756895]\n",
      "loss: 0.019502, accuracy: 93.030%, batch [288000/756895]\n",
      "loss: 0.021965, accuracy: 92.999%, batch [289280/756895]\n",
      "loss: 0.021882, accuracy: 93.019%, batch [290560/756895]\n",
      "loss: 0.026032, accuracy: 92.874%, batch [291840/756895]\n",
      "loss: 0.018041, accuracy: 93.008%, batch [293120/756895]\n",
      "loss: 0.019173, accuracy: 93.191%, batch [294400/756895]\n",
      "loss: 0.020974, accuracy: 92.873%, batch [295680/756895]\n",
      "loss: 0.017503, accuracy: 93.016%, batch [296960/756895]\n",
      "loss: 0.022560, accuracy: 92.783%, batch [298240/756895]\n",
      "loss: 0.023358, accuracy: 92.939%, batch [299520/756895]\n",
      "loss: 0.023865, accuracy: 92.992%, batch [300800/756895]\n",
      "loss: 0.020173, accuracy: 93.156%, batch [302080/756895]\n",
      "loss: 0.016170, accuracy: 93.286%, batch [303360/756895]\n",
      "loss: 0.017881, accuracy: 93.143%, batch [304640/756895]\n",
      "loss: 0.019691, accuracy: 93.076%, batch [305920/756895]\n",
      "loss: 0.018727, accuracy: 93.187%, batch [307200/756895]\n",
      "loss: 0.014847, accuracy: 93.295%, batch [308480/756895]\n",
      "loss: 0.015883, accuracy: 93.237%, batch [309760/756895]\n",
      "loss: 0.016661, accuracy: 93.097%, batch [311040/756895]\n",
      "loss: 0.021172, accuracy: 93.075%, batch [312320/756895]\n",
      "loss: 0.016086, accuracy: 93.189%, batch [313600/756895]\n",
      "loss: 0.018981, accuracy: 93.098%, batch [314880/756895]\n",
      "loss: 0.017867, accuracy: 93.121%, batch [316160/756895]\n",
      "loss: 0.016114, accuracy: 93.165%, batch [317440/756895]\n",
      "loss: 0.021762, accuracy: 93.174%, batch [318720/756895]\n",
      "loss: 0.021826, accuracy: 93.078%, batch [320000/756895]\n",
      "loss: 0.022440, accuracy: 92.886%, batch [321280/756895]\n",
      "loss: 0.019987, accuracy: 93.063%, batch [322560/756895]\n",
      "loss: 0.023422, accuracy: 93.059%, batch [323840/756895]\n",
      "loss: 0.019527, accuracy: 93.128%, batch [325120/756895]\n",
      "loss: 0.019905, accuracy: 93.033%, batch [326400/756895]\n",
      "loss: 0.024145, accuracy: 93.006%, batch [327680/756895]\n",
      "loss: 0.019721, accuracy: 93.053%, batch [328960/756895]\n",
      "loss: 0.016340, accuracy: 93.274%, batch [330240/756895]\n",
      "loss: 0.022249, accuracy: 92.912%, batch [331520/756895]\n",
      "loss: 0.016744, accuracy: 93.166%, batch [332800/756895]\n",
      "loss: 0.017079, accuracy: 93.112%, batch [334080/756895]\n",
      "loss: 0.022889, accuracy: 92.823%, batch [335360/756895]\n",
      "loss: 0.015624, accuracy: 93.242%, batch [336640/756895]\n",
      "loss: 0.018983, accuracy: 93.087%, batch [337920/756895]\n",
      "loss: 0.021899, accuracy: 92.979%, batch [339200/756895]\n",
      "loss: 0.019507, accuracy: 92.923%, batch [340480/756895]\n",
      "loss: 0.019055, accuracy: 93.115%, batch [341760/756895]\n",
      "loss: 0.021535, accuracy: 92.930%, batch [343040/756895]\n",
      "loss: 0.025385, accuracy: 92.912%, batch [344320/756895]\n",
      "loss: 0.023706, accuracy: 92.911%, batch [345600/756895]\n",
      "loss: 0.016284, accuracy: 93.124%, batch [346880/756895]\n",
      "loss: 0.018506, accuracy: 93.075%, batch [348160/756895]\n",
      "loss: 0.027485, accuracy: 92.535%, batch [349440/756895]\n",
      "loss: 0.013928, accuracy: 93.278%, batch [350720/756895]\n",
      "loss: 0.015692, accuracy: 93.199%, batch [352000/756895]\n",
      "loss: 0.019076, accuracy: 93.064%, batch [353280/756895]\n",
      "loss: 0.021572, accuracy: 92.951%, batch [354560/756895]\n",
      "loss: 0.017962, accuracy: 92.921%, batch [355840/756895]\n",
      "loss: 0.021403, accuracy: 92.995%, batch [357120/756895]\n",
      "loss: 0.028911, accuracy: 92.956%, batch [358400/756895]\n",
      "loss: 0.017852, accuracy: 93.176%, batch [359680/756895]\n",
      "loss: 0.017145, accuracy: 93.265%, batch [360960/756895]\n",
      "loss: 0.024030, accuracy: 92.766%, batch [362240/756895]\n",
      "loss: 0.027518, accuracy: 92.814%, batch [363520/756895]\n",
      "loss: 0.020636, accuracy: 93.010%, batch [364800/756895]\n",
      "loss: 0.026430, accuracy: 92.795%, batch [366080/756895]\n",
      "loss: 0.019780, accuracy: 92.945%, batch [367360/756895]\n",
      "loss: 0.016881, accuracy: 93.180%, batch [368640/756895]\n",
      "loss: 0.019285, accuracy: 93.066%, batch [369920/756895]\n",
      "loss: 0.020825, accuracy: 92.884%, batch [371200/756895]\n",
      "loss: 0.016804, accuracy: 92.996%, batch [372480/756895]\n",
      "loss: 0.018163, accuracy: 93.138%, batch [373760/756895]\n",
      "loss: 0.017895, accuracy: 93.104%, batch [375040/756895]\n",
      "loss: 0.021150, accuracy: 92.917%, batch [376320/756895]\n",
      "loss: 0.017977, accuracy: 93.152%, batch [377600/756895]\n",
      "loss: 0.026158, accuracy: 92.837%, batch [378880/756895]\n",
      "loss: 0.022343, accuracy: 92.721%, batch [380160/756895]\n",
      "loss: 0.015751, accuracy: 93.231%, batch [381440/756895]\n",
      "loss: 0.019656, accuracy: 92.930%, batch [382720/756895]\n",
      "loss: 0.023236, accuracy: 92.846%, batch [384000/756895]\n",
      "loss: 0.021120, accuracy: 92.890%, batch [385280/756895]\n",
      "loss: 0.017739, accuracy: 93.236%, batch [386560/756895]\n",
      "loss: 0.015386, accuracy: 93.218%, batch [387840/756895]\n",
      "loss: 0.017584, accuracy: 93.100%, batch [389120/756895]\n",
      "loss: 0.023624, accuracy: 93.056%, batch [390400/756895]\n",
      "loss: 0.020854, accuracy: 92.874%, batch [391680/756895]\n",
      "loss: 0.015134, accuracy: 93.269%, batch [392960/756895]\n",
      "loss: 0.015284, accuracy: 93.011%, batch [394240/756895]\n",
      "loss: 0.020201, accuracy: 93.006%, batch [395520/756895]\n",
      "loss: 0.017135, accuracy: 93.231%, batch [396800/756895]\n",
      "loss: 0.016175, accuracy: 93.166%, batch [398080/756895]\n",
      "loss: 0.017856, accuracy: 93.177%, batch [399360/756895]\n",
      "loss: 0.019253, accuracy: 92.975%, batch [400640/756895]\n",
      "loss: 0.017030, accuracy: 93.041%, batch [401920/756895]\n",
      "loss: 0.019491, accuracy: 92.872%, batch [403200/756895]\n",
      "loss: 0.017323, accuracy: 93.010%, batch [404480/756895]\n",
      "loss: 0.019085, accuracy: 92.975%, batch [405760/756895]\n",
      "loss: 0.019977, accuracy: 92.900%, batch [407040/756895]\n",
      "loss: 0.019891, accuracy: 92.928%, batch [408320/756895]\n",
      "loss: 0.018085, accuracy: 92.990%, batch [409600/756895]\n",
      "loss: 0.019386, accuracy: 92.993%, batch [410880/756895]\n",
      "loss: 0.016786, accuracy: 93.007%, batch [412160/756895]\n",
      "loss: 0.025150, accuracy: 92.854%, batch [413440/756895]\n",
      "loss: 0.017054, accuracy: 93.150%, batch [414720/756895]\n",
      "loss: 0.019899, accuracy: 92.988%, batch [416000/756895]\n",
      "loss: 0.019649, accuracy: 93.047%, batch [417280/756895]\n",
      "loss: 0.021569, accuracy: 92.887%, batch [418560/756895]\n",
      "loss: 0.014863, accuracy: 93.144%, batch [419840/756895]\n",
      "loss: 0.017448, accuracy: 92.965%, batch [421120/756895]\n",
      "loss: 0.018758, accuracy: 93.020%, batch [422400/756895]\n",
      "loss: 0.017143, accuracy: 93.032%, batch [423680/756895]\n",
      "loss: 0.021772, accuracy: 92.907%, batch [424960/756895]\n",
      "loss: 0.024755, accuracy: 92.808%, batch [426240/756895]\n",
      "loss: 0.021768, accuracy: 92.861%, batch [427520/756895]\n",
      "loss: 0.022040, accuracy: 92.932%, batch [428800/756895]\n",
      "loss: 0.017845, accuracy: 93.138%, batch [430080/756895]\n",
      "loss: 0.017623, accuracy: 93.111%, batch [431360/756895]\n",
      "loss: 0.017020, accuracy: 93.119%, batch [432640/756895]\n",
      "loss: 0.022103, accuracy: 92.905%, batch [433920/756895]\n",
      "loss: 0.025004, accuracy: 92.813%, batch [435200/756895]\n",
      "loss: 0.014902, accuracy: 93.122%, batch [436480/756895]\n",
      "loss: 0.016499, accuracy: 92.981%, batch [437760/756895]\n",
      "loss: 0.017277, accuracy: 93.231%, batch [439040/756895]\n",
      "loss: 0.018170, accuracy: 92.948%, batch [440320/756895]\n",
      "loss: 0.017411, accuracy: 93.237%, batch [441600/756895]\n",
      "loss: 0.018469, accuracy: 93.166%, batch [442880/756895]\n",
      "loss: 0.020519, accuracy: 93.026%, batch [444160/756895]\n",
      "loss: 0.018820, accuracy: 92.984%, batch [445440/756895]\n",
      "loss: 0.025491, accuracy: 92.931%, batch [446720/756895]\n",
      "loss: 0.020729, accuracy: 93.119%, batch [448000/756895]\n",
      "loss: 0.015798, accuracy: 93.188%, batch [449280/756895]\n",
      "loss: 0.019831, accuracy: 92.917%, batch [450560/756895]\n",
      "loss: 0.019935, accuracy: 92.798%, batch [451840/756895]\n",
      "loss: 0.016750, accuracy: 92.999%, batch [453120/756895]\n",
      "loss: 0.024777, accuracy: 93.009%, batch [454400/756895]\n",
      "loss: 0.021189, accuracy: 93.106%, batch [455680/756895]\n",
      "loss: 0.020493, accuracy: 93.168%, batch [456960/756895]\n",
      "loss: 0.016443, accuracy: 93.110%, batch [458240/756895]\n",
      "loss: 0.021718, accuracy: 92.963%, batch [459520/756895]\n",
      "loss: 0.019360, accuracy: 92.993%, batch [460800/756895]\n",
      "loss: 0.015757, accuracy: 93.200%, batch [462080/756895]\n",
      "loss: 0.014612, accuracy: 93.286%, batch [463360/756895]\n",
      "loss: 0.014221, accuracy: 93.128%, batch [464640/756895]\n",
      "loss: 0.017905, accuracy: 93.195%, batch [465920/756895]\n",
      "loss: 0.019205, accuracy: 93.002%, batch [467200/756895]\n",
      "loss: 0.018246, accuracy: 93.041%, batch [468480/756895]\n",
      "loss: 0.018564, accuracy: 93.237%, batch [469760/756895]\n",
      "loss: 0.023449, accuracy: 92.995%, batch [471040/756895]\n",
      "loss: 0.018016, accuracy: 93.228%, batch [472320/756895]\n",
      "loss: 0.015322, accuracy: 93.206%, batch [473600/756895]\n",
      "loss: 0.017043, accuracy: 92.987%, batch [474880/756895]\n",
      "loss: 0.020728, accuracy: 92.981%, batch [476160/756895]\n",
      "loss: 0.017244, accuracy: 93.224%, batch [477440/756895]\n",
      "loss: 0.020747, accuracy: 93.038%, batch [478720/756895]\n",
      "loss: 0.021056, accuracy: 92.808%, batch [480000/756895]\n",
      "loss: 0.018047, accuracy: 92.970%, batch [481280/756895]\n",
      "loss: 0.021311, accuracy: 93.019%, batch [482560/756895]\n",
      "loss: 0.016198, accuracy: 93.183%, batch [483840/756895]\n",
      "loss: 0.015267, accuracy: 93.011%, batch [485120/756895]\n",
      "loss: 0.021294, accuracy: 92.966%, batch [486400/756895]\n",
      "loss: 0.019932, accuracy: 92.933%, batch [487680/756895]\n",
      "loss: 0.018961, accuracy: 92.906%, batch [488960/756895]\n",
      "loss: 0.021623, accuracy: 92.801%, batch [490240/756895]\n",
      "loss: 0.017799, accuracy: 93.049%, batch [491520/756895]\n",
      "loss: 0.016599, accuracy: 93.167%, batch [492800/756895]\n",
      "loss: 0.018498, accuracy: 93.104%, batch [494080/756895]\n",
      "loss: 0.019830, accuracy: 93.105%, batch [495360/756895]\n",
      "loss: 0.016318, accuracy: 92.964%, batch [496640/756895]\n",
      "loss: 0.022125, accuracy: 92.900%, batch [497920/756895]\n",
      "loss: 0.021695, accuracy: 92.729%, batch [499200/756895]\n",
      "loss: 0.016958, accuracy: 92.961%, batch [500480/756895]\n",
      "loss: 0.017480, accuracy: 92.968%, batch [501760/756895]\n",
      "loss: 0.017118, accuracy: 93.206%, batch [503040/756895]\n",
      "loss: 0.016619, accuracy: 93.229%, batch [504320/756895]\n",
      "loss: 0.021401, accuracy: 92.864%, batch [505600/756895]\n",
      "loss: 0.018830, accuracy: 92.957%, batch [506880/756895]\n",
      "loss: 0.017093, accuracy: 92.888%, batch [508160/756895]\n",
      "loss: 0.020259, accuracy: 93.003%, batch [509440/756895]\n",
      "loss: 0.026147, accuracy: 92.900%, batch [510720/756895]\n",
      "loss: 0.019313, accuracy: 92.898%, batch [512000/756895]\n",
      "loss: 0.016376, accuracy: 93.051%, batch [513280/756895]\n",
      "loss: 0.019071, accuracy: 93.025%, batch [514560/756895]\n",
      "loss: 0.023026, accuracy: 92.957%, batch [515840/756895]\n",
      "loss: 0.020481, accuracy: 92.882%, batch [517120/756895]\n",
      "loss: 0.016634, accuracy: 93.218%, batch [518400/756895]\n",
      "loss: 0.022827, accuracy: 92.864%, batch [519680/756895]\n",
      "loss: 0.016884, accuracy: 93.223%, batch [520960/756895]\n",
      "loss: 0.017235, accuracy: 93.025%, batch [522240/756895]\n",
      "loss: 0.019321, accuracy: 93.150%, batch [523520/756895]\n",
      "loss: 0.018601, accuracy: 93.004%, batch [524800/756895]\n",
      "loss: 0.016382, accuracy: 93.083%, batch [526080/756895]\n",
      "loss: 0.018843, accuracy: 92.828%, batch [527360/756895]\n",
      "loss: 0.018418, accuracy: 93.023%, batch [528640/756895]\n",
      "loss: 0.016849, accuracy: 92.978%, batch [529920/756895]\n",
      "loss: 0.018491, accuracy: 92.958%, batch [531200/756895]\n",
      "loss: 0.022904, accuracy: 92.867%, batch [532480/756895]\n",
      "loss: 0.023409, accuracy: 92.906%, batch [533760/756895]\n",
      "loss: 0.019213, accuracy: 92.925%, batch [535040/756895]\n",
      "loss: 0.016818, accuracy: 93.304%, batch [536320/756895]\n",
      "loss: 0.021304, accuracy: 93.020%, batch [537600/756895]\n",
      "loss: 0.013501, accuracy: 93.153%, batch [538880/756895]\n",
      "loss: 0.016429, accuracy: 93.183%, batch [540160/756895]\n",
      "loss: 0.018881, accuracy: 92.979%, batch [541440/756895]\n",
      "loss: 0.023195, accuracy: 92.860%, batch [542720/756895]\n",
      "loss: 0.019298, accuracy: 92.981%, batch [544000/756895]\n",
      "loss: 0.019061, accuracy: 92.953%, batch [545280/756895]\n",
      "loss: 0.017958, accuracy: 93.083%, batch [546560/756895]\n",
      "loss: 0.019935, accuracy: 92.943%, batch [547840/756895]\n",
      "loss: 0.016783, accuracy: 93.211%, batch [549120/756895]\n",
      "loss: 0.024470, accuracy: 92.840%, batch [550400/756895]\n",
      "loss: 0.017071, accuracy: 93.025%, batch [551680/756895]\n",
      "loss: 0.015309, accuracy: 93.265%, batch [552960/756895]\n",
      "loss: 0.022767, accuracy: 92.835%, batch [554240/756895]\n",
      "loss: 0.016168, accuracy: 93.244%, batch [555520/756895]\n",
      "loss: 0.014805, accuracy: 93.236%, batch [556800/756895]\n",
      "loss: 0.018414, accuracy: 93.040%, batch [558080/756895]\n",
      "loss: 0.019381, accuracy: 92.787%, batch [559360/756895]\n",
      "loss: 0.020487, accuracy: 93.026%, batch [560640/756895]\n",
      "loss: 0.017886, accuracy: 93.129%, batch [561920/756895]\n",
      "loss: 0.019515, accuracy: 93.040%, batch [563200/756895]\n",
      "loss: 0.020923, accuracy: 92.928%, batch [564480/756895]\n",
      "loss: 0.019284, accuracy: 93.107%, batch [565760/756895]\n",
      "loss: 0.014982, accuracy: 93.313%, batch [567040/756895]\n",
      "loss: 0.017956, accuracy: 93.049%, batch [568320/756895]\n",
      "loss: 0.017857, accuracy: 93.020%, batch [569600/756895]\n",
      "loss: 0.022765, accuracy: 92.883%, batch [570880/756895]\n",
      "loss: 0.017588, accuracy: 93.118%, batch [572160/756895]\n",
      "loss: 0.031305, accuracy: 92.879%, batch [573440/756895]\n",
      "loss: 0.015943, accuracy: 93.228%, batch [574720/756895]\n",
      "loss: 0.021124, accuracy: 92.848%, batch [576000/756895]\n",
      "loss: 0.019620, accuracy: 92.940%, batch [577280/756895]\n",
      "loss: 0.017536, accuracy: 93.131%, batch [578560/756895]\n",
      "loss: 0.014360, accuracy: 93.207%, batch [579840/756895]\n",
      "loss: 0.016188, accuracy: 93.181%, batch [581120/756895]\n",
      "loss: 0.022069, accuracy: 93.164%, batch [582400/756895]\n",
      "loss: 0.017954, accuracy: 93.139%, batch [583680/756895]\n",
      "loss: 0.020083, accuracy: 93.208%, batch [584960/756895]\n",
      "loss: 0.016728, accuracy: 93.044%, batch [586240/756895]\n",
      "loss: 0.013846, accuracy: 93.205%, batch [587520/756895]\n",
      "loss: 0.019897, accuracy: 93.064%, batch [588800/756895]\n",
      "loss: 0.020288, accuracy: 92.907%, batch [590080/756895]\n",
      "loss: 0.016691, accuracy: 93.137%, batch [591360/756895]\n",
      "loss: 0.018070, accuracy: 93.173%, batch [592640/756895]\n",
      "loss: 0.015383, accuracy: 93.151%, batch [593920/756895]\n",
      "loss: 0.014435, accuracy: 93.126%, batch [595200/756895]\n",
      "loss: 0.017120, accuracy: 93.049%, batch [596480/756895]\n",
      "loss: 0.016245, accuracy: 93.168%, batch [597760/756895]\n",
      "loss: 0.018316, accuracy: 93.140%, batch [599040/756895]\n",
      "loss: 0.021909, accuracy: 92.909%, batch [600320/756895]\n",
      "loss: 0.019874, accuracy: 93.110%, batch [601600/756895]\n",
      "loss: 0.020088, accuracy: 92.952%, batch [602880/756895]\n",
      "loss: 0.022285, accuracy: 93.041%, batch [604160/756895]\n",
      "loss: 0.024038, accuracy: 93.054%, batch [605440/756895]\n",
      "loss: 0.016840, accuracy: 93.173%, batch [606720/756895]\n",
      "loss: 0.016433, accuracy: 93.280%, batch [608000/756895]\n",
      "loss: 0.014482, accuracy: 93.236%, batch [609280/756895]\n",
      "loss: 0.020031, accuracy: 92.850%, batch [610560/756895]\n",
      "loss: 0.026573, accuracy: 92.704%, batch [611840/756895]\n",
      "loss: 0.017799, accuracy: 93.097%, batch [613120/756895]\n",
      "loss: 0.017671, accuracy: 93.336%, batch [614400/756895]\n",
      "loss: 0.016766, accuracy: 93.027%, batch [615680/756895]\n",
      "loss: 0.018553, accuracy: 93.090%, batch [616960/756895]\n",
      "loss: 0.016863, accuracy: 93.196%, batch [618240/756895]\n",
      "loss: 0.015111, accuracy: 93.296%, batch [619520/756895]\n",
      "loss: 0.022285, accuracy: 92.832%, batch [620800/756895]\n",
      "loss: 0.015634, accuracy: 93.220%, batch [622080/756895]\n",
      "loss: 0.019896, accuracy: 92.881%, batch [623360/756895]\n",
      "loss: 0.016342, accuracy: 93.181%, batch [624640/756895]\n",
      "loss: 0.016674, accuracy: 93.115%, batch [625920/756895]\n",
      "loss: 0.015204, accuracy: 93.244%, batch [627200/756895]\n",
      "loss: 0.016925, accuracy: 92.935%, batch [628480/756895]\n",
      "loss: 0.016039, accuracy: 93.017%, batch [629760/756895]\n",
      "loss: 0.023106, accuracy: 92.768%, batch [631040/756895]\n",
      "loss: 0.017403, accuracy: 93.171%, batch [632320/756895]\n",
      "loss: 0.016543, accuracy: 93.053%, batch [633600/756895]\n",
      "loss: 0.018000, accuracy: 93.152%, batch [634880/756895]\n",
      "loss: 0.018359, accuracy: 92.986%, batch [636160/756895]\n",
      "loss: 0.021648, accuracy: 92.873%, batch [637440/756895]\n",
      "loss: 0.018727, accuracy: 93.050%, batch [638720/756895]\n",
      "loss: 0.018119, accuracy: 93.146%, batch [640000/756895]\n",
      "loss: 0.016506, accuracy: 93.155%, batch [641280/756895]\n",
      "loss: 0.017164, accuracy: 93.134%, batch [642560/756895]\n",
      "loss: 0.027838, accuracy: 92.772%, batch [643840/756895]\n",
      "loss: 0.017719, accuracy: 93.045%, batch [645120/756895]\n",
      "loss: 0.026549, accuracy: 92.898%, batch [646400/756895]\n",
      "loss: 0.018047, accuracy: 93.087%, batch [647680/756895]\n",
      "loss: 0.017098, accuracy: 93.119%, batch [648960/756895]\n",
      "loss: 0.019672, accuracy: 93.030%, batch [650240/756895]\n",
      "loss: 0.016721, accuracy: 93.182%, batch [651520/756895]\n",
      "loss: 0.022418, accuracy: 93.003%, batch [652800/756895]\n",
      "loss: 0.015995, accuracy: 93.251%, batch [654080/756895]\n",
      "loss: 0.019320, accuracy: 92.886%, batch [655360/756895]\n",
      "loss: 0.023437, accuracy: 93.048%, batch [656640/756895]\n",
      "loss: 0.019110, accuracy: 93.029%, batch [657920/756895]\n",
      "loss: 0.015196, accuracy: 93.212%, batch [659200/756895]\n",
      "loss: 0.017593, accuracy: 93.111%, batch [660480/756895]\n",
      "loss: 0.015496, accuracy: 93.180%, batch [661760/756895]\n",
      "loss: 0.025063, accuracy: 93.011%, batch [663040/756895]\n",
      "loss: 0.023280, accuracy: 92.970%, batch [664320/756895]\n",
      "loss: 0.020453, accuracy: 92.934%, batch [665600/756895]\n",
      "loss: 0.015915, accuracy: 93.287%, batch [666880/756895]\n",
      "loss: 0.018003, accuracy: 93.081%, batch [668160/756895]\n",
      "loss: 0.018745, accuracy: 93.194%, batch [669440/756895]\n",
      "loss: 0.015014, accuracy: 93.230%, batch [670720/756895]\n",
      "loss: 0.018282, accuracy: 93.043%, batch [672000/756895]\n",
      "loss: 0.019621, accuracy: 93.102%, batch [673280/756895]\n",
      "loss: 0.018875, accuracy: 93.155%, batch [674560/756895]\n",
      "loss: 0.020754, accuracy: 92.985%, batch [675840/756895]\n",
      "loss: 0.018717, accuracy: 92.932%, batch [677120/756895]\n",
      "loss: 0.015972, accuracy: 93.177%, batch [678400/756895]\n",
      "loss: 0.018912, accuracy: 92.967%, batch [679680/756895]\n",
      "loss: 0.019526, accuracy: 93.089%, batch [680960/756895]\n",
      "loss: 0.015590, accuracy: 93.227%, batch [682240/756895]\n",
      "loss: 0.019121, accuracy: 93.089%, batch [683520/756895]\n",
      "loss: 0.017902, accuracy: 92.928%, batch [684800/756895]\n",
      "loss: 0.021375, accuracy: 92.711%, batch [686080/756895]\n",
      "loss: 0.019634, accuracy: 93.186%, batch [687360/756895]\n",
      "loss: 0.024346, accuracy: 92.987%, batch [688640/756895]\n",
      "loss: 0.018548, accuracy: 93.025%, batch [689920/756895]\n",
      "loss: 0.018036, accuracy: 93.143%, batch [691200/756895]\n",
      "loss: 0.018317, accuracy: 93.133%, batch [692480/756895]\n",
      "loss: 0.016215, accuracy: 93.238%, batch [693760/756895]\n",
      "loss: 0.017439, accuracy: 93.164%, batch [695040/756895]\n",
      "loss: 0.017881, accuracy: 93.034%, batch [696320/756895]\n",
      "loss: 0.018565, accuracy: 93.049%, batch [697600/756895]\n",
      "loss: 0.023311, accuracy: 92.954%, batch [698880/756895]\n",
      "loss: 0.020807, accuracy: 92.935%, batch [700160/756895]\n",
      "loss: 0.021530, accuracy: 92.860%, batch [701440/756895]\n",
      "loss: 0.019308, accuracy: 93.048%, batch [702720/756895]\n",
      "loss: 0.020679, accuracy: 92.942%, batch [704000/756895]\n",
      "loss: 0.022566, accuracy: 92.685%, batch [705280/756895]\n",
      "loss: 0.020548, accuracy: 93.121%, batch [706560/756895]\n",
      "loss: 0.017425, accuracy: 93.049%, batch [707840/756895]\n",
      "loss: 0.019203, accuracy: 93.037%, batch [709120/756895]\n",
      "loss: 0.017365, accuracy: 93.075%, batch [710400/756895]\n",
      "loss: 0.020090, accuracy: 92.902%, batch [711680/756895]\n",
      "loss: 0.017235, accuracy: 93.075%, batch [712960/756895]\n",
      "loss: 0.021933, accuracy: 92.970%, batch [714240/756895]\n",
      "loss: 0.025628, accuracy: 92.833%, batch [715520/756895]\n",
      "loss: 0.021438, accuracy: 92.831%, batch [716800/756895]\n",
      "loss: 0.016228, accuracy: 93.214%, batch [718080/756895]\n",
      "loss: 0.019627, accuracy: 93.167%, batch [719360/756895]\n",
      "loss: 0.022366, accuracy: 93.075%, batch [720640/756895]\n",
      "loss: 0.016386, accuracy: 93.268%, batch [721920/756895]\n",
      "loss: 0.019087, accuracy: 92.938%, batch [723200/756895]\n",
      "loss: 0.026051, accuracy: 92.752%, batch [724480/756895]\n",
      "loss: 0.021158, accuracy: 92.958%, batch [725760/756895]\n",
      "loss: 0.015897, accuracy: 93.248%, batch [727040/756895]\n",
      "loss: 0.020431, accuracy: 93.004%, batch [728320/756895]\n",
      "loss: 0.019030, accuracy: 93.085%, batch [729600/756895]\n",
      "loss: 0.019181, accuracy: 93.116%, batch [730880/756895]\n",
      "loss: 0.018450, accuracy: 93.339%, batch [732160/756895]\n",
      "loss: 0.017202, accuracy: 93.157%, batch [733440/756895]\n",
      "loss: 0.016313, accuracy: 93.213%, batch [734720/756895]\n",
      "loss: 0.021537, accuracy: 92.983%, batch [736000/756895]\n",
      "loss: 0.018340, accuracy: 92.960%, batch [737280/756895]\n",
      "loss: 0.015561, accuracy: 93.161%, batch [738560/756895]\n",
      "loss: 0.024873, accuracy: 92.744%, batch [739840/756895]\n",
      "loss: 0.019671, accuracy: 92.974%, batch [741120/756895]\n",
      "loss: 0.017319, accuracy: 93.119%, batch [742400/756895]\n",
      "loss: 0.017278, accuracy: 92.966%, batch [743680/756895]\n",
      "loss: 0.017412, accuracy: 93.247%, batch [744960/756895]\n",
      "loss: 0.018722, accuracy: 93.119%, batch [746240/756895]\n",
      "loss: 0.020185, accuracy: 92.896%, batch [747520/756895]\n",
      "loss: 0.021965, accuracy: 92.780%, batch [748800/756895]\n",
      "loss: 0.021244, accuracy: 93.078%, batch [750080/756895]\n",
      "loss: 0.018789, accuracy: 92.927%, batch [751360/756895]\n",
      "loss: 0.017288, accuracy: 93.082%, batch [752640/756895]\n",
      "loss: 0.018963, accuracy: 93.015%, batch [753920/756895]\n",
      "loss: 0.017411, accuracy: 93.057%, batch [755200/756895]\n",
      "loss: 0.017202, accuracy: 93.197%, batch [756480/756895]\n",
      "Test avg loss: 0.019821, test avg accuracy: 93.024% \n",
      "\n",
      "Test avg loss: 0.019252, test avg accuracy: 93.052% \n",
      "\n",
      "Epoch 111\n",
      "------------------------\n",
      "loss: 0.023628, accuracy: 93.064%, batch [    0/756895]\n",
      "loss: 0.020956, accuracy: 92.948%, batch [ 1280/756895]\n",
      "loss: 0.017338, accuracy: 93.037%, batch [ 2560/756895]\n",
      "loss: 0.016428, accuracy: 93.147%, batch [ 3840/756895]\n",
      "loss: 0.016604, accuracy: 93.068%, batch [ 5120/756895]\n",
      "loss: 0.016203, accuracy: 93.149%, batch [ 6400/756895]\n",
      "loss: 0.018906, accuracy: 92.941%, batch [ 7680/756895]\n",
      "loss: 0.024159, accuracy: 92.877%, batch [ 8960/756895]\n",
      "loss: 0.017803, accuracy: 92.844%, batch [10240/756895]\n",
      "loss: 0.020143, accuracy: 93.085%, batch [11520/756895]\n",
      "loss: 0.022696, accuracy: 92.723%, batch [12800/756895]\n",
      "loss: 0.020753, accuracy: 92.941%, batch [14080/756895]\n",
      "loss: 0.017326, accuracy: 93.083%, batch [15360/756895]\n",
      "loss: 0.014336, accuracy: 93.180%, batch [16640/756895]\n",
      "loss: 0.020179, accuracy: 92.993%, batch [17920/756895]\n",
      "loss: 0.022898, accuracy: 92.908%, batch [19200/756895]\n",
      "loss: 0.020780, accuracy: 93.084%, batch [20480/756895]\n",
      "loss: 0.020463, accuracy: 92.962%, batch [21760/756895]\n",
      "loss: 0.020528, accuracy: 93.205%, batch [23040/756895]\n",
      "loss: 0.019489, accuracy: 93.024%, batch [24320/756895]\n",
      "loss: 0.019313, accuracy: 93.078%, batch [25600/756895]\n",
      "loss: 0.016920, accuracy: 93.207%, batch [26880/756895]\n",
      "loss: 0.028139, accuracy: 92.732%, batch [28160/756895]\n",
      "loss: 0.016968, accuracy: 93.242%, batch [29440/756895]\n",
      "loss: 0.015479, accuracy: 93.127%, batch [30720/756895]\n",
      "loss: 0.016750, accuracy: 93.204%, batch [32000/756895]\n",
      "loss: 0.020069, accuracy: 93.051%, batch [33280/756895]\n",
      "loss: 0.018296, accuracy: 93.025%, batch [34560/756895]\n",
      "loss: 0.019094, accuracy: 93.135%, batch [35840/756895]\n",
      "loss: 0.016546, accuracy: 93.203%, batch [37120/756895]\n",
      "loss: 0.018693, accuracy: 93.084%, batch [38400/756895]\n",
      "loss: 0.017764, accuracy: 93.190%, batch [39680/756895]\n",
      "loss: 0.018925, accuracy: 93.065%, batch [40960/756895]\n",
      "loss: 0.021706, accuracy: 93.057%, batch [42240/756895]\n",
      "loss: 0.016770, accuracy: 93.247%, batch [43520/756895]\n",
      "loss: 0.017228, accuracy: 93.124%, batch [44800/756895]\n",
      "loss: 0.019366, accuracy: 92.960%, batch [46080/756895]\n",
      "loss: 0.018016, accuracy: 92.831%, batch [47360/756895]\n",
      "loss: 0.016195, accuracy: 93.228%, batch [48640/756895]\n",
      "loss: 0.015110, accuracy: 93.153%, batch [49920/756895]\n",
      "loss: 0.016087, accuracy: 93.204%, batch [51200/756895]\n",
      "loss: 0.019490, accuracy: 93.010%, batch [52480/756895]\n",
      "loss: 0.018097, accuracy: 93.098%, batch [53760/756895]\n",
      "loss: 0.018684, accuracy: 92.961%, batch [55040/756895]\n",
      "loss: 0.021131, accuracy: 93.214%, batch [56320/756895]\n",
      "loss: 0.017034, accuracy: 93.173%, batch [57600/756895]\n",
      "loss: 0.018277, accuracy: 93.218%, batch [58880/756895]\n",
      "loss: 0.019995, accuracy: 92.932%, batch [60160/756895]\n",
      "loss: 0.019207, accuracy: 92.965%, batch [61440/756895]\n",
      "loss: 0.024678, accuracy: 92.806%, batch [62720/756895]\n",
      "loss: 0.018284, accuracy: 93.092%, batch [64000/756895]\n",
      "loss: 0.023407, accuracy: 93.049%, batch [65280/756895]\n",
      "loss: 0.021697, accuracy: 92.924%, batch [66560/756895]\n",
      "loss: 0.018992, accuracy: 93.031%, batch [67840/756895]\n",
      "loss: 0.023882, accuracy: 92.757%, batch [69120/756895]\n",
      "loss: 0.022253, accuracy: 93.108%, batch [70400/756895]\n",
      "loss: 0.022439, accuracy: 92.926%, batch [71680/756895]\n",
      "loss: 0.021670, accuracy: 92.705%, batch [72960/756895]\n",
      "loss: 0.017557, accuracy: 93.041%, batch [74240/756895]\n",
      "loss: 0.017923, accuracy: 93.184%, batch [75520/756895]\n",
      "loss: 0.017903, accuracy: 93.035%, batch [76800/756895]\n",
      "loss: 0.017378, accuracy: 93.115%, batch [78080/756895]\n",
      "loss: 0.017039, accuracy: 92.994%, batch [79360/756895]\n",
      "loss: 0.018308, accuracy: 92.987%, batch [80640/756895]\n",
      "loss: 0.014744, accuracy: 93.106%, batch [81920/756895]\n",
      "loss: 0.017237, accuracy: 93.141%, batch [83200/756895]\n",
      "loss: 0.015891, accuracy: 93.078%, batch [84480/756895]\n",
      "loss: 0.017553, accuracy: 93.141%, batch [85760/756895]\n",
      "loss: 0.015712, accuracy: 93.121%, batch [87040/756895]\n",
      "loss: 0.019350, accuracy: 93.212%, batch [88320/756895]\n",
      "loss: 0.016972, accuracy: 93.099%, batch [89600/756895]\n",
      "loss: 0.021176, accuracy: 93.002%, batch [90880/756895]\n",
      "loss: 0.021517, accuracy: 92.904%, batch [92160/756895]\n",
      "loss: 0.017056, accuracy: 93.283%, batch [93440/756895]\n",
      "loss: 0.015773, accuracy: 93.072%, batch [94720/756895]\n",
      "loss: 0.019778, accuracy: 93.029%, batch [96000/756895]\n",
      "loss: 0.016453, accuracy: 93.197%, batch [97280/756895]\n",
      "loss: 0.015884, accuracy: 93.207%, batch [98560/756895]\n",
      "loss: 0.024767, accuracy: 92.872%, batch [99840/756895]\n",
      "loss: 0.017787, accuracy: 93.095%, batch [101120/756895]\n",
      "loss: 0.021681, accuracy: 92.969%, batch [102400/756895]\n",
      "loss: 0.022027, accuracy: 93.073%, batch [103680/756895]\n",
      "loss: 0.019010, accuracy: 93.148%, batch [104960/756895]\n",
      "loss: 0.019664, accuracy: 93.026%, batch [106240/756895]\n",
      "loss: 0.020486, accuracy: 92.990%, batch [107520/756895]\n",
      "loss: 0.018204, accuracy: 93.082%, batch [108800/756895]\n",
      "loss: 0.018707, accuracy: 93.055%, batch [110080/756895]\n",
      "loss: 0.028854, accuracy: 92.818%, batch [111360/756895]\n",
      "loss: 0.019240, accuracy: 92.910%, batch [112640/756895]\n",
      "loss: 0.019120, accuracy: 92.957%, batch [113920/756895]\n",
      "loss: 0.016785, accuracy: 93.017%, batch [115200/756895]\n",
      "loss: 0.018344, accuracy: 93.194%, batch [116480/756895]\n",
      "loss: 0.017863, accuracy: 93.036%, batch [117760/756895]\n",
      "loss: 0.019289, accuracy: 92.949%, batch [119040/756895]\n",
      "loss: 0.015619, accuracy: 93.344%, batch [120320/756895]\n",
      "loss: 0.021861, accuracy: 92.970%, batch [121600/756895]\n",
      "loss: 0.018297, accuracy: 92.880%, batch [122880/756895]\n",
      "loss: 0.017855, accuracy: 93.124%, batch [124160/756895]\n",
      "loss: 0.015560, accuracy: 93.232%, batch [125440/756895]\n",
      "loss: 0.017827, accuracy: 93.075%, batch [126720/756895]\n",
      "loss: 0.018916, accuracy: 92.949%, batch [128000/756895]\n",
      "loss: 0.020997, accuracy: 92.946%, batch [129280/756895]\n",
      "loss: 0.018077, accuracy: 93.061%, batch [130560/756895]\n",
      "loss: 0.020384, accuracy: 93.054%, batch [131840/756895]\n",
      "loss: 0.015157, accuracy: 93.168%, batch [133120/756895]\n",
      "loss: 0.017336, accuracy: 92.942%, batch [134400/756895]\n",
      "loss: 0.021017, accuracy: 92.957%, batch [135680/756895]\n",
      "loss: 0.021743, accuracy: 92.980%, batch [136960/756895]\n",
      "loss: 0.017954, accuracy: 93.018%, batch [138240/756895]\n",
      "loss: 0.017701, accuracy: 93.102%, batch [139520/756895]\n",
      "loss: 0.020422, accuracy: 93.030%, batch [140800/756895]\n",
      "loss: 0.019060, accuracy: 93.195%, batch [142080/756895]\n",
      "loss: 0.021407, accuracy: 93.088%, batch [143360/756895]\n",
      "loss: 0.016994, accuracy: 92.989%, batch [144640/756895]\n",
      "loss: 0.017277, accuracy: 93.095%, batch [145920/756895]\n",
      "loss: 0.019089, accuracy: 93.071%, batch [147200/756895]\n",
      "loss: 0.021400, accuracy: 92.919%, batch [148480/756895]\n",
      "loss: 0.016549, accuracy: 93.093%, batch [149760/756895]\n",
      "loss: 0.018611, accuracy: 92.966%, batch [151040/756895]\n",
      "loss: 0.017945, accuracy: 93.048%, batch [152320/756895]\n",
      "loss: 0.020011, accuracy: 93.111%, batch [153600/756895]\n",
      "loss: 0.017076, accuracy: 93.152%, batch [154880/756895]\n",
      "loss: 0.015054, accuracy: 93.164%, batch [156160/756895]\n",
      "loss: 0.023000, accuracy: 93.081%, batch [157440/756895]\n",
      "loss: 0.019041, accuracy: 93.106%, batch [158720/756895]\n",
      "loss: 0.021392, accuracy: 92.946%, batch [160000/756895]\n",
      "loss: 0.019843, accuracy: 92.990%, batch [161280/756895]\n",
      "loss: 0.015374, accuracy: 93.194%, batch [162560/756895]\n",
      "loss: 0.019027, accuracy: 92.972%, batch [163840/756895]\n",
      "loss: 0.023424, accuracy: 93.009%, batch [165120/756895]\n",
      "loss: 0.017402, accuracy: 93.240%, batch [166400/756895]\n",
      "loss: 0.017777, accuracy: 92.939%, batch [167680/756895]\n",
      "loss: 0.018644, accuracy: 93.004%, batch [168960/756895]\n",
      "loss: 0.017783, accuracy: 93.012%, batch [170240/756895]\n",
      "loss: 0.023127, accuracy: 92.919%, batch [171520/756895]\n",
      "loss: 0.017085, accuracy: 93.231%, batch [172800/756895]\n",
      "loss: 0.019681, accuracy: 93.057%, batch [174080/756895]\n",
      "loss: 0.016196, accuracy: 93.298%, batch [175360/756895]\n",
      "loss: 0.016008, accuracy: 93.204%, batch [176640/756895]\n",
      "loss: 0.027667, accuracy: 92.705%, batch [177920/756895]\n",
      "loss: 0.020558, accuracy: 93.118%, batch [179200/756895]\n",
      "loss: 0.025438, accuracy: 92.782%, batch [180480/756895]\n",
      "loss: 0.018533, accuracy: 93.073%, batch [181760/756895]\n",
      "loss: 0.017680, accuracy: 93.089%, batch [183040/756895]\n",
      "loss: 0.014661, accuracy: 93.154%, batch [184320/756895]\n",
      "loss: 0.017495, accuracy: 93.211%, batch [185600/756895]\n",
      "loss: 0.021611, accuracy: 93.112%, batch [186880/756895]\n",
      "loss: 0.019350, accuracy: 93.030%, batch [188160/756895]\n",
      "loss: 0.017348, accuracy: 93.015%, batch [189440/756895]\n",
      "loss: 0.018144, accuracy: 93.119%, batch [190720/756895]\n",
      "loss: 0.021335, accuracy: 93.002%, batch [192000/756895]\n",
      "loss: 0.023061, accuracy: 93.104%, batch [193280/756895]\n",
      "loss: 0.015218, accuracy: 93.214%, batch [194560/756895]\n",
      "loss: 0.021909, accuracy: 92.822%, batch [195840/756895]\n",
      "loss: 0.017379, accuracy: 93.127%, batch [197120/756895]\n",
      "loss: 0.019735, accuracy: 93.144%, batch [198400/756895]\n",
      "loss: 0.015100, accuracy: 93.340%, batch [199680/756895]\n",
      "loss: 0.017255, accuracy: 93.055%, batch [200960/756895]\n",
      "loss: 0.016105, accuracy: 93.001%, batch [202240/756895]\n",
      "loss: 0.018059, accuracy: 93.073%, batch [203520/756895]\n",
      "loss: 0.024164, accuracy: 93.071%, batch [204800/756895]\n",
      "loss: 0.025386, accuracy: 92.832%, batch [206080/756895]\n",
      "loss: 0.015643, accuracy: 93.144%, batch [207360/756895]\n",
      "loss: 0.015508, accuracy: 93.176%, batch [208640/756895]\n",
      "loss: 0.019405, accuracy: 92.950%, batch [209920/756895]\n",
      "loss: 0.020751, accuracy: 92.947%, batch [211200/756895]\n",
      "loss: 0.018696, accuracy: 93.056%, batch [212480/756895]\n",
      "loss: 0.015645, accuracy: 93.316%, batch [213760/756895]\n",
      "loss: 0.015677, accuracy: 93.182%, batch [215040/756895]\n",
      "loss: 0.017809, accuracy: 93.069%, batch [216320/756895]\n",
      "loss: 0.018052, accuracy: 92.909%, batch [217600/756895]\n",
      "loss: 0.017780, accuracy: 93.094%, batch [218880/756895]\n",
      "loss: 0.019158, accuracy: 92.825%, batch [220160/756895]\n",
      "loss: 0.017266, accuracy: 93.039%, batch [221440/756895]\n",
      "loss: 0.020452, accuracy: 93.101%, batch [222720/756895]\n",
      "loss: 0.016212, accuracy: 93.101%, batch [224000/756895]\n",
      "loss: 0.015170, accuracy: 93.120%, batch [225280/756895]\n",
      "loss: 0.013964, accuracy: 93.284%, batch [226560/756895]\n",
      "loss: 0.016899, accuracy: 93.059%, batch [227840/756895]\n",
      "loss: 0.017859, accuracy: 93.111%, batch [229120/756895]\n",
      "loss: 0.017097, accuracy: 93.138%, batch [230400/756895]\n",
      "loss: 0.020611, accuracy: 93.012%, batch [231680/756895]\n",
      "loss: 0.019811, accuracy: 93.079%, batch [232960/756895]\n",
      "loss: 0.020710, accuracy: 92.814%, batch [234240/756895]\n",
      "loss: 0.019632, accuracy: 93.056%, batch [235520/756895]\n",
      "loss: 0.020554, accuracy: 92.771%, batch [236800/756895]\n",
      "loss: 0.021927, accuracy: 93.164%, batch [238080/756895]\n",
      "loss: 0.016449, accuracy: 93.176%, batch [239360/756895]\n",
      "loss: 0.017093, accuracy: 93.151%, batch [240640/756895]\n",
      "loss: 0.015876, accuracy: 93.217%, batch [241920/756895]\n",
      "loss: 0.021195, accuracy: 92.884%, batch [243200/756895]\n",
      "loss: 0.023450, accuracy: 92.863%, batch [244480/756895]\n",
      "loss: 0.017874, accuracy: 93.108%, batch [245760/756895]\n",
      "loss: 0.020389, accuracy: 93.009%, batch [247040/756895]\n",
      "loss: 0.020254, accuracy: 92.986%, batch [248320/756895]\n",
      "loss: 0.018890, accuracy: 93.077%, batch [249600/756895]\n",
      "loss: 0.017156, accuracy: 93.299%, batch [250880/756895]\n",
      "loss: 0.017179, accuracy: 93.221%, batch [252160/756895]\n",
      "loss: 0.021902, accuracy: 92.971%, batch [253440/756895]\n",
      "loss: 0.016475, accuracy: 93.190%, batch [254720/756895]\n",
      "loss: 0.016800, accuracy: 93.094%, batch [256000/756895]\n",
      "loss: 0.018646, accuracy: 93.113%, batch [257280/756895]\n",
      "loss: 0.016984, accuracy: 93.281%, batch [258560/756895]\n",
      "loss: 0.017383, accuracy: 93.160%, batch [259840/756895]\n",
      "loss: 0.017677, accuracy: 93.142%, batch [261120/756895]\n",
      "loss: 0.024717, accuracy: 93.026%, batch [262400/756895]\n",
      "loss: 0.018627, accuracy: 93.031%, batch [263680/756895]\n",
      "loss: 0.017483, accuracy: 93.107%, batch [264960/756895]\n",
      "loss: 0.021404, accuracy: 93.135%, batch [266240/756895]\n",
      "loss: 0.017148, accuracy: 93.123%, batch [267520/756895]\n",
      "loss: 0.020912, accuracy: 92.850%, batch [268800/756895]\n",
      "loss: 0.020295, accuracy: 93.169%, batch [270080/756895]\n",
      "loss: 0.018444, accuracy: 93.088%, batch [271360/756895]\n",
      "loss: 0.015272, accuracy: 93.224%, batch [272640/756895]\n",
      "loss: 0.022604, accuracy: 92.880%, batch [273920/756895]\n",
      "loss: 0.017749, accuracy: 93.117%, batch [275200/756895]\n",
      "loss: 0.017929, accuracy: 93.151%, batch [276480/756895]\n",
      "loss: 0.017054, accuracy: 93.149%, batch [277760/756895]\n",
      "loss: 0.019042, accuracy: 93.006%, batch [279040/756895]\n",
      "loss: 0.017800, accuracy: 93.205%, batch [280320/756895]\n",
      "loss: 0.022784, accuracy: 93.040%, batch [281600/756895]\n",
      "loss: 0.018050, accuracy: 92.994%, batch [282880/756895]\n",
      "loss: 0.020346, accuracy: 93.019%, batch [284160/756895]\n",
      "loss: 0.022685, accuracy: 92.760%, batch [285440/756895]\n",
      "loss: 0.015753, accuracy: 93.294%, batch [286720/756895]\n",
      "loss: 0.015367, accuracy: 93.242%, batch [288000/756895]\n",
      "loss: 0.021440, accuracy: 92.920%, batch [289280/756895]\n",
      "loss: 0.016295, accuracy: 93.128%, batch [290560/756895]\n",
      "loss: 0.020917, accuracy: 93.029%, batch [291840/756895]\n",
      "loss: 0.019763, accuracy: 93.153%, batch [293120/756895]\n",
      "loss: 0.021868, accuracy: 92.932%, batch [294400/756895]\n",
      "loss: 0.015709, accuracy: 93.213%, batch [295680/756895]\n",
      "loss: 0.019762, accuracy: 93.086%, batch [296960/756895]\n",
      "loss: 0.016409, accuracy: 93.162%, batch [298240/756895]\n",
      "loss: 0.018228, accuracy: 93.138%, batch [299520/756895]\n",
      "loss: 0.015071, accuracy: 93.189%, batch [300800/756895]\n",
      "loss: 0.014126, accuracy: 93.119%, batch [302080/756895]\n",
      "loss: 0.018359, accuracy: 93.060%, batch [303360/756895]\n",
      "loss: 0.020028, accuracy: 93.236%, batch [304640/756895]\n",
      "loss: 0.014667, accuracy: 93.012%, batch [305920/756895]\n",
      "loss: 0.017597, accuracy: 93.040%, batch [307200/756895]\n",
      "loss: 0.019931, accuracy: 93.077%, batch [308480/756895]\n",
      "loss: 0.020122, accuracy: 92.904%, batch [309760/756895]\n",
      "loss: 0.026626, accuracy: 92.826%, batch [311040/756895]\n",
      "loss: 0.020294, accuracy: 92.912%, batch [312320/756895]\n",
      "loss: 0.021625, accuracy: 92.956%, batch [313600/756895]\n",
      "loss: 0.023475, accuracy: 92.978%, batch [314880/756895]\n",
      "loss: 0.025806, accuracy: 92.941%, batch [316160/756895]\n",
      "loss: 0.031932, accuracy: 92.644%, batch [317440/756895]\n",
      "loss: 0.022021, accuracy: 92.957%, batch [318720/756895]\n",
      "loss: 0.020888, accuracy: 93.004%, batch [320000/756895]\n",
      "loss: 0.017000, accuracy: 93.149%, batch [321280/756895]\n",
      "loss: 0.017484, accuracy: 93.072%, batch [322560/756895]\n",
      "loss: 0.017511, accuracy: 93.071%, batch [323840/756895]\n",
      "loss: 0.022092, accuracy: 93.023%, batch [325120/756895]\n",
      "loss: 0.017851, accuracy: 93.174%, batch [326400/756895]\n",
      "loss: 0.019760, accuracy: 92.998%, batch [327680/756895]\n",
      "loss: 0.016080, accuracy: 93.194%, batch [328960/756895]\n",
      "loss: 0.017494, accuracy: 93.022%, batch [330240/756895]\n",
      "loss: 0.015906, accuracy: 93.054%, batch [331520/756895]\n",
      "loss: 0.023501, accuracy: 92.892%, batch [332800/756895]\n",
      "loss: 0.019979, accuracy: 92.858%, batch [334080/756895]\n",
      "loss: 0.018310, accuracy: 93.140%, batch [335360/756895]\n",
      "loss: 0.018932, accuracy: 93.109%, batch [336640/756895]\n",
      "loss: 0.018759, accuracy: 92.874%, batch [337920/756895]\n",
      "loss: 0.023090, accuracy: 92.905%, batch [339200/756895]\n",
      "loss: 0.014746, accuracy: 93.248%, batch [340480/756895]\n",
      "loss: 0.015866, accuracy: 93.093%, batch [341760/756895]\n",
      "loss: 0.021488, accuracy: 93.092%, batch [343040/756895]\n",
      "loss: 0.020227, accuracy: 93.032%, batch [344320/756895]\n",
      "loss: 0.017267, accuracy: 93.185%, batch [345600/756895]\n",
      "loss: 0.020111, accuracy: 92.836%, batch [346880/756895]\n",
      "loss: 0.018703, accuracy: 93.168%, batch [348160/756895]\n",
      "loss: 0.019356, accuracy: 93.081%, batch [349440/756895]\n",
      "loss: 0.025337, accuracy: 92.943%, batch [350720/756895]\n",
      "loss: 0.021829, accuracy: 92.852%, batch [352000/756895]\n",
      "loss: 0.018186, accuracy: 93.122%, batch [353280/756895]\n",
      "loss: 0.019450, accuracy: 93.025%, batch [354560/756895]\n",
      "loss: 0.017426, accuracy: 93.210%, batch [355840/756895]\n",
      "loss: 0.021355, accuracy: 92.981%, batch [357120/756895]\n",
      "loss: 0.017861, accuracy: 93.222%, batch [358400/756895]\n",
      "loss: 0.018258, accuracy: 93.100%, batch [359680/756895]\n",
      "loss: 0.017097, accuracy: 92.955%, batch [360960/756895]\n",
      "loss: 0.014838, accuracy: 93.320%, batch [362240/756895]\n",
      "loss: 0.019367, accuracy: 92.840%, batch [363520/756895]\n",
      "loss: 0.019627, accuracy: 93.047%, batch [364800/756895]\n",
      "loss: 0.019464, accuracy: 92.994%, batch [366080/756895]\n",
      "loss: 0.023638, accuracy: 92.783%, batch [367360/756895]\n",
      "loss: 0.019608, accuracy: 93.196%, batch [368640/756895]\n",
      "loss: 0.017651, accuracy: 93.112%, batch [369920/756895]\n",
      "loss: 0.023211, accuracy: 92.957%, batch [371200/756895]\n",
      "loss: 0.020494, accuracy: 92.982%, batch [372480/756895]\n",
      "loss: 0.020465, accuracy: 92.977%, batch [373760/756895]\n",
      "loss: 0.017422, accuracy: 93.084%, batch [375040/756895]\n",
      "loss: 0.021453, accuracy: 92.844%, batch [376320/756895]\n",
      "loss: 0.020332, accuracy: 92.968%, batch [377600/756895]\n",
      "loss: 0.019963, accuracy: 93.051%, batch [378880/756895]\n",
      "loss: 0.015967, accuracy: 93.182%, batch [380160/756895]\n",
      "loss: 0.022650, accuracy: 93.008%, batch [381440/756895]\n",
      "loss: 0.025553, accuracy: 92.757%, batch [382720/756895]\n",
      "loss: 0.020580, accuracy: 92.814%, batch [384000/756895]\n",
      "loss: 0.018459, accuracy: 93.198%, batch [385280/756895]\n",
      "loss: 0.020640, accuracy: 92.803%, batch [386560/756895]\n",
      "loss: 0.017664, accuracy: 93.113%, batch [387840/756895]\n",
      "loss: 0.020180, accuracy: 93.009%, batch [389120/756895]\n",
      "loss: 0.019362, accuracy: 93.162%, batch [390400/756895]\n",
      "loss: 0.015424, accuracy: 93.216%, batch [391680/756895]\n",
      "loss: 0.017730, accuracy: 93.171%, batch [392960/756895]\n",
      "loss: 0.015327, accuracy: 93.137%, batch [394240/756895]\n",
      "loss: 0.018250, accuracy: 93.015%, batch [395520/756895]\n",
      "loss: 0.019647, accuracy: 93.059%, batch [396800/756895]\n",
      "loss: 0.020019, accuracy: 93.039%, batch [398080/756895]\n",
      "loss: 0.014580, accuracy: 93.241%, batch [399360/756895]\n",
      "loss: 0.017636, accuracy: 93.028%, batch [400640/756895]\n",
      "loss: 0.018240, accuracy: 92.917%, batch [401920/756895]\n",
      "loss: 0.014907, accuracy: 93.315%, batch [403200/756895]\n",
      "loss: 0.019253, accuracy: 93.156%, batch [404480/756895]\n",
      "loss: 0.015138, accuracy: 93.223%, batch [405760/756895]\n",
      "loss: 0.020895, accuracy: 92.958%, batch [407040/756895]\n",
      "loss: 0.016254, accuracy: 93.115%, batch [408320/756895]\n",
      "loss: 0.017107, accuracy: 93.091%, batch [409600/756895]\n",
      "loss: 0.014344, accuracy: 93.328%, batch [410880/756895]\n",
      "loss: 0.021240, accuracy: 92.947%, batch [412160/756895]\n",
      "loss: 0.014491, accuracy: 93.267%, batch [413440/756895]\n",
      "loss: 0.019846, accuracy: 93.156%, batch [414720/756895]\n",
      "loss: 0.020334, accuracy: 93.065%, batch [416000/756895]\n",
      "loss: 0.016075, accuracy: 93.330%, batch [417280/756895]\n",
      "loss: 0.020433, accuracy: 92.940%, batch [418560/756895]\n",
      "loss: 0.017431, accuracy: 93.034%, batch [419840/756895]\n",
      "loss: 0.013963, accuracy: 93.228%, batch [421120/756895]\n",
      "loss: 0.016546, accuracy: 93.216%, batch [422400/756895]\n",
      "loss: 0.019152, accuracy: 92.991%, batch [423680/756895]\n",
      "loss: 0.016083, accuracy: 93.236%, batch [424960/756895]\n",
      "loss: 0.026020, accuracy: 92.932%, batch [426240/756895]\n",
      "loss: 0.022144, accuracy: 93.009%, batch [427520/756895]\n",
      "loss: 0.022267, accuracy: 92.888%, batch [428800/756895]\n",
      "loss: 0.019534, accuracy: 93.093%, batch [430080/756895]\n",
      "loss: 0.023230, accuracy: 92.852%, batch [431360/756895]\n",
      "loss: 0.018466, accuracy: 92.905%, batch [432640/756895]\n",
      "loss: 0.019119, accuracy: 93.173%, batch [433920/756895]\n",
      "loss: 0.015820, accuracy: 93.151%, batch [435200/756895]\n",
      "loss: 0.016660, accuracy: 93.154%, batch [436480/756895]\n",
      "loss: 0.020764, accuracy: 93.081%, batch [437760/756895]\n",
      "loss: 0.019758, accuracy: 93.134%, batch [439040/756895]\n",
      "loss: 0.018392, accuracy: 93.179%, batch [440320/756895]\n",
      "loss: 0.015641, accuracy: 93.222%, batch [441600/756895]\n",
      "loss: 0.018449, accuracy: 92.957%, batch [442880/756895]\n",
      "loss: 0.023156, accuracy: 93.037%, batch [444160/756895]\n",
      "loss: 0.017554, accuracy: 93.110%, batch [445440/756895]\n",
      "loss: 0.016534, accuracy: 93.192%, batch [446720/756895]\n",
      "loss: 0.018133, accuracy: 93.080%, batch [448000/756895]\n",
      "loss: 0.018207, accuracy: 93.191%, batch [449280/756895]\n",
      "loss: 0.024832, accuracy: 92.892%, batch [450560/756895]\n",
      "loss: 0.015587, accuracy: 93.233%, batch [451840/756895]\n",
      "loss: 0.018287, accuracy: 93.071%, batch [453120/756895]\n",
      "loss: 0.018351, accuracy: 93.023%, batch [454400/756895]\n",
      "loss: 0.020616, accuracy: 92.831%, batch [455680/756895]\n",
      "loss: 0.018887, accuracy: 92.756%, batch [456960/756895]\n",
      "loss: 0.018052, accuracy: 93.112%, batch [458240/756895]\n",
      "loss: 0.023076, accuracy: 92.918%, batch [459520/756895]\n",
      "loss: 0.020000, accuracy: 92.986%, batch [460800/756895]\n",
      "loss: 0.019938, accuracy: 92.952%, batch [462080/756895]\n",
      "loss: 0.019921, accuracy: 93.206%, batch [463360/756895]\n",
      "loss: 0.016578, accuracy: 92.955%, batch [464640/756895]\n",
      "loss: 0.019908, accuracy: 92.819%, batch [465920/756895]\n",
      "loss: 0.020696, accuracy: 92.932%, batch [467200/756895]\n",
      "loss: 0.017740, accuracy: 93.087%, batch [468480/756895]\n",
      "loss: 0.016182, accuracy: 93.149%, batch [469760/756895]\n",
      "loss: 0.025132, accuracy: 92.764%, batch [471040/756895]\n",
      "loss: 0.019295, accuracy: 92.855%, batch [472320/756895]\n",
      "loss: 0.016150, accuracy: 93.209%, batch [473600/756895]\n",
      "loss: 0.020470, accuracy: 93.066%, batch [474880/756895]\n",
      "loss: 0.026175, accuracy: 92.817%, batch [476160/756895]\n",
      "loss: 0.017159, accuracy: 93.172%, batch [477440/756895]\n",
      "loss: 0.017437, accuracy: 93.235%, batch [478720/756895]\n",
      "loss: 0.016384, accuracy: 93.298%, batch [480000/756895]\n",
      "loss: 0.020983, accuracy: 92.861%, batch [481280/756895]\n",
      "loss: 0.021493, accuracy: 92.961%, batch [482560/756895]\n",
      "loss: 0.021437, accuracy: 92.956%, batch [483840/756895]\n",
      "loss: 0.017768, accuracy: 93.113%, batch [485120/756895]\n",
      "loss: 0.021022, accuracy: 92.924%, batch [486400/756895]\n",
      "loss: 0.024414, accuracy: 93.006%, batch [487680/756895]\n",
      "loss: 0.018998, accuracy: 93.109%, batch [488960/756895]\n",
      "loss: 0.023497, accuracy: 92.869%, batch [490240/756895]\n",
      "loss: 0.020739, accuracy: 93.187%, batch [491520/756895]\n",
      "loss: 0.020893, accuracy: 93.012%, batch [492800/756895]\n",
      "loss: 0.015105, accuracy: 93.273%, batch [494080/756895]\n",
      "loss: 0.014944, accuracy: 93.277%, batch [495360/756895]\n",
      "loss: 0.025620, accuracy: 92.878%, batch [496640/756895]\n",
      "loss: 0.017628, accuracy: 92.934%, batch [497920/756895]\n",
      "loss: 0.017915, accuracy: 93.262%, batch [499200/756895]\n",
      "loss: 0.017452, accuracy: 93.078%, batch [500480/756895]\n",
      "loss: 0.017356, accuracy: 93.152%, batch [501760/756895]\n",
      "loss: 0.020085, accuracy: 93.015%, batch [503040/756895]\n",
      "loss: 0.021779, accuracy: 93.144%, batch [504320/756895]\n",
      "loss: 0.016026, accuracy: 93.230%, batch [505600/756895]\n",
      "loss: 0.016764, accuracy: 93.272%, batch [506880/756895]\n",
      "loss: 0.020482, accuracy: 93.001%, batch [508160/756895]\n",
      "loss: 0.019909, accuracy: 93.039%, batch [509440/756895]\n",
      "loss: 0.015880, accuracy: 93.023%, batch [510720/756895]\n",
      "loss: 0.017524, accuracy: 92.912%, batch [512000/756895]\n",
      "loss: 0.015255, accuracy: 93.287%, batch [513280/756895]\n",
      "loss: 0.018209, accuracy: 93.183%, batch [514560/756895]\n",
      "loss: 0.017485, accuracy: 93.066%, batch [515840/756895]\n",
      "loss: 0.017608, accuracy: 92.892%, batch [517120/756895]\n",
      "loss: 0.019978, accuracy: 92.936%, batch [518400/756895]\n",
      "loss: 0.017520, accuracy: 93.137%, batch [519680/756895]\n",
      "loss: 0.019336, accuracy: 92.856%, batch [520960/756895]\n",
      "loss: 0.022052, accuracy: 92.641%, batch [522240/756895]\n",
      "loss: 0.017763, accuracy: 92.963%, batch [523520/756895]\n",
      "loss: 0.015840, accuracy: 93.012%, batch [524800/756895]\n",
      "loss: 0.016323, accuracy: 93.100%, batch [526080/756895]\n",
      "loss: 0.022957, accuracy: 93.013%, batch [527360/756895]\n",
      "loss: 0.018858, accuracy: 93.123%, batch [528640/756895]\n",
      "loss: 0.020347, accuracy: 93.020%, batch [529920/756895]\n",
      "loss: 0.014070, accuracy: 93.136%, batch [531200/756895]\n",
      "loss: 0.018854, accuracy: 93.037%, batch [532480/756895]\n",
      "loss: 0.025291, accuracy: 93.041%, batch [533760/756895]\n",
      "loss: 0.018101, accuracy: 93.163%, batch [535040/756895]\n",
      "loss: 0.019011, accuracy: 93.131%, batch [536320/756895]\n",
      "loss: 0.013236, accuracy: 93.215%, batch [537600/756895]\n",
      "loss: 0.013850, accuracy: 93.260%, batch [538880/756895]\n",
      "loss: 0.018376, accuracy: 93.078%, batch [540160/756895]\n",
      "loss: 0.024807, accuracy: 92.896%, batch [541440/756895]\n",
      "loss: 0.020240, accuracy: 92.998%, batch [542720/756895]\n",
      "loss: 0.021156, accuracy: 92.937%, batch [544000/756895]\n",
      "loss: 0.015372, accuracy: 93.263%, batch [545280/756895]\n",
      "loss: 0.021288, accuracy: 92.839%, batch [546560/756895]\n",
      "loss: 0.019151, accuracy: 93.006%, batch [547840/756895]\n",
      "loss: 0.015657, accuracy: 93.275%, batch [549120/756895]\n",
      "loss: 0.016327, accuracy: 93.170%, batch [550400/756895]\n",
      "loss: 0.019667, accuracy: 92.874%, batch [551680/756895]\n",
      "loss: 0.017468, accuracy: 93.195%, batch [552960/756895]\n",
      "loss: 0.017334, accuracy: 93.229%, batch [554240/756895]\n",
      "loss: 0.018792, accuracy: 93.119%, batch [555520/756895]\n",
      "loss: 0.018776, accuracy: 92.978%, batch [556800/756895]\n",
      "loss: 0.017404, accuracy: 93.113%, batch [558080/756895]\n",
      "loss: 0.023631, accuracy: 92.895%, batch [559360/756895]\n",
      "loss: 0.014887, accuracy: 93.078%, batch [560640/756895]\n",
      "loss: 0.016384, accuracy: 93.441%, batch [561920/756895]\n",
      "loss: 0.017475, accuracy: 93.073%, batch [563200/756895]\n",
      "loss: 0.019086, accuracy: 93.243%, batch [564480/756895]\n",
      "loss: 0.020189, accuracy: 93.020%, batch [565760/756895]\n",
      "loss: 0.022496, accuracy: 92.796%, batch [567040/756895]\n",
      "loss: 0.014703, accuracy: 93.342%, batch [568320/756895]\n",
      "loss: 0.018944, accuracy: 92.786%, batch [569600/756895]\n",
      "loss: 0.024670, accuracy: 92.885%, batch [570880/756895]\n",
      "loss: 0.016802, accuracy: 93.088%, batch [572160/756895]\n",
      "loss: 0.015535, accuracy: 93.280%, batch [573440/756895]\n",
      "loss: 0.021103, accuracy: 93.049%, batch [574720/756895]\n",
      "loss: 0.021146, accuracy: 92.998%, batch [576000/756895]\n",
      "loss: 0.020159, accuracy: 93.040%, batch [577280/756895]\n",
      "loss: 0.020325, accuracy: 93.125%, batch [578560/756895]\n",
      "loss: 0.018635, accuracy: 93.071%, batch [579840/756895]\n",
      "loss: 0.017506, accuracy: 92.935%, batch [581120/756895]\n",
      "loss: 0.022020, accuracy: 93.019%, batch [582400/756895]\n",
      "loss: 0.019828, accuracy: 93.260%, batch [583680/756895]\n",
      "loss: 0.021358, accuracy: 92.807%, batch [584960/756895]\n",
      "loss: 0.018412, accuracy: 93.129%, batch [586240/756895]\n",
      "loss: 0.016095, accuracy: 93.122%, batch [587520/756895]\n",
      "loss: 0.017950, accuracy: 92.965%, batch [588800/756895]\n",
      "loss: 0.018225, accuracy: 93.082%, batch [590080/756895]\n",
      "loss: 0.017803, accuracy: 92.938%, batch [591360/756895]\n",
      "loss: 0.017071, accuracy: 93.083%, batch [592640/756895]\n",
      "loss: 0.017566, accuracy: 93.136%, batch [593920/756895]\n",
      "loss: 0.015547, accuracy: 93.229%, batch [595200/756895]\n",
      "loss: 0.017936, accuracy: 93.215%, batch [596480/756895]\n",
      "loss: 0.020718, accuracy: 93.004%, batch [597760/756895]\n",
      "loss: 0.022451, accuracy: 92.980%, batch [599040/756895]\n",
      "loss: 0.018322, accuracy: 93.231%, batch [600320/756895]\n",
      "loss: 0.017343, accuracy: 93.204%, batch [601600/756895]\n",
      "loss: 0.018063, accuracy: 92.924%, batch [602880/756895]\n",
      "loss: 0.013684, accuracy: 93.281%, batch [604160/756895]\n",
      "loss: 0.023393, accuracy: 93.014%, batch [605440/756895]\n",
      "loss: 0.018315, accuracy: 92.924%, batch [606720/756895]\n",
      "loss: 0.026817, accuracy: 92.821%, batch [608000/756895]\n",
      "loss: 0.016949, accuracy: 93.171%, batch [609280/756895]\n",
      "loss: 0.015271, accuracy: 93.246%, batch [610560/756895]\n",
      "loss: 0.017434, accuracy: 93.052%, batch [611840/756895]\n",
      "loss: 0.018473, accuracy: 92.913%, batch [613120/756895]\n",
      "loss: 0.020076, accuracy: 92.849%, batch [614400/756895]\n",
      "loss: 0.018999, accuracy: 92.914%, batch [615680/756895]\n",
      "loss: 0.019055, accuracy: 93.041%, batch [616960/756895]\n",
      "loss: 0.016636, accuracy: 93.178%, batch [618240/756895]\n",
      "loss: 0.014399, accuracy: 93.265%, batch [619520/756895]\n",
      "loss: 0.016450, accuracy: 93.029%, batch [620800/756895]\n",
      "loss: 0.016245, accuracy: 93.129%, batch [622080/756895]\n",
      "loss: 0.022458, accuracy: 92.894%, batch [623360/756895]\n",
      "loss: 0.027999, accuracy: 92.948%, batch [624640/756895]\n",
      "loss: 0.019256, accuracy: 92.909%, batch [625920/756895]\n",
      "loss: 0.017839, accuracy: 93.256%, batch [627200/756895]\n",
      "loss: 0.019610, accuracy: 93.142%, batch [628480/756895]\n",
      "loss: 0.020973, accuracy: 93.028%, batch [629760/756895]\n",
      "loss: 0.022244, accuracy: 92.946%, batch [631040/756895]\n",
      "loss: 0.026417, accuracy: 92.683%, batch [632320/756895]\n",
      "loss: 0.020177, accuracy: 92.882%, batch [633600/756895]\n",
      "loss: 0.019665, accuracy: 93.065%, batch [634880/756895]\n",
      "loss: 0.024909, accuracy: 93.033%, batch [636160/756895]\n",
      "loss: 0.015994, accuracy: 93.099%, batch [637440/756895]\n",
      "loss: 0.018323, accuracy: 92.979%, batch [638720/756895]\n",
      "loss: 0.018066, accuracy: 92.954%, batch [640000/756895]\n",
      "loss: 0.016429, accuracy: 93.177%, batch [641280/756895]\n",
      "loss: 0.018101, accuracy: 92.970%, batch [642560/756895]\n",
      "loss: 0.019310, accuracy: 93.131%, batch [643840/756895]\n",
      "loss: 0.014250, accuracy: 93.010%, batch [645120/756895]\n",
      "loss: 0.015432, accuracy: 93.097%, batch [646400/756895]\n",
      "loss: 0.017958, accuracy: 93.095%, batch [647680/756895]\n",
      "loss: 0.021764, accuracy: 93.006%, batch [648960/756895]\n",
      "loss: 0.020264, accuracy: 92.905%, batch [650240/756895]\n",
      "loss: 0.020372, accuracy: 92.890%, batch [651520/756895]\n",
      "loss: 0.023149, accuracy: 93.058%, batch [652800/756895]\n",
      "loss: 0.018286, accuracy: 93.200%, batch [654080/756895]\n",
      "loss: 0.018543, accuracy: 93.025%, batch [655360/756895]\n",
      "loss: 0.032175, accuracy: 92.826%, batch [656640/756895]\n",
      "loss: 0.017908, accuracy: 93.137%, batch [657920/756895]\n",
      "loss: 0.019235, accuracy: 93.196%, batch [659200/756895]\n",
      "loss: 0.017583, accuracy: 93.196%, batch [660480/756895]\n",
      "loss: 0.016665, accuracy: 93.215%, batch [661760/756895]\n",
      "loss: 0.017087, accuracy: 93.152%, batch [663040/756895]\n",
      "loss: 0.018536, accuracy: 92.994%, batch [664320/756895]\n",
      "loss: 0.023006, accuracy: 92.806%, batch [665600/756895]\n",
      "loss: 0.016792, accuracy: 93.060%, batch [666880/756895]\n",
      "loss: 0.016600, accuracy: 93.168%, batch [668160/756895]\n",
      "loss: 0.017899, accuracy: 93.074%, batch [669440/756895]\n",
      "loss: 0.018341, accuracy: 93.107%, batch [670720/756895]\n",
      "loss: 0.016771, accuracy: 93.140%, batch [672000/756895]\n",
      "loss: 0.019943, accuracy: 92.950%, batch [673280/756895]\n",
      "loss: 0.018918, accuracy: 93.053%, batch [674560/756895]\n",
      "loss: 0.018728, accuracy: 93.176%, batch [675840/756895]\n",
      "loss: 0.016876, accuracy: 93.121%, batch [677120/756895]\n",
      "loss: 0.016586, accuracy: 93.187%, batch [678400/756895]\n",
      "loss: 0.016624, accuracy: 93.166%, batch [679680/756895]\n",
      "loss: 0.017047, accuracy: 93.186%, batch [680960/756895]\n",
      "loss: 0.023953, accuracy: 92.830%, batch [682240/756895]\n",
      "loss: 0.019625, accuracy: 93.183%, batch [683520/756895]\n",
      "loss: 0.020258, accuracy: 93.051%, batch [684800/756895]\n",
      "loss: 0.019409, accuracy: 93.029%, batch [686080/756895]\n",
      "loss: 0.023172, accuracy: 93.030%, batch [687360/756895]\n",
      "loss: 0.020842, accuracy: 93.122%, batch [688640/756895]\n",
      "loss: 0.019638, accuracy: 93.040%, batch [689920/756895]\n",
      "loss: 0.017095, accuracy: 93.161%, batch [691200/756895]\n",
      "loss: 0.016035, accuracy: 93.398%, batch [692480/756895]\n",
      "loss: 0.016008, accuracy: 93.302%, batch [693760/756895]\n",
      "loss: 0.015827, accuracy: 92.960%, batch [695040/756895]\n",
      "loss: 0.018628, accuracy: 92.952%, batch [696320/756895]\n",
      "loss: 0.027242, accuracy: 92.859%, batch [697600/756895]\n",
      "loss: 0.021829, accuracy: 92.975%, batch [698880/756895]\n",
      "loss: 0.019389, accuracy: 92.868%, batch [700160/756895]\n",
      "loss: 0.021386, accuracy: 93.153%, batch [701440/756895]\n",
      "loss: 0.017672, accuracy: 93.051%, batch [702720/756895]\n",
      "loss: 0.016576, accuracy: 93.236%, batch [704000/756895]\n",
      "loss: 0.024720, accuracy: 93.003%, batch [705280/756895]\n",
      "loss: 0.020323, accuracy: 93.036%, batch [706560/756895]\n",
      "loss: 0.017649, accuracy: 93.116%, batch [707840/756895]\n",
      "loss: 0.015638, accuracy: 93.095%, batch [709120/756895]\n",
      "loss: 0.018450, accuracy: 92.961%, batch [710400/756895]\n",
      "loss: 0.017247, accuracy: 93.093%, batch [711680/756895]\n",
      "loss: 0.023849, accuracy: 93.007%, batch [712960/756895]\n",
      "loss: 0.020424, accuracy: 92.960%, batch [714240/756895]\n",
      "loss: 0.020635, accuracy: 93.004%, batch [715520/756895]\n",
      "loss: 0.019609, accuracy: 92.982%, batch [716800/756895]\n",
      "loss: 0.020994, accuracy: 92.985%, batch [718080/756895]\n",
      "loss: 0.023568, accuracy: 92.842%, batch [719360/756895]\n",
      "loss: 0.020134, accuracy: 93.078%, batch [720640/756895]\n",
      "loss: 0.020480, accuracy: 92.969%, batch [721920/756895]\n",
      "loss: 0.022244, accuracy: 92.943%, batch [723200/756895]\n",
      "loss: 0.018174, accuracy: 92.988%, batch [724480/756895]\n",
      "loss: 0.016298, accuracy: 93.135%, batch [725760/756895]\n",
      "loss: 0.016106, accuracy: 93.154%, batch [727040/756895]\n",
      "loss: 0.016316, accuracy: 93.123%, batch [728320/756895]\n",
      "loss: 0.018530, accuracy: 93.264%, batch [729600/756895]\n",
      "loss: 0.018949, accuracy: 92.977%, batch [730880/756895]\n",
      "loss: 0.017643, accuracy: 93.085%, batch [732160/756895]\n",
      "loss: 0.016335, accuracy: 93.070%, batch [733440/756895]\n",
      "loss: 0.017687, accuracy: 93.222%, batch [734720/756895]\n",
      "loss: 0.015310, accuracy: 93.133%, batch [736000/756895]\n",
      "loss: 0.020196, accuracy: 93.064%, batch [737280/756895]\n",
      "loss: 0.016329, accuracy: 93.174%, batch [738560/756895]\n",
      "loss: 0.017781, accuracy: 93.226%, batch [739840/756895]\n",
      "loss: 0.021154, accuracy: 92.961%, batch [741120/756895]\n",
      "loss: 0.020404, accuracy: 93.018%, batch [742400/756895]\n",
      "loss: 0.020267, accuracy: 93.147%, batch [743680/756895]\n",
      "loss: 0.021443, accuracy: 92.944%, batch [744960/756895]\n",
      "loss: 0.021220, accuracy: 92.883%, batch [746240/756895]\n",
      "loss: 0.016945, accuracy: 93.138%, batch [747520/756895]\n",
      "loss: 0.021144, accuracy: 93.044%, batch [748800/756895]\n",
      "loss: 0.018062, accuracy: 93.149%, batch [750080/756895]\n",
      "loss: 0.017287, accuracy: 93.247%, batch [751360/756895]\n",
      "loss: 0.017406, accuracy: 93.337%, batch [752640/756895]\n",
      "loss: 0.025854, accuracy: 92.873%, batch [753920/756895]\n",
      "loss: 0.019283, accuracy: 92.995%, batch [755200/756895]\n",
      "loss: 0.025378, accuracy: 92.799%, batch [756480/756895]\n",
      "Test avg loss: 0.019804, test avg accuracy: 93.016% \n",
      "\n",
      "Test avg loss: 0.019335, test avg accuracy: 93.051% \n",
      "\n",
      "Epoch 112\n",
      "------------------------\n",
      "loss: 0.017964, accuracy: 93.242%, batch [    0/756895]\n",
      "loss: 0.021267, accuracy: 92.968%, batch [ 1280/756895]\n",
      "loss: 0.021206, accuracy: 92.889%, batch [ 2560/756895]\n",
      "loss: 0.018551, accuracy: 92.958%, batch [ 3840/756895]\n",
      "loss: 0.018547, accuracy: 93.221%, batch [ 5120/756895]\n",
      "loss: 0.016523, accuracy: 93.162%, batch [ 6400/756895]\n",
      "loss: 0.030073, accuracy: 92.794%, batch [ 7680/756895]\n",
      "loss: 0.019207, accuracy: 93.101%, batch [ 8960/756895]\n",
      "loss: 0.016628, accuracy: 93.087%, batch [10240/756895]\n",
      "loss: 0.016788, accuracy: 93.206%, batch [11520/756895]\n",
      "loss: 0.017469, accuracy: 93.057%, batch [12800/756895]\n",
      "loss: 0.017449, accuracy: 92.944%, batch [14080/756895]\n",
      "loss: 0.022688, accuracy: 92.992%, batch [15360/756895]\n",
      "loss: 0.023570, accuracy: 93.045%, batch [16640/756895]\n",
      "loss: 0.018205, accuracy: 93.135%, batch [17920/756895]\n",
      "loss: 0.017655, accuracy: 93.046%, batch [19200/756895]\n",
      "loss: 0.017517, accuracy: 93.140%, batch [20480/756895]\n",
      "loss: 0.020875, accuracy: 92.976%, batch [21760/756895]\n",
      "loss: 0.017398, accuracy: 93.162%, batch [23040/756895]\n",
      "loss: 0.018506, accuracy: 93.061%, batch [24320/756895]\n",
      "loss: 0.018526, accuracy: 92.915%, batch [25600/756895]\n",
      "loss: 0.017259, accuracy: 93.008%, batch [26880/756895]\n",
      "loss: 0.015912, accuracy: 93.274%, batch [28160/756895]\n",
      "loss: 0.015888, accuracy: 93.072%, batch [29440/756895]\n",
      "loss: 0.017225, accuracy: 93.074%, batch [30720/756895]\n",
      "loss: 0.025515, accuracy: 92.814%, batch [32000/756895]\n",
      "loss: 0.020686, accuracy: 93.144%, batch [33280/756895]\n",
      "loss: 0.016045, accuracy: 93.308%, batch [34560/756895]\n",
      "loss: 0.019235, accuracy: 92.997%, batch [35840/756895]\n",
      "loss: 0.020498, accuracy: 93.036%, batch [37120/756895]\n",
      "loss: 0.016923, accuracy: 93.261%, batch [38400/756895]\n",
      "loss: 0.016568, accuracy: 93.178%, batch [39680/756895]\n",
      "loss: 0.021314, accuracy: 92.972%, batch [40960/756895]\n",
      "loss: 0.020596, accuracy: 92.828%, batch [42240/756895]\n",
      "loss: 0.016093, accuracy: 93.110%, batch [43520/756895]\n",
      "loss: 0.015242, accuracy: 93.237%, batch [44800/756895]\n",
      "loss: 0.018510, accuracy: 93.014%, batch [46080/756895]\n",
      "loss: 0.016826, accuracy: 93.131%, batch [47360/756895]\n",
      "loss: 0.019151, accuracy: 93.201%, batch [48640/756895]\n",
      "loss: 0.022234, accuracy: 93.044%, batch [49920/756895]\n",
      "loss: 0.015409, accuracy: 93.179%, batch [51200/756895]\n",
      "loss: 0.016445, accuracy: 93.112%, batch [52480/756895]\n",
      "loss: 0.017887, accuracy: 93.130%, batch [53760/756895]\n",
      "loss: 0.018336, accuracy: 93.114%, batch [55040/756895]\n",
      "loss: 0.020029, accuracy: 93.091%, batch [56320/756895]\n",
      "loss: 0.021429, accuracy: 92.921%, batch [57600/756895]\n",
      "loss: 0.018058, accuracy: 93.153%, batch [58880/756895]\n",
      "loss: 0.017439, accuracy: 92.873%, batch [60160/756895]\n",
      "loss: 0.019300, accuracy: 93.100%, batch [61440/756895]\n",
      "loss: 0.016534, accuracy: 93.331%, batch [62720/756895]\n",
      "loss: 0.019766, accuracy: 93.190%, batch [64000/756895]\n",
      "loss: 0.018618, accuracy: 92.921%, batch [65280/756895]\n",
      "loss: 0.021294, accuracy: 92.888%, batch [66560/756895]\n",
      "loss: 0.017958, accuracy: 93.023%, batch [67840/756895]\n",
      "loss: 0.016905, accuracy: 92.987%, batch [69120/756895]\n",
      "loss: 0.016086, accuracy: 93.000%, batch [70400/756895]\n",
      "loss: 0.017778, accuracy: 93.085%, batch [71680/756895]\n",
      "loss: 0.019185, accuracy: 93.179%, batch [72960/756895]\n",
      "loss: 0.020143, accuracy: 93.039%, batch [74240/756895]\n",
      "loss: 0.016951, accuracy: 93.214%, batch [75520/756895]\n",
      "loss: 0.019823, accuracy: 93.005%, batch [76800/756895]\n",
      "loss: 0.022925, accuracy: 92.913%, batch [78080/756895]\n",
      "loss: 0.018272, accuracy: 93.070%, batch [79360/756895]\n",
      "loss: 0.018536, accuracy: 92.973%, batch [80640/756895]\n",
      "loss: 0.017528, accuracy: 93.079%, batch [81920/756895]\n",
      "loss: 0.018545, accuracy: 93.086%, batch [83200/756895]\n",
      "loss: 0.022515, accuracy: 92.997%, batch [84480/756895]\n",
      "loss: 0.015765, accuracy: 93.224%, batch [85760/756895]\n",
      "loss: 0.019992, accuracy: 93.104%, batch [87040/756895]\n",
      "loss: 0.015983, accuracy: 93.207%, batch [88320/756895]\n",
      "loss: 0.020828, accuracy: 92.984%, batch [89600/756895]\n",
      "loss: 0.015394, accuracy: 93.216%, batch [90880/756895]\n",
      "loss: 0.017841, accuracy: 93.037%, batch [92160/756895]\n",
      "loss: 0.020891, accuracy: 93.012%, batch [93440/756895]\n",
      "loss: 0.023198, accuracy: 92.907%, batch [94720/756895]\n",
      "loss: 0.014449, accuracy: 93.254%, batch [96000/756895]\n",
      "loss: 0.019446, accuracy: 93.176%, batch [97280/756895]\n",
      "loss: 0.016575, accuracy: 93.111%, batch [98560/756895]\n",
      "loss: 0.020513, accuracy: 93.068%, batch [99840/756895]\n",
      "loss: 0.018936, accuracy: 93.121%, batch [101120/756895]\n",
      "loss: 0.021254, accuracy: 93.087%, batch [102400/756895]\n",
      "loss: 0.019027, accuracy: 93.019%, batch [103680/756895]\n",
      "loss: 0.014617, accuracy: 93.238%, batch [104960/756895]\n",
      "loss: 0.019695, accuracy: 92.984%, batch [106240/756895]\n",
      "loss: 0.020524, accuracy: 93.012%, batch [107520/756895]\n",
      "loss: 0.018297, accuracy: 93.092%, batch [108800/756895]\n",
      "loss: 0.016073, accuracy: 93.156%, batch [110080/756895]\n",
      "loss: 0.016751, accuracy: 93.094%, batch [111360/756895]\n",
      "loss: 0.017187, accuracy: 93.234%, batch [112640/756895]\n",
      "loss: 0.019867, accuracy: 93.021%, batch [113920/756895]\n",
      "loss: 0.020477, accuracy: 93.083%, batch [115200/756895]\n",
      "loss: 0.014363, accuracy: 93.149%, batch [116480/756895]\n",
      "loss: 0.019338, accuracy: 92.838%, batch [117760/756895]\n",
      "loss: 0.014129, accuracy: 93.284%, batch [119040/756895]\n",
      "loss: 0.027038, accuracy: 92.801%, batch [120320/756895]\n",
      "loss: 0.014260, accuracy: 93.310%, batch [121600/756895]\n",
      "loss: 0.019275, accuracy: 93.144%, batch [122880/756895]\n",
      "loss: 0.016259, accuracy: 93.014%, batch [124160/756895]\n",
      "loss: 0.018301, accuracy: 93.126%, batch [125440/756895]\n",
      "loss: 0.018414, accuracy: 93.098%, batch [126720/756895]\n",
      "loss: 0.017141, accuracy: 93.099%, batch [128000/756895]\n",
      "loss: 0.018406, accuracy: 92.916%, batch [129280/756895]\n",
      "loss: 0.022169, accuracy: 92.938%, batch [130560/756895]\n",
      "loss: 0.017460, accuracy: 93.163%, batch [131840/756895]\n",
      "loss: 0.019152, accuracy: 92.922%, batch [133120/756895]\n",
      "loss: 0.016142, accuracy: 93.103%, batch [134400/756895]\n",
      "loss: 0.020711, accuracy: 92.889%, batch [135680/756895]\n",
      "loss: 0.019561, accuracy: 93.048%, batch [136960/756895]\n",
      "loss: 0.017069, accuracy: 93.230%, batch [138240/756895]\n",
      "loss: 0.017270, accuracy: 93.060%, batch [139520/756895]\n",
      "loss: 0.018749, accuracy: 92.954%, batch [140800/756895]\n",
      "loss: 0.017362, accuracy: 93.106%, batch [142080/756895]\n",
      "loss: 0.018255, accuracy: 92.983%, batch [143360/756895]\n",
      "loss: 0.018877, accuracy: 92.890%, batch [144640/756895]\n",
      "loss: 0.016480, accuracy: 93.166%, batch [145920/756895]\n",
      "loss: 0.016342, accuracy: 93.086%, batch [147200/756895]\n",
      "loss: 0.016596, accuracy: 93.246%, batch [148480/756895]\n",
      "loss: 0.018428, accuracy: 92.977%, batch [149760/756895]\n",
      "loss: 0.013956, accuracy: 93.222%, batch [151040/756895]\n",
      "loss: 0.016680, accuracy: 93.162%, batch [152320/756895]\n",
      "loss: 0.018612, accuracy: 93.101%, batch [153600/756895]\n",
      "loss: 0.025124, accuracy: 92.971%, batch [154880/756895]\n",
      "loss: 0.014926, accuracy: 93.124%, batch [156160/756895]\n",
      "loss: 0.018404, accuracy: 93.088%, batch [157440/756895]\n",
      "loss: 0.018979, accuracy: 92.973%, batch [158720/756895]\n",
      "loss: 0.017606, accuracy: 93.143%, batch [160000/756895]\n",
      "loss: 0.018419, accuracy: 93.217%, batch [161280/756895]\n",
      "loss: 0.024164, accuracy: 92.956%, batch [162560/756895]\n",
      "loss: 0.017092, accuracy: 93.093%, batch [163840/756895]\n",
      "loss: 0.017386, accuracy: 93.067%, batch [165120/756895]\n",
      "loss: 0.016474, accuracy: 93.221%, batch [166400/756895]\n",
      "loss: 0.016576, accuracy: 93.122%, batch [167680/756895]\n",
      "loss: 0.020755, accuracy: 93.022%, batch [168960/756895]\n",
      "loss: 0.019722, accuracy: 92.981%, batch [170240/756895]\n",
      "loss: 0.017128, accuracy: 93.263%, batch [171520/756895]\n",
      "loss: 0.018946, accuracy: 92.939%, batch [172800/756895]\n",
      "loss: 0.018003, accuracy: 93.291%, batch [174080/756895]\n",
      "loss: 0.018543, accuracy: 93.055%, batch [175360/756895]\n",
      "loss: 0.015931, accuracy: 93.222%, batch [176640/756895]\n",
      "loss: 0.019441, accuracy: 92.953%, batch [177920/756895]\n",
      "loss: 0.022725, accuracy: 93.153%, batch [179200/756895]\n",
      "loss: 0.018085, accuracy: 92.926%, batch [180480/756895]\n",
      "loss: 0.021066, accuracy: 93.210%, batch [181760/756895]\n",
      "loss: 0.019907, accuracy: 92.993%, batch [183040/756895]\n",
      "loss: 0.017350, accuracy: 93.141%, batch [184320/756895]\n",
      "loss: 0.022119, accuracy: 92.909%, batch [185600/756895]\n",
      "loss: 0.016515, accuracy: 93.005%, batch [186880/756895]\n",
      "loss: 0.021449, accuracy: 92.899%, batch [188160/756895]\n",
      "loss: 0.018151, accuracy: 93.108%, batch [189440/756895]\n",
      "loss: 0.019187, accuracy: 93.073%, batch [190720/756895]\n",
      "loss: 0.021487, accuracy: 93.071%, batch [192000/756895]\n",
      "loss: 0.025641, accuracy: 93.009%, batch [193280/756895]\n",
      "loss: 0.024422, accuracy: 92.764%, batch [194560/756895]\n",
      "loss: 0.019410, accuracy: 93.112%, batch [195840/756895]\n",
      "loss: 0.023418, accuracy: 92.853%, batch [197120/756895]\n",
      "loss: 0.017032, accuracy: 93.107%, batch [198400/756895]\n",
      "loss: 0.018096, accuracy: 93.061%, batch [199680/756895]\n",
      "loss: 0.016096, accuracy: 93.186%, batch [200960/756895]\n",
      "loss: 0.018780, accuracy: 92.918%, batch [202240/756895]\n",
      "loss: 0.018579, accuracy: 93.176%, batch [203520/756895]\n",
      "loss: 0.021332, accuracy: 92.968%, batch [204800/756895]\n",
      "loss: 0.016179, accuracy: 93.201%, batch [206080/756895]\n",
      "loss: 0.017279, accuracy: 93.294%, batch [207360/756895]\n",
      "loss: 0.021343, accuracy: 93.175%, batch [208640/756895]\n",
      "loss: 0.019694, accuracy: 93.139%, batch [209920/756895]\n",
      "loss: 0.018702, accuracy: 93.214%, batch [211200/756895]\n",
      "loss: 0.021566, accuracy: 92.931%, batch [212480/756895]\n",
      "loss: 0.016021, accuracy: 93.317%, batch [213760/756895]\n",
      "loss: 0.021336, accuracy: 93.035%, batch [215040/756895]\n",
      "loss: 0.017422, accuracy: 93.142%, batch [216320/756895]\n",
      "loss: 0.022795, accuracy: 92.752%, batch [217600/756895]\n",
      "loss: 0.016326, accuracy: 92.989%, batch [218880/756895]\n",
      "loss: 0.015402, accuracy: 93.248%, batch [220160/756895]\n",
      "loss: 0.021911, accuracy: 92.981%, batch [221440/756895]\n",
      "loss: 0.018641, accuracy: 93.198%, batch [222720/756895]\n",
      "loss: 0.016781, accuracy: 93.160%, batch [224000/756895]\n",
      "loss: 0.020461, accuracy: 93.007%, batch [225280/756895]\n",
      "loss: 0.017324, accuracy: 92.933%, batch [226560/756895]\n",
      "loss: 0.016007, accuracy: 93.142%, batch [227840/756895]\n",
      "loss: 0.014836, accuracy: 93.186%, batch [229120/756895]\n",
      "loss: 0.017431, accuracy: 93.118%, batch [230400/756895]\n",
      "loss: 0.016695, accuracy: 93.088%, batch [231680/756895]\n",
      "loss: 0.019194, accuracy: 93.090%, batch [232960/756895]\n",
      "loss: 0.024209, accuracy: 92.837%, batch [234240/756895]\n",
      "loss: 0.020408, accuracy: 92.954%, batch [235520/756895]\n",
      "loss: 0.016230, accuracy: 93.226%, batch [236800/756895]\n",
      "loss: 0.014992, accuracy: 93.380%, batch [238080/756895]\n",
      "loss: 0.016756, accuracy: 93.030%, batch [239360/756895]\n",
      "loss: 0.017056, accuracy: 93.262%, batch [240640/756895]\n",
      "loss: 0.020943, accuracy: 93.078%, batch [241920/756895]\n",
      "loss: 0.018725, accuracy: 93.243%, batch [243200/756895]\n",
      "loss: 0.019102, accuracy: 93.049%, batch [244480/756895]\n",
      "loss: 0.021739, accuracy: 92.955%, batch [245760/756895]\n",
      "loss: 0.020689, accuracy: 93.096%, batch [247040/756895]\n",
      "loss: 0.016078, accuracy: 93.193%, batch [248320/756895]\n",
      "loss: 0.022250, accuracy: 93.064%, batch [249600/756895]\n",
      "loss: 0.016329, accuracy: 93.161%, batch [250880/756895]\n",
      "loss: 0.015984, accuracy: 93.230%, batch [252160/756895]\n",
      "loss: 0.017382, accuracy: 93.158%, batch [253440/756895]\n",
      "loss: 0.022132, accuracy: 92.662%, batch [254720/756895]\n",
      "loss: 0.017733, accuracy: 93.172%, batch [256000/756895]\n",
      "loss: 0.016933, accuracy: 93.173%, batch [257280/756895]\n",
      "loss: 0.017202, accuracy: 93.194%, batch [258560/756895]\n",
      "loss: 0.019618, accuracy: 93.008%, batch [259840/756895]\n",
      "loss: 0.019413, accuracy: 93.036%, batch [261120/756895]\n",
      "loss: 0.017309, accuracy: 93.181%, batch [262400/756895]\n",
      "loss: 0.016961, accuracy: 92.919%, batch [263680/756895]\n",
      "loss: 0.021072, accuracy: 93.193%, batch [264960/756895]\n",
      "loss: 0.016850, accuracy: 93.226%, batch [266240/756895]\n",
      "loss: 0.016713, accuracy: 93.152%, batch [267520/756895]\n",
      "loss: 0.016619, accuracy: 93.001%, batch [268800/756895]\n",
      "loss: 0.018820, accuracy: 93.059%, batch [270080/756895]\n",
      "loss: 0.018067, accuracy: 93.160%, batch [271360/756895]\n",
      "loss: 0.016565, accuracy: 93.118%, batch [272640/756895]\n",
      "loss: 0.022613, accuracy: 93.060%, batch [273920/756895]\n",
      "loss: 0.018273, accuracy: 92.947%, batch [275200/756895]\n",
      "loss: 0.019459, accuracy: 92.903%, batch [276480/756895]\n",
      "loss: 0.020786, accuracy: 92.942%, batch [277760/756895]\n",
      "loss: 0.017009, accuracy: 93.193%, batch [279040/756895]\n",
      "loss: 0.018112, accuracy: 93.069%, batch [280320/756895]\n",
      "loss: 0.018094, accuracy: 93.000%, batch [281600/756895]\n",
      "loss: 0.018489, accuracy: 92.947%, batch [282880/756895]\n",
      "loss: 0.020009, accuracy: 92.972%, batch [284160/756895]\n",
      "loss: 0.019890, accuracy: 93.198%, batch [285440/756895]\n",
      "loss: 0.019094, accuracy: 92.950%, batch [286720/756895]\n",
      "loss: 0.018424, accuracy: 93.261%, batch [288000/756895]\n",
      "loss: 0.017856, accuracy: 93.202%, batch [289280/756895]\n",
      "loss: 0.025238, accuracy: 92.948%, batch [290560/756895]\n",
      "loss: 0.017846, accuracy: 93.019%, batch [291840/756895]\n",
      "loss: 0.022737, accuracy: 92.960%, batch [293120/756895]\n",
      "loss: 0.018280, accuracy: 93.169%, batch [294400/756895]\n",
      "loss: 0.017305, accuracy: 93.141%, batch [295680/756895]\n",
      "loss: 0.016418, accuracy: 93.240%, batch [296960/756895]\n",
      "loss: 0.017930, accuracy: 93.274%, batch [298240/756895]\n",
      "loss: 0.017337, accuracy: 93.056%, batch [299520/756895]\n",
      "loss: 0.017899, accuracy: 93.027%, batch [300800/756895]\n",
      "loss: 0.018630, accuracy: 93.078%, batch [302080/756895]\n",
      "loss: 0.016998, accuracy: 93.123%, batch [303360/756895]\n",
      "loss: 0.019186, accuracy: 92.935%, batch [304640/756895]\n",
      "loss: 0.019592, accuracy: 92.901%, batch [305920/756895]\n",
      "loss: 0.016950, accuracy: 93.139%, batch [307200/756895]\n",
      "loss: 0.018700, accuracy: 93.016%, batch [308480/756895]\n",
      "loss: 0.019094, accuracy: 92.896%, batch [309760/756895]\n",
      "loss: 0.015531, accuracy: 93.266%, batch [311040/756895]\n",
      "loss: 0.015295, accuracy: 93.182%, batch [312320/756895]\n",
      "loss: 0.017874, accuracy: 92.907%, batch [313600/756895]\n",
      "loss: 0.015521, accuracy: 93.203%, batch [314880/756895]\n",
      "loss: 0.015156, accuracy: 93.010%, batch [316160/756895]\n",
      "loss: 0.014147, accuracy: 93.337%, batch [317440/756895]\n",
      "loss: 0.017632, accuracy: 93.015%, batch [318720/756895]\n",
      "loss: 0.016335, accuracy: 93.228%, batch [320000/756895]\n",
      "loss: 0.018916, accuracy: 93.188%, batch [321280/756895]\n",
      "loss: 0.017641, accuracy: 93.135%, batch [322560/756895]\n",
      "loss: 0.019570, accuracy: 92.948%, batch [323840/756895]\n",
      "loss: 0.015314, accuracy: 93.147%, batch [325120/756895]\n",
      "loss: 0.021230, accuracy: 92.931%, batch [326400/756895]\n",
      "loss: 0.016847, accuracy: 93.092%, batch [327680/756895]\n",
      "loss: 0.016866, accuracy: 93.092%, batch [328960/756895]\n",
      "loss: 0.017240, accuracy: 93.158%, batch [330240/756895]\n",
      "loss: 0.016380, accuracy: 93.081%, batch [331520/756895]\n",
      "loss: 0.019447, accuracy: 93.167%, batch [332800/756895]\n",
      "loss: 0.020661, accuracy: 93.093%, batch [334080/756895]\n",
      "loss: 0.016793, accuracy: 92.979%, batch [335360/756895]\n",
      "loss: 0.015204, accuracy: 93.255%, batch [336640/756895]\n",
      "loss: 0.015763, accuracy: 93.101%, batch [337920/756895]\n",
      "loss: 0.016127, accuracy: 93.018%, batch [339200/756895]\n",
      "loss: 0.016261, accuracy: 93.054%, batch [340480/756895]\n",
      "loss: 0.028494, accuracy: 92.753%, batch [341760/756895]\n",
      "loss: 0.022890, accuracy: 92.769%, batch [343040/756895]\n",
      "loss: 0.019485, accuracy: 93.033%, batch [344320/756895]\n",
      "loss: 0.018730, accuracy: 93.330%, batch [345600/756895]\n",
      "loss: 0.018128, accuracy: 92.943%, batch [346880/756895]\n",
      "loss: 0.015425, accuracy: 93.038%, batch [348160/756895]\n",
      "loss: 0.018082, accuracy: 93.034%, batch [349440/756895]\n",
      "loss: 0.022853, accuracy: 92.678%, batch [350720/756895]\n",
      "loss: 0.019910, accuracy: 92.817%, batch [352000/756895]\n",
      "loss: 0.016153, accuracy: 93.052%, batch [353280/756895]\n",
      "loss: 0.021189, accuracy: 92.971%, batch [354560/756895]\n",
      "loss: 0.014713, accuracy: 93.192%, batch [355840/756895]\n",
      "loss: 0.025537, accuracy: 92.913%, batch [357120/756895]\n",
      "loss: 0.024574, accuracy: 92.988%, batch [358400/756895]\n",
      "loss: 0.016765, accuracy: 93.278%, batch [359680/756895]\n",
      "loss: 0.014531, accuracy: 93.104%, batch [360960/756895]\n",
      "loss: 0.020617, accuracy: 93.021%, batch [362240/756895]\n",
      "loss: 0.017296, accuracy: 93.179%, batch [363520/756895]\n",
      "loss: 0.018121, accuracy: 92.877%, batch [364800/756895]\n",
      "loss: 0.020903, accuracy: 92.957%, batch [366080/756895]\n",
      "loss: 0.014272, accuracy: 93.412%, batch [367360/756895]\n",
      "loss: 0.015793, accuracy: 93.313%, batch [368640/756895]\n",
      "loss: 0.028211, accuracy: 92.781%, batch [369920/756895]\n",
      "loss: 0.017058, accuracy: 93.062%, batch [371200/756895]\n",
      "loss: 0.017938, accuracy: 93.237%, batch [372480/756895]\n",
      "loss: 0.018259, accuracy: 93.025%, batch [373760/756895]\n",
      "loss: 0.014768, accuracy: 93.156%, batch [375040/756895]\n",
      "loss: 0.018216, accuracy: 92.926%, batch [376320/756895]\n",
      "loss: 0.016617, accuracy: 93.213%, batch [377600/756895]\n",
      "loss: 0.016528, accuracy: 93.138%, batch [378880/756895]\n",
      "loss: 0.019094, accuracy: 92.972%, batch [380160/756895]\n",
      "loss: 0.013624, accuracy: 93.225%, batch [381440/756895]\n",
      "loss: 0.018554, accuracy: 93.088%, batch [382720/756895]\n",
      "loss: 0.019546, accuracy: 93.039%, batch [384000/756895]\n",
      "loss: 0.016571, accuracy: 93.083%, batch [385280/756895]\n",
      "loss: 0.018564, accuracy: 93.006%, batch [386560/756895]\n",
      "loss: 0.016092, accuracy: 93.243%, batch [387840/756895]\n",
      "loss: 0.023392, accuracy: 92.831%, batch [389120/756895]\n",
      "loss: 0.020057, accuracy: 93.155%, batch [390400/756895]\n",
      "loss: 0.014808, accuracy: 93.123%, batch [391680/756895]\n",
      "loss: 0.019166, accuracy: 93.135%, batch [392960/756895]\n",
      "loss: 0.023210, accuracy: 92.999%, batch [394240/756895]\n",
      "loss: 0.021783, accuracy: 93.121%, batch [395520/756895]\n",
      "loss: 0.017139, accuracy: 93.095%, batch [396800/756895]\n",
      "loss: 0.015094, accuracy: 93.253%, batch [398080/756895]\n",
      "loss: 0.021919, accuracy: 92.944%, batch [399360/756895]\n",
      "loss: 0.015698, accuracy: 93.154%, batch [400640/756895]\n",
      "loss: 0.021684, accuracy: 92.866%, batch [401920/756895]\n",
      "loss: 0.024852, accuracy: 92.817%, batch [403200/756895]\n",
      "loss: 0.018580, accuracy: 92.979%, batch [404480/756895]\n",
      "loss: 0.024754, accuracy: 92.938%, batch [405760/756895]\n",
      "loss: 0.015887, accuracy: 93.179%, batch [407040/756895]\n",
      "loss: 0.016751, accuracy: 93.272%, batch [408320/756895]\n",
      "loss: 0.022224, accuracy: 92.949%, batch [409600/756895]\n",
      "loss: 0.018675, accuracy: 93.079%, batch [410880/756895]\n",
      "loss: 0.015976, accuracy: 93.028%, batch [412160/756895]\n",
      "loss: 0.022761, accuracy: 92.770%, batch [413440/756895]\n",
      "loss: 0.014926, accuracy: 93.270%, batch [414720/756895]\n",
      "loss: 0.017153, accuracy: 93.063%, batch [416000/756895]\n",
      "loss: 0.018535, accuracy: 93.042%, batch [417280/756895]\n",
      "loss: 0.020742, accuracy: 93.081%, batch [418560/756895]\n",
      "loss: 0.019874, accuracy: 93.117%, batch [419840/756895]\n",
      "loss: 0.017368, accuracy: 92.961%, batch [421120/756895]\n",
      "loss: 0.017876, accuracy: 92.997%, batch [422400/756895]\n",
      "loss: 0.014734, accuracy: 93.191%, batch [423680/756895]\n",
      "loss: 0.018264, accuracy: 93.128%, batch [424960/756895]\n",
      "loss: 0.019626, accuracy: 92.928%, batch [426240/756895]\n",
      "loss: 0.021215, accuracy: 92.910%, batch [427520/756895]\n",
      "loss: 0.017600, accuracy: 92.973%, batch [428800/756895]\n",
      "loss: 0.015369, accuracy: 93.107%, batch [430080/756895]\n",
      "loss: 0.019537, accuracy: 92.948%, batch [431360/756895]\n",
      "loss: 0.017487, accuracy: 93.160%, batch [432640/756895]\n",
      "loss: 0.021241, accuracy: 92.925%, batch [433920/756895]\n",
      "loss: 0.017686, accuracy: 93.306%, batch [435200/756895]\n",
      "loss: 0.024304, accuracy: 93.060%, batch [436480/756895]\n",
      "loss: 0.026311, accuracy: 92.766%, batch [437760/756895]\n",
      "loss: 0.017534, accuracy: 93.046%, batch [439040/756895]\n",
      "loss: 0.019646, accuracy: 93.165%, batch [440320/756895]\n",
      "loss: 0.021367, accuracy: 92.765%, batch [441600/756895]\n",
      "loss: 0.019766, accuracy: 92.884%, batch [442880/756895]\n",
      "loss: 0.016198, accuracy: 93.110%, batch [444160/756895]\n",
      "loss: 0.021754, accuracy: 93.050%, batch [445440/756895]\n",
      "loss: 0.015219, accuracy: 93.015%, batch [446720/756895]\n",
      "loss: 0.020141, accuracy: 93.162%, batch [448000/756895]\n",
      "loss: 0.017218, accuracy: 93.000%, batch [449280/756895]\n",
      "loss: 0.015171, accuracy: 93.252%, batch [450560/756895]\n",
      "loss: 0.020623, accuracy: 92.915%, batch [451840/756895]\n",
      "loss: 0.016397, accuracy: 93.231%, batch [453120/756895]\n",
      "loss: 0.024407, accuracy: 92.895%, batch [454400/756895]\n",
      "loss: 0.019469, accuracy: 93.084%, batch [455680/756895]\n",
      "loss: 0.016735, accuracy: 93.208%, batch [456960/756895]\n",
      "loss: 0.017491, accuracy: 93.009%, batch [458240/756895]\n",
      "loss: 0.021542, accuracy: 92.843%, batch [459520/756895]\n",
      "loss: 0.016649, accuracy: 93.140%, batch [460800/756895]\n",
      "loss: 0.020224, accuracy: 93.056%, batch [462080/756895]\n",
      "loss: 0.021262, accuracy: 92.859%, batch [463360/756895]\n",
      "loss: 0.017357, accuracy: 93.323%, batch [464640/756895]\n",
      "loss: 0.018359, accuracy: 93.132%, batch [465920/756895]\n",
      "loss: 0.016213, accuracy: 93.217%, batch [467200/756895]\n",
      "loss: 0.020228, accuracy: 92.974%, batch [468480/756895]\n",
      "loss: 0.018458, accuracy: 93.068%, batch [469760/756895]\n",
      "loss: 0.020044, accuracy: 92.948%, batch [471040/756895]\n",
      "loss: 0.015667, accuracy: 93.212%, batch [472320/756895]\n",
      "loss: 0.018257, accuracy: 93.134%, batch [473600/756895]\n",
      "loss: 0.019354, accuracy: 92.990%, batch [474880/756895]\n",
      "loss: 0.018780, accuracy: 92.995%, batch [476160/756895]\n",
      "loss: 0.018166, accuracy: 93.066%, batch [477440/756895]\n",
      "loss: 0.023426, accuracy: 92.729%, batch [478720/756895]\n",
      "loss: 0.018585, accuracy: 92.996%, batch [480000/756895]\n",
      "loss: 0.016792, accuracy: 93.141%, batch [481280/756895]\n",
      "loss: 0.019692, accuracy: 92.949%, batch [482560/756895]\n",
      "loss: 0.018734, accuracy: 93.113%, batch [483840/756895]\n",
      "loss: 0.022440, accuracy: 92.947%, batch [485120/756895]\n",
      "loss: 0.021397, accuracy: 92.896%, batch [486400/756895]\n",
      "loss: 0.017115, accuracy: 93.045%, batch [487680/756895]\n",
      "loss: 0.019533, accuracy: 92.937%, batch [488960/756895]\n",
      "loss: 0.022978, accuracy: 93.086%, batch [490240/756895]\n",
      "loss: 0.020186, accuracy: 93.187%, batch [491520/756895]\n",
      "loss: 0.015410, accuracy: 93.191%, batch [492800/756895]\n",
      "loss: 0.018687, accuracy: 93.212%, batch [494080/756895]\n",
      "loss: 0.023336, accuracy: 92.966%, batch [495360/756895]\n",
      "loss: 0.019121, accuracy: 93.070%, batch [496640/756895]\n",
      "loss: 0.019625, accuracy: 93.011%, batch [497920/756895]\n",
      "loss: 0.017068, accuracy: 93.005%, batch [499200/756895]\n",
      "loss: 0.024574, accuracy: 92.921%, batch [500480/756895]\n",
      "loss: 0.020163, accuracy: 93.083%, batch [501760/756895]\n",
      "loss: 0.018077, accuracy: 93.238%, batch [503040/756895]\n",
      "loss: 0.017199, accuracy: 93.074%, batch [504320/756895]\n",
      "loss: 0.018683, accuracy: 93.045%, batch [505600/756895]\n",
      "loss: 0.018446, accuracy: 92.922%, batch [506880/756895]\n",
      "loss: 0.019121, accuracy: 93.022%, batch [508160/756895]\n",
      "loss: 0.020513, accuracy: 93.003%, batch [509440/756895]\n",
      "loss: 0.024039, accuracy: 92.681%, batch [510720/756895]\n",
      "loss: 0.018650, accuracy: 93.161%, batch [512000/756895]\n",
      "loss: 0.015153, accuracy: 93.346%, batch [513280/756895]\n",
      "loss: 0.021553, accuracy: 92.926%, batch [514560/756895]\n",
      "loss: 0.028667, accuracy: 92.678%, batch [515840/756895]\n",
      "loss: 0.018483, accuracy: 92.974%, batch [517120/756895]\n",
      "loss: 0.022157, accuracy: 93.045%, batch [518400/756895]\n",
      "loss: 0.026489, accuracy: 92.903%, batch [519680/756895]\n",
      "loss: 0.028249, accuracy: 92.713%, batch [520960/756895]\n",
      "loss: 0.023126, accuracy: 92.648%, batch [522240/756895]\n",
      "loss: 0.020541, accuracy: 92.913%, batch [523520/756895]\n",
      "loss: 0.023293, accuracy: 93.006%, batch [524800/756895]\n",
      "loss: 0.017636, accuracy: 93.173%, batch [526080/756895]\n",
      "loss: 0.019497, accuracy: 92.872%, batch [527360/756895]\n",
      "loss: 0.017059, accuracy: 93.039%, batch [528640/756895]\n",
      "loss: 0.025304, accuracy: 92.871%, batch [529920/756895]\n",
      "loss: 0.017148, accuracy: 93.236%, batch [531200/756895]\n",
      "loss: 0.024012, accuracy: 92.917%, batch [532480/756895]\n",
      "loss: 0.018425, accuracy: 93.146%, batch [533760/756895]\n",
      "loss: 0.018806, accuracy: 93.230%, batch [535040/756895]\n",
      "loss: 0.018032, accuracy: 93.031%, batch [536320/756895]\n",
      "loss: 0.018253, accuracy: 92.997%, batch [537600/756895]\n",
      "loss: 0.017586, accuracy: 93.208%, batch [538880/756895]\n",
      "loss: 0.016353, accuracy: 93.161%, batch [540160/756895]\n",
      "loss: 0.018141, accuracy: 93.143%, batch [541440/756895]\n",
      "loss: 0.020437, accuracy: 92.904%, batch [542720/756895]\n",
      "loss: 0.019169, accuracy: 93.138%, batch [544000/756895]\n",
      "loss: 0.016576, accuracy: 93.115%, batch [545280/756895]\n",
      "loss: 0.017976, accuracy: 93.123%, batch [546560/756895]\n",
      "loss: 0.019872, accuracy: 93.025%, batch [547840/756895]\n",
      "loss: 0.015574, accuracy: 93.157%, batch [549120/756895]\n",
      "loss: 0.021508, accuracy: 93.062%, batch [550400/756895]\n",
      "loss: 0.027635, accuracy: 92.803%, batch [551680/756895]\n",
      "loss: 0.020894, accuracy: 92.970%, batch [552960/756895]\n",
      "loss: 0.019532, accuracy: 93.180%, batch [554240/756895]\n",
      "loss: 0.015880, accuracy: 93.233%, batch [555520/756895]\n",
      "loss: 0.016570, accuracy: 93.080%, batch [556800/756895]\n",
      "loss: 0.017944, accuracy: 93.152%, batch [558080/756895]\n",
      "loss: 0.020585, accuracy: 93.146%, batch [559360/756895]\n",
      "loss: 0.024746, accuracy: 92.889%, batch [560640/756895]\n",
      "loss: 0.016099, accuracy: 93.266%, batch [561920/756895]\n",
      "loss: 0.017214, accuracy: 93.126%, batch [563200/756895]\n",
      "loss: 0.015457, accuracy: 93.108%, batch [564480/756895]\n",
      "loss: 0.018188, accuracy: 93.073%, batch [565760/756895]\n",
      "loss: 0.019275, accuracy: 92.908%, batch [567040/756895]\n",
      "loss: 0.015963, accuracy: 93.201%, batch [568320/756895]\n",
      "loss: 0.021839, accuracy: 92.984%, batch [569600/756895]\n",
      "loss: 0.026819, accuracy: 92.852%, batch [570880/756895]\n",
      "loss: 0.022136, accuracy: 92.908%, batch [572160/756895]\n",
      "loss: 0.016864, accuracy: 93.095%, batch [573440/756895]\n",
      "loss: 0.018070, accuracy: 93.159%, batch [574720/756895]\n",
      "loss: 0.022879, accuracy: 92.955%, batch [576000/756895]\n",
      "loss: 0.017393, accuracy: 92.956%, batch [577280/756895]\n",
      "loss: 0.016764, accuracy: 93.044%, batch [578560/756895]\n",
      "loss: 0.022988, accuracy: 93.088%, batch [579840/756895]\n",
      "loss: 0.021683, accuracy: 93.136%, batch [581120/756895]\n",
      "loss: 0.018370, accuracy: 93.075%, batch [582400/756895]\n",
      "loss: 0.016283, accuracy: 93.083%, batch [583680/756895]\n",
      "loss: 0.019620, accuracy: 93.032%, batch [584960/756895]\n",
      "loss: 0.016485, accuracy: 93.216%, batch [586240/756895]\n",
      "loss: 0.015973, accuracy: 93.174%, batch [587520/756895]\n",
      "loss: 0.017748, accuracy: 93.068%, batch [588800/756895]\n",
      "loss: 0.017096, accuracy: 93.408%, batch [590080/756895]\n",
      "loss: 0.021336, accuracy: 93.229%, batch [591360/756895]\n",
      "loss: 0.017874, accuracy: 93.059%, batch [592640/756895]\n",
      "loss: 0.018968, accuracy: 93.040%, batch [593920/756895]\n",
      "loss: 0.019058, accuracy: 93.099%, batch [595200/756895]\n",
      "loss: 0.017201, accuracy: 93.058%, batch [596480/756895]\n",
      "loss: 0.018170, accuracy: 93.121%, batch [597760/756895]\n",
      "loss: 0.017327, accuracy: 93.074%, batch [599040/756895]\n",
      "loss: 0.017881, accuracy: 92.959%, batch [600320/756895]\n",
      "loss: 0.019867, accuracy: 93.025%, batch [601600/756895]\n",
      "loss: 0.016831, accuracy: 93.052%, batch [602880/756895]\n",
      "loss: 0.019029, accuracy: 93.060%, batch [604160/756895]\n",
      "loss: 0.020495, accuracy: 93.013%, batch [605440/756895]\n",
      "loss: 0.016716, accuracy: 93.189%, batch [606720/756895]\n",
      "loss: 0.018204, accuracy: 93.017%, batch [608000/756895]\n",
      "loss: 0.019211, accuracy: 93.145%, batch [609280/756895]\n",
      "loss: 0.021444, accuracy: 92.953%, batch [610560/756895]\n",
      "loss: 0.019067, accuracy: 92.981%, batch [611840/756895]\n",
      "loss: 0.025295, accuracy: 93.006%, batch [613120/756895]\n",
      "loss: 0.016703, accuracy: 93.229%, batch [614400/756895]\n",
      "loss: 0.019461, accuracy: 93.070%, batch [615680/756895]\n",
      "loss: 0.014952, accuracy: 93.293%, batch [616960/756895]\n",
      "loss: 0.014789, accuracy: 93.130%, batch [618240/756895]\n",
      "loss: 0.019318, accuracy: 92.951%, batch [619520/756895]\n",
      "loss: 0.014815, accuracy: 93.158%, batch [620800/756895]\n",
      "loss: 0.021403, accuracy: 92.910%, batch [622080/756895]\n",
      "loss: 0.018389, accuracy: 93.086%, batch [623360/756895]\n",
      "loss: 0.022772, accuracy: 92.995%, batch [624640/756895]\n",
      "loss: 0.027688, accuracy: 93.071%, batch [625920/756895]\n",
      "loss: 0.021475, accuracy: 93.079%, batch [627200/756895]\n",
      "loss: 0.019084, accuracy: 93.051%, batch [628480/756895]\n",
      "loss: 0.017656, accuracy: 92.974%, batch [629760/756895]\n",
      "loss: 0.013189, accuracy: 93.212%, batch [631040/756895]\n",
      "loss: 0.020862, accuracy: 93.177%, batch [632320/756895]\n",
      "loss: 0.017443, accuracy: 93.100%, batch [633600/756895]\n",
      "loss: 0.016829, accuracy: 93.008%, batch [634880/756895]\n",
      "loss: 0.016701, accuracy: 93.080%, batch [636160/756895]\n",
      "loss: 0.017751, accuracy: 93.120%, batch [637440/756895]\n",
      "loss: 0.017874, accuracy: 93.140%, batch [638720/756895]\n",
      "loss: 0.017723, accuracy: 93.200%, batch [640000/756895]\n",
      "loss: 0.024562, accuracy: 92.971%, batch [641280/756895]\n",
      "loss: 0.023607, accuracy: 93.109%, batch [642560/756895]\n",
      "loss: 0.019693, accuracy: 92.836%, batch [643840/756895]\n",
      "loss: 0.017478, accuracy: 93.117%, batch [645120/756895]\n",
      "loss: 0.018357, accuracy: 93.135%, batch [646400/756895]\n",
      "loss: 0.016609, accuracy: 93.260%, batch [647680/756895]\n",
      "loss: 0.015472, accuracy: 93.035%, batch [648960/756895]\n",
      "loss: 0.018690, accuracy: 93.111%, batch [650240/756895]\n",
      "loss: 0.024911, accuracy: 92.800%, batch [651520/756895]\n",
      "loss: 0.019074, accuracy: 93.226%, batch [652800/756895]\n",
      "loss: 0.020205, accuracy: 92.919%, batch [654080/756895]\n",
      "loss: 0.019576, accuracy: 93.029%, batch [655360/756895]\n",
      "loss: 0.021143, accuracy: 93.054%, batch [656640/756895]\n",
      "loss: 0.017781, accuracy: 93.141%, batch [657920/756895]\n",
      "loss: 0.016840, accuracy: 93.258%, batch [659200/756895]\n",
      "loss: 0.017537, accuracy: 93.097%, batch [660480/756895]\n",
      "loss: 0.020584, accuracy: 92.856%, batch [661760/756895]\n",
      "loss: 0.027898, accuracy: 92.576%, batch [663040/756895]\n",
      "loss: 0.018128, accuracy: 93.042%, batch [664320/756895]\n",
      "loss: 0.016997, accuracy: 93.138%, batch [665600/756895]\n",
      "loss: 0.021710, accuracy: 93.126%, batch [666880/756895]\n",
      "loss: 0.016588, accuracy: 93.201%, batch [668160/756895]\n",
      "loss: 0.020232, accuracy: 93.003%, batch [669440/756895]\n",
      "loss: 0.019070, accuracy: 93.194%, batch [670720/756895]\n",
      "loss: 0.020536, accuracy: 92.992%, batch [672000/756895]\n",
      "loss: 0.018013, accuracy: 93.136%, batch [673280/756895]\n",
      "loss: 0.018482, accuracy: 93.105%, batch [674560/756895]\n",
      "loss: 0.018649, accuracy: 92.914%, batch [675840/756895]\n",
      "loss: 0.017386, accuracy: 93.068%, batch [677120/756895]\n",
      "loss: 0.018673, accuracy: 93.094%, batch [678400/756895]\n",
      "loss: 0.015744, accuracy: 93.348%, batch [679680/756895]\n",
      "loss: 0.016685, accuracy: 93.011%, batch [680960/756895]\n",
      "loss: 0.019085, accuracy: 93.063%, batch [682240/756895]\n",
      "loss: 0.018698, accuracy: 93.084%, batch [683520/756895]\n",
      "loss: 0.023269, accuracy: 93.072%, batch [684800/756895]\n",
      "loss: 0.018008, accuracy: 93.100%, batch [686080/756895]\n",
      "loss: 0.020681, accuracy: 93.065%, batch [687360/756895]\n",
      "loss: 0.021187, accuracy: 93.055%, batch [688640/756895]\n",
      "loss: 0.017447, accuracy: 92.983%, batch [689920/756895]\n",
      "loss: 0.021224, accuracy: 93.193%, batch [691200/756895]\n",
      "loss: 0.017007, accuracy: 93.188%, batch [692480/756895]\n",
      "loss: 0.020610, accuracy: 92.867%, batch [693760/756895]\n",
      "loss: 0.020043, accuracy: 92.971%, batch [695040/756895]\n",
      "loss: 0.019199, accuracy: 93.191%, batch [696320/756895]\n",
      "loss: 0.018818, accuracy: 93.163%, batch [697600/756895]\n",
      "loss: 0.016284, accuracy: 93.156%, batch [698880/756895]\n",
      "loss: 0.017022, accuracy: 93.236%, batch [700160/756895]\n",
      "loss: 0.019399, accuracy: 92.961%, batch [701440/756895]\n",
      "loss: 0.020053, accuracy: 92.993%, batch [702720/756895]\n",
      "loss: 0.022969, accuracy: 93.029%, batch [704000/756895]\n",
      "loss: 0.021901, accuracy: 93.116%, batch [705280/756895]\n",
      "loss: 0.018860, accuracy: 93.054%, batch [706560/756895]\n",
      "loss: 0.022467, accuracy: 93.053%, batch [707840/756895]\n",
      "loss: 0.019495, accuracy: 92.958%, batch [709120/756895]\n",
      "loss: 0.016931, accuracy: 93.070%, batch [710400/756895]\n",
      "loss: 0.022671, accuracy: 93.071%, batch [711680/756895]\n",
      "loss: 0.016583, accuracy: 92.990%, batch [712960/756895]\n",
      "loss: 0.017558, accuracy: 93.012%, batch [714240/756895]\n",
      "loss: 0.023218, accuracy: 92.976%, batch [715520/756895]\n",
      "loss: 0.015824, accuracy: 93.166%, batch [716800/756895]\n",
      "loss: 0.014704, accuracy: 93.363%, batch [718080/756895]\n",
      "loss: 0.022780, accuracy: 93.033%, batch [719360/756895]\n",
      "loss: 0.026070, accuracy: 92.896%, batch [720640/756895]\n",
      "loss: 0.019455, accuracy: 92.974%, batch [721920/756895]\n",
      "loss: 0.020064, accuracy: 93.038%, batch [723200/756895]\n",
      "loss: 0.020213, accuracy: 93.068%, batch [724480/756895]\n",
      "loss: 0.018304, accuracy: 93.089%, batch [725760/756895]\n",
      "loss: 0.018940, accuracy: 93.110%, batch [727040/756895]\n",
      "loss: 0.020059, accuracy: 92.800%, batch [728320/756895]\n",
      "loss: 0.026070, accuracy: 92.798%, batch [729600/756895]\n",
      "loss: 0.024463, accuracy: 92.987%, batch [730880/756895]\n",
      "loss: 0.020368, accuracy: 92.919%, batch [732160/756895]\n",
      "loss: 0.024458, accuracy: 92.739%, batch [733440/756895]\n",
      "loss: 0.017259, accuracy: 93.185%, batch [734720/756895]\n",
      "loss: 0.022485, accuracy: 92.913%, batch [736000/756895]\n",
      "loss: 0.024525, accuracy: 92.880%, batch [737280/756895]\n",
      "loss: 0.017140, accuracy: 93.196%, batch [738560/756895]\n",
      "loss: 0.016214, accuracy: 93.151%, batch [739840/756895]\n",
      "loss: 0.019247, accuracy: 93.199%, batch [741120/756895]\n",
      "loss: 0.016516, accuracy: 93.122%, batch [742400/756895]\n",
      "loss: 0.021065, accuracy: 92.972%, batch [743680/756895]\n",
      "loss: 0.022548, accuracy: 92.865%, batch [744960/756895]\n",
      "loss: 0.030791, accuracy: 92.553%, batch [746240/756895]\n",
      "loss: 0.015180, accuracy: 93.221%, batch [747520/756895]\n",
      "loss: 0.016392, accuracy: 93.062%, batch [748800/756895]\n",
      "loss: 0.021576, accuracy: 92.989%, batch [750080/756895]\n",
      "loss: 0.022294, accuracy: 93.028%, batch [751360/756895]\n",
      "loss: 0.023583, accuracy: 93.075%, batch [752640/756895]\n",
      "loss: 0.020852, accuracy: 93.015%, batch [753920/756895]\n",
      "loss: 0.019735, accuracy: 93.040%, batch [755200/756895]\n",
      "loss: 0.021927, accuracy: 92.923%, batch [756480/756895]\n",
      "Test avg loss: 0.020177, test avg accuracy: 92.999% \n",
      "\n",
      "Test avg loss: 0.019900, test avg accuracy: 93.027% \n",
      "\n",
      "Epoch 113\n",
      "------------------------\n",
      "loss: 0.016630, accuracy: 93.204%, batch [    0/756895]\n",
      "loss: 0.019004, accuracy: 93.040%, batch [ 1280/756895]\n",
      "loss: 0.015706, accuracy: 93.195%, batch [ 2560/756895]\n",
      "loss: 0.019912, accuracy: 92.968%, batch [ 3840/756895]\n",
      "loss: 0.018578, accuracy: 93.118%, batch [ 5120/756895]\n",
      "loss: 0.019621, accuracy: 93.235%, batch [ 6400/756895]\n",
      "loss: 0.017126, accuracy: 93.057%, batch [ 7680/756895]\n",
      "loss: 0.020023, accuracy: 92.934%, batch [ 8960/756895]\n",
      "loss: 0.016080, accuracy: 93.219%, batch [10240/756895]\n",
      "loss: 0.018672, accuracy: 93.214%, batch [11520/756895]\n",
      "loss: 0.019825, accuracy: 92.963%, batch [12800/756895]\n",
      "loss: 0.018291, accuracy: 93.119%, batch [14080/756895]\n",
      "loss: 0.016955, accuracy: 93.195%, batch [15360/756895]\n",
      "loss: 0.015379, accuracy: 93.122%, batch [16640/756895]\n",
      "loss: 0.022719, accuracy: 92.937%, batch [17920/756895]\n",
      "loss: 0.024177, accuracy: 92.904%, batch [19200/756895]\n",
      "loss: 0.020229, accuracy: 92.899%, batch [20480/756895]\n",
      "loss: 0.019654, accuracy: 93.149%, batch [21760/756895]\n",
      "loss: 0.017690, accuracy: 93.119%, batch [23040/756895]\n",
      "loss: 0.019988, accuracy: 93.077%, batch [24320/756895]\n",
      "loss: 0.016551, accuracy: 93.086%, batch [25600/756895]\n",
      "loss: 0.015666, accuracy: 93.324%, batch [26880/756895]\n",
      "loss: 0.015959, accuracy: 93.194%, batch [28160/756895]\n",
      "loss: 0.018248, accuracy: 93.091%, batch [29440/756895]\n",
      "loss: 0.016658, accuracy: 93.131%, batch [30720/756895]\n",
      "loss: 0.017576, accuracy: 93.022%, batch [32000/756895]\n",
      "loss: 0.021396, accuracy: 92.852%, batch [33280/756895]\n",
      "loss: 0.018822, accuracy: 93.277%, batch [34560/756895]\n",
      "loss: 0.016848, accuracy: 93.160%, batch [35840/756895]\n",
      "loss: 0.015492, accuracy: 93.287%, batch [37120/756895]\n",
      "loss: 0.019760, accuracy: 93.025%, batch [38400/756895]\n",
      "loss: 0.018140, accuracy: 93.111%, batch [39680/756895]\n",
      "loss: 0.017261, accuracy: 93.076%, batch [40960/756895]\n",
      "loss: 0.022221, accuracy: 92.760%, batch [42240/756895]\n",
      "loss: 0.018985, accuracy: 92.875%, batch [43520/756895]\n",
      "loss: 0.016314, accuracy: 93.172%, batch [44800/756895]\n",
      "loss: 0.019515, accuracy: 92.867%, batch [46080/756895]\n",
      "loss: 0.021962, accuracy: 93.137%, batch [47360/756895]\n",
      "loss: 0.019046, accuracy: 92.858%, batch [48640/756895]\n",
      "loss: 0.018372, accuracy: 92.997%, batch [49920/756895]\n",
      "loss: 0.017315, accuracy: 93.158%, batch [51200/756895]\n",
      "loss: 0.014268, accuracy: 93.194%, batch [52480/756895]\n",
      "loss: 0.022908, accuracy: 93.042%, batch [53760/756895]\n",
      "loss: 0.018850, accuracy: 93.139%, batch [55040/756895]\n",
      "loss: 0.015777, accuracy: 93.122%, batch [56320/756895]\n",
      "loss: 0.016861, accuracy: 93.095%, batch [57600/756895]\n",
      "loss: 0.015964, accuracy: 93.309%, batch [58880/756895]\n",
      "loss: 0.019330, accuracy: 93.059%, batch [60160/756895]\n",
      "loss: 0.015339, accuracy: 93.196%, batch [61440/756895]\n",
      "loss: 0.023465, accuracy: 92.890%, batch [62720/756895]\n",
      "loss: 0.019043, accuracy: 93.099%, batch [64000/756895]\n",
      "loss: 0.016386, accuracy: 93.153%, batch [65280/756895]\n",
      "loss: 0.015875, accuracy: 93.165%, batch [66560/756895]\n",
      "loss: 0.019461, accuracy: 92.970%, batch [67840/756895]\n",
      "loss: 0.019926, accuracy: 92.904%, batch [69120/756895]\n",
      "loss: 0.018472, accuracy: 93.003%, batch [70400/756895]\n",
      "loss: 0.016242, accuracy: 93.028%, batch [71680/756895]\n",
      "loss: 0.021196, accuracy: 92.956%, batch [72960/756895]\n",
      "loss: 0.017802, accuracy: 92.999%, batch [74240/756895]\n",
      "loss: 0.018251, accuracy: 93.140%, batch [75520/756895]\n",
      "loss: 0.019609, accuracy: 93.164%, batch [76800/756895]\n",
      "loss: 0.020557, accuracy: 92.950%, batch [78080/756895]\n",
      "loss: 0.024184, accuracy: 92.905%, batch [79360/756895]\n",
      "loss: 0.020931, accuracy: 92.972%, batch [80640/756895]\n",
      "loss: 0.018389, accuracy: 93.016%, batch [81920/756895]\n",
      "loss: 0.015837, accuracy: 93.202%, batch [83200/756895]\n",
      "loss: 0.020993, accuracy: 93.029%, batch [84480/756895]\n",
      "loss: 0.015397, accuracy: 93.320%, batch [85760/756895]\n",
      "loss: 0.017751, accuracy: 93.048%, batch [87040/756895]\n",
      "loss: 0.014493, accuracy: 93.099%, batch [88320/756895]\n",
      "loss: 0.015511, accuracy: 93.170%, batch [89600/756895]\n",
      "loss: 0.017940, accuracy: 93.236%, batch [90880/756895]\n",
      "loss: 0.018170, accuracy: 93.055%, batch [92160/756895]\n",
      "loss: 0.014668, accuracy: 93.191%, batch [93440/756895]\n",
      "loss: 0.016655, accuracy: 93.157%, batch [94720/756895]\n",
      "loss: 0.015550, accuracy: 93.122%, batch [96000/756895]\n",
      "loss: 0.022535, accuracy: 92.912%, batch [97280/756895]\n",
      "loss: 0.018227, accuracy: 93.039%, batch [98560/756895]\n",
      "loss: 0.020432, accuracy: 93.049%, batch [99840/756895]\n",
      "loss: 0.019720, accuracy: 93.108%, batch [101120/756895]\n",
      "loss: 0.017457, accuracy: 93.079%, batch [102400/756895]\n",
      "loss: 0.017375, accuracy: 93.107%, batch [103680/756895]\n",
      "loss: 0.021138, accuracy: 92.999%, batch [104960/756895]\n",
      "loss: 0.015867, accuracy: 93.283%, batch [106240/756895]\n",
      "loss: 0.018776, accuracy: 93.038%, batch [107520/756895]\n",
      "loss: 0.021714, accuracy: 92.837%, batch [108800/756895]\n",
      "loss: 0.018024, accuracy: 92.929%, batch [110080/756895]\n",
      "loss: 0.021867, accuracy: 92.929%, batch [111360/756895]\n",
      "loss: 0.016693, accuracy: 93.196%, batch [112640/756895]\n",
      "loss: 0.019871, accuracy: 93.015%, batch [113920/756895]\n",
      "loss: 0.016207, accuracy: 93.274%, batch [115200/756895]\n",
      "loss: 0.019438, accuracy: 92.886%, batch [116480/756895]\n",
      "loss: 0.019414, accuracy: 92.987%, batch [117760/756895]\n",
      "loss: 0.023074, accuracy: 92.964%, batch [119040/756895]\n",
      "loss: 0.018852, accuracy: 93.077%, batch [120320/756895]\n",
      "loss: 0.019584, accuracy: 93.110%, batch [121600/756895]\n",
      "loss: 0.022170, accuracy: 93.018%, batch [122880/756895]\n",
      "loss: 0.016945, accuracy: 93.247%, batch [124160/756895]\n",
      "loss: 0.016544, accuracy: 93.191%, batch [125440/756895]\n",
      "loss: 0.018192, accuracy: 93.009%, batch [126720/756895]\n",
      "loss: 0.022652, accuracy: 92.920%, batch [128000/756895]\n",
      "loss: 0.016439, accuracy: 93.084%, batch [129280/756895]\n",
      "loss: 0.018538, accuracy: 93.234%, batch [130560/756895]\n",
      "loss: 0.019558, accuracy: 92.988%, batch [131840/756895]\n",
      "loss: 0.015382, accuracy: 93.022%, batch [133120/756895]\n",
      "loss: 0.016884, accuracy: 93.055%, batch [134400/756895]\n",
      "loss: 0.018829, accuracy: 93.095%, batch [135680/756895]\n",
      "loss: 0.017910, accuracy: 93.041%, batch [136960/756895]\n",
      "loss: 0.016643, accuracy: 93.144%, batch [138240/756895]\n",
      "loss: 0.017283, accuracy: 93.141%, batch [139520/756895]\n",
      "loss: 0.016737, accuracy: 93.346%, batch [140800/756895]\n",
      "loss: 0.018379, accuracy: 93.013%, batch [142080/756895]\n",
      "loss: 0.021133, accuracy: 93.016%, batch [143360/756895]\n",
      "loss: 0.019995, accuracy: 92.905%, batch [144640/756895]\n",
      "loss: 0.016870, accuracy: 93.210%, batch [145920/756895]\n",
      "loss: 0.016868, accuracy: 93.211%, batch [147200/756895]\n",
      "loss: 0.018958, accuracy: 93.075%, batch [148480/756895]\n",
      "loss: 0.017494, accuracy: 92.928%, batch [149760/756895]\n",
      "loss: 0.016401, accuracy: 93.174%, batch [151040/756895]\n",
      "loss: 0.015774, accuracy: 93.189%, batch [152320/756895]\n",
      "loss: 0.015864, accuracy: 93.274%, batch [153600/756895]\n",
      "loss: 0.016617, accuracy: 93.092%, batch [154880/756895]\n",
      "loss: 0.019936, accuracy: 92.867%, batch [156160/756895]\n",
      "loss: 0.018499, accuracy: 93.071%, batch [157440/756895]\n",
      "loss: 0.018735, accuracy: 93.189%, batch [158720/756895]\n",
      "loss: 0.018168, accuracy: 93.084%, batch [160000/756895]\n",
      "loss: 0.028390, accuracy: 92.904%, batch [161280/756895]\n",
      "loss: 0.018051, accuracy: 93.105%, batch [162560/756895]\n",
      "loss: 0.020680, accuracy: 93.050%, batch [163840/756895]\n",
      "loss: 0.022697, accuracy: 92.991%, batch [165120/756895]\n",
      "loss: 0.018619, accuracy: 93.009%, batch [166400/756895]\n",
      "loss: 0.020408, accuracy: 92.989%, batch [167680/756895]\n",
      "loss: 0.022011, accuracy: 93.147%, batch [168960/756895]\n",
      "loss: 0.016883, accuracy: 93.161%, batch [170240/756895]\n",
      "loss: 0.016597, accuracy: 93.054%, batch [171520/756895]\n",
      "loss: 0.020039, accuracy: 93.047%, batch [172800/756895]\n",
      "loss: 0.017987, accuracy: 93.011%, batch [174080/756895]\n",
      "loss: 0.015620, accuracy: 93.279%, batch [175360/756895]\n",
      "loss: 0.016801, accuracy: 93.268%, batch [176640/756895]\n",
      "loss: 0.022125, accuracy: 92.806%, batch [177920/756895]\n",
      "loss: 0.017020, accuracy: 93.228%, batch [179200/756895]\n",
      "loss: 0.017287, accuracy: 93.091%, batch [180480/756895]\n",
      "loss: 0.019998, accuracy: 92.994%, batch [181760/756895]\n",
      "loss: 0.018003, accuracy: 93.054%, batch [183040/756895]\n",
      "loss: 0.014727, accuracy: 93.197%, batch [184320/756895]\n",
      "loss: 0.016852, accuracy: 93.135%, batch [185600/756895]\n",
      "loss: 0.015952, accuracy: 93.134%, batch [186880/756895]\n",
      "loss: 0.013541, accuracy: 93.173%, batch [188160/756895]\n",
      "loss: 0.020913, accuracy: 92.969%, batch [189440/756895]\n",
      "loss: 0.017334, accuracy: 93.054%, batch [190720/756895]\n",
      "loss: 0.016758, accuracy: 93.246%, batch [192000/756895]\n",
      "loss: 0.016017, accuracy: 93.162%, batch [193280/756895]\n",
      "loss: 0.018870, accuracy: 93.126%, batch [194560/756895]\n",
      "loss: 0.022113, accuracy: 93.022%, batch [195840/756895]\n",
      "loss: 0.020936, accuracy: 93.142%, batch [197120/756895]\n",
      "loss: 0.016682, accuracy: 93.196%, batch [198400/756895]\n",
      "loss: 0.022140, accuracy: 92.802%, batch [199680/756895]\n",
      "loss: 0.020128, accuracy: 93.004%, batch [200960/756895]\n",
      "loss: 0.020607, accuracy: 93.017%, batch [202240/756895]\n",
      "loss: 0.017353, accuracy: 92.906%, batch [203520/756895]\n",
      "loss: 0.015853, accuracy: 93.145%, batch [204800/756895]\n",
      "loss: 0.021190, accuracy: 92.873%, batch [206080/756895]\n",
      "loss: 0.016597, accuracy: 93.100%, batch [207360/756895]\n",
      "loss: 0.019351, accuracy: 92.986%, batch [208640/756895]\n",
      "loss: 0.017055, accuracy: 92.826%, batch [209920/756895]\n",
      "loss: 0.015604, accuracy: 93.081%, batch [211200/756895]\n",
      "loss: 0.019379, accuracy: 92.972%, batch [212480/756895]\n",
      "loss: 0.018652, accuracy: 93.072%, batch [213760/756895]\n",
      "loss: 0.016859, accuracy: 93.259%, batch [215040/756895]\n",
      "loss: 0.021194, accuracy: 92.772%, batch [216320/756895]\n",
      "loss: 0.019709, accuracy: 93.239%, batch [217600/756895]\n",
      "loss: 0.018878, accuracy: 92.922%, batch [218880/756895]\n",
      "loss: 0.018250, accuracy: 93.175%, batch [220160/756895]\n",
      "loss: 0.016889, accuracy: 93.098%, batch [221440/756895]\n",
      "loss: 0.020829, accuracy: 93.026%, batch [222720/756895]\n",
      "loss: 0.023166, accuracy: 92.761%, batch [224000/756895]\n",
      "loss: 0.019206, accuracy: 92.925%, batch [225280/756895]\n",
      "loss: 0.021354, accuracy: 92.813%, batch [226560/756895]\n",
      "loss: 0.018612, accuracy: 93.154%, batch [227840/756895]\n",
      "loss: 0.016208, accuracy: 93.138%, batch [229120/756895]\n",
      "loss: 0.020130, accuracy: 92.884%, batch [230400/756895]\n",
      "loss: 0.019502, accuracy: 93.002%, batch [231680/756895]\n",
      "loss: 0.025556, accuracy: 92.857%, batch [232960/756895]\n",
      "loss: 0.016384, accuracy: 93.179%, batch [234240/756895]\n",
      "loss: 0.021443, accuracy: 92.958%, batch [235520/756895]\n",
      "loss: 0.019203, accuracy: 92.974%, batch [236800/756895]\n",
      "loss: 0.023644, accuracy: 92.804%, batch [238080/756895]\n",
      "loss: 0.017520, accuracy: 93.017%, batch [239360/756895]\n",
      "loss: 0.016093, accuracy: 93.283%, batch [240640/756895]\n",
      "loss: 0.014454, accuracy: 93.264%, batch [241920/756895]\n",
      "loss: 0.017600, accuracy: 93.010%, batch [243200/756895]\n",
      "loss: 0.022911, accuracy: 92.721%, batch [244480/756895]\n",
      "loss: 0.018375, accuracy: 92.988%, batch [245760/756895]\n",
      "loss: 0.017497, accuracy: 92.945%, batch [247040/756895]\n",
      "loss: 0.019762, accuracy: 92.937%, batch [248320/756895]\n",
      "loss: 0.021162, accuracy: 92.948%, batch [249600/756895]\n",
      "loss: 0.021263, accuracy: 92.909%, batch [250880/756895]\n",
      "loss: 0.021645, accuracy: 93.027%, batch [252160/756895]\n",
      "loss: 0.022594, accuracy: 92.745%, batch [253440/756895]\n",
      "loss: 0.018918, accuracy: 92.991%, batch [254720/756895]\n",
      "loss: 0.018454, accuracy: 92.996%, batch [256000/756895]\n",
      "loss: 0.016935, accuracy: 93.075%, batch [257280/756895]\n",
      "loss: 0.017378, accuracy: 93.026%, batch [258560/756895]\n",
      "loss: 0.018603, accuracy: 93.013%, batch [259840/756895]\n",
      "loss: 0.019069, accuracy: 93.014%, batch [261120/756895]\n",
      "loss: 0.017176, accuracy: 93.235%, batch [262400/756895]\n",
      "loss: 0.019779, accuracy: 93.077%, batch [263680/756895]\n",
      "loss: 0.021220, accuracy: 93.095%, batch [264960/756895]\n",
      "loss: 0.014908, accuracy: 93.050%, batch [266240/756895]\n",
      "loss: 0.018831, accuracy: 93.209%, batch [267520/756895]\n",
      "loss: 0.020169, accuracy: 93.026%, batch [268800/756895]\n",
      "loss: 0.017723, accuracy: 93.077%, batch [270080/756895]\n",
      "loss: 0.022792, accuracy: 92.968%, batch [271360/756895]\n",
      "loss: 0.019589, accuracy: 93.040%, batch [272640/756895]\n",
      "loss: 0.016033, accuracy: 93.153%, batch [273920/756895]\n",
      "loss: 0.020823, accuracy: 93.058%, batch [275200/756895]\n",
      "loss: 0.019368, accuracy: 93.150%, batch [276480/756895]\n",
      "loss: 0.018758, accuracy: 93.180%, batch [277760/756895]\n",
      "loss: 0.019549, accuracy: 93.071%, batch [279040/756895]\n",
      "loss: 0.021940, accuracy: 92.782%, batch [280320/756895]\n",
      "loss: 0.018746, accuracy: 93.013%, batch [281600/756895]\n",
      "loss: 0.018607, accuracy: 93.212%, batch [282880/756895]\n",
      "loss: 0.020049, accuracy: 93.084%, batch [284160/756895]\n",
      "loss: 0.016908, accuracy: 93.134%, batch [285440/756895]\n",
      "loss: 0.014389, accuracy: 93.278%, batch [286720/756895]\n",
      "loss: 0.016883, accuracy: 93.192%, batch [288000/756895]\n",
      "loss: 0.025657, accuracy: 92.932%, batch [289280/756895]\n",
      "loss: 0.023503, accuracy: 92.925%, batch [290560/756895]\n",
      "loss: 0.018383, accuracy: 93.096%, batch [291840/756895]\n",
      "loss: 0.015431, accuracy: 93.101%, batch [293120/756895]\n",
      "loss: 0.019121, accuracy: 93.101%, batch [294400/756895]\n",
      "loss: 0.017384, accuracy: 93.282%, batch [295680/756895]\n",
      "loss: 0.016750, accuracy: 93.094%, batch [296960/756895]\n",
      "loss: 0.015400, accuracy: 93.179%, batch [298240/756895]\n",
      "loss: 0.018123, accuracy: 92.957%, batch [299520/756895]\n",
      "loss: 0.022776, accuracy: 92.891%, batch [300800/756895]\n",
      "loss: 0.022430, accuracy: 92.938%, batch [302080/756895]\n",
      "loss: 0.014970, accuracy: 93.115%, batch [303360/756895]\n",
      "loss: 0.017884, accuracy: 93.112%, batch [304640/756895]\n",
      "loss: 0.021871, accuracy: 93.017%, batch [305920/756895]\n",
      "loss: 0.022758, accuracy: 93.036%, batch [307200/756895]\n",
      "loss: 0.018737, accuracy: 93.083%, batch [308480/756895]\n",
      "loss: 0.019329, accuracy: 93.095%, batch [309760/756895]\n",
      "loss: 0.024750, accuracy: 93.031%, batch [311040/756895]\n",
      "loss: 0.015759, accuracy: 93.196%, batch [312320/756895]\n",
      "loss: 0.018967, accuracy: 93.112%, batch [313600/756895]\n",
      "loss: 0.021472, accuracy: 92.971%, batch [314880/756895]\n",
      "loss: 0.020327, accuracy: 92.991%, batch [316160/756895]\n",
      "loss: 0.018051, accuracy: 92.986%, batch [317440/756895]\n",
      "loss: 0.019213, accuracy: 93.165%, batch [318720/756895]\n",
      "loss: 0.020629, accuracy: 92.895%, batch [320000/756895]\n",
      "loss: 0.020443, accuracy: 92.984%, batch [321280/756895]\n",
      "loss: 0.016591, accuracy: 93.050%, batch [322560/756895]\n",
      "loss: 0.016960, accuracy: 93.049%, batch [323840/756895]\n",
      "loss: 0.021253, accuracy: 92.983%, batch [325120/756895]\n",
      "loss: 0.017645, accuracy: 93.135%, batch [326400/756895]\n",
      "loss: 0.018555, accuracy: 93.139%, batch [327680/756895]\n",
      "loss: 0.019901, accuracy: 93.086%, batch [328960/756895]\n",
      "loss: 0.022373, accuracy: 92.942%, batch [330240/756895]\n",
      "loss: 0.015763, accuracy: 93.030%, batch [331520/756895]\n",
      "loss: 0.017312, accuracy: 93.082%, batch [332800/756895]\n",
      "loss: 0.019320, accuracy: 92.943%, batch [334080/756895]\n",
      "loss: 0.018985, accuracy: 93.208%, batch [335360/756895]\n",
      "loss: 0.020241, accuracy: 93.030%, batch [336640/756895]\n",
      "loss: 0.015772, accuracy: 93.159%, batch [337920/756895]\n",
      "loss: 0.021732, accuracy: 92.994%, batch [339200/756895]\n",
      "loss: 0.018013, accuracy: 93.137%, batch [340480/756895]\n",
      "loss: 0.019079, accuracy: 93.060%, batch [341760/756895]\n",
      "loss: 0.017332, accuracy: 93.061%, batch [343040/756895]\n",
      "loss: 0.018842, accuracy: 93.168%, batch [344320/756895]\n",
      "loss: 0.022755, accuracy: 92.996%, batch [345600/756895]\n",
      "loss: 0.018060, accuracy: 93.084%, batch [346880/756895]\n",
      "loss: 0.017547, accuracy: 93.014%, batch [348160/756895]\n",
      "loss: 0.020639, accuracy: 93.082%, batch [349440/756895]\n",
      "loss: 0.017117, accuracy: 93.200%, batch [350720/756895]\n",
      "loss: 0.017442, accuracy: 93.234%, batch [352000/756895]\n",
      "loss: 0.019485, accuracy: 93.049%, batch [353280/756895]\n",
      "loss: 0.018500, accuracy: 93.011%, batch [354560/756895]\n",
      "loss: 0.016305, accuracy: 93.079%, batch [355840/756895]\n",
      "loss: 0.019562, accuracy: 92.839%, batch [357120/756895]\n",
      "loss: 0.016294, accuracy: 93.157%, batch [358400/756895]\n",
      "loss: 0.018266, accuracy: 93.225%, batch [359680/756895]\n",
      "loss: 0.020905, accuracy: 93.024%, batch [360960/756895]\n",
      "loss: 0.021582, accuracy: 93.085%, batch [362240/756895]\n",
      "loss: 0.016511, accuracy: 93.061%, batch [363520/756895]\n",
      "loss: 0.017346, accuracy: 92.957%, batch [364800/756895]\n",
      "loss: 0.024290, accuracy: 92.990%, batch [366080/756895]\n",
      "loss: 0.020463, accuracy: 93.053%, batch [367360/756895]\n",
      "loss: 0.019977, accuracy: 93.185%, batch [368640/756895]\n",
      "loss: 0.020591, accuracy: 92.883%, batch [369920/756895]\n",
      "loss: 0.018450, accuracy: 92.893%, batch [371200/756895]\n",
      "loss: 0.020081, accuracy: 93.049%, batch [372480/756895]\n",
      "loss: 0.017203, accuracy: 93.070%, batch [373760/756895]\n",
      "loss: 0.024589, accuracy: 92.977%, batch [375040/756895]\n",
      "loss: 0.021543, accuracy: 92.845%, batch [376320/756895]\n",
      "loss: 0.019247, accuracy: 92.961%, batch [377600/756895]\n",
      "loss: 0.017217, accuracy: 93.036%, batch [378880/756895]\n",
      "loss: 0.019230, accuracy: 93.019%, batch [380160/756895]\n",
      "loss: 0.028034, accuracy: 92.950%, batch [381440/756895]\n",
      "loss: 0.017647, accuracy: 93.249%, batch [382720/756895]\n",
      "loss: 0.024008, accuracy: 92.804%, batch [384000/756895]\n",
      "loss: 0.016217, accuracy: 93.296%, batch [385280/756895]\n",
      "loss: 0.016983, accuracy: 93.069%, batch [386560/756895]\n",
      "loss: 0.021347, accuracy: 93.129%, batch [387840/756895]\n",
      "loss: 0.021444, accuracy: 93.079%, batch [389120/756895]\n",
      "loss: 0.020604, accuracy: 93.038%, batch [390400/756895]\n",
      "loss: 0.018784, accuracy: 92.970%, batch [391680/756895]\n",
      "loss: 0.023832, accuracy: 92.714%, batch [392960/756895]\n",
      "loss: 0.016875, accuracy: 93.068%, batch [394240/756895]\n",
      "loss: 0.015295, accuracy: 93.322%, batch [395520/756895]\n",
      "loss: 0.023824, accuracy: 92.946%, batch [396800/756895]\n",
      "loss: 0.014790, accuracy: 93.322%, batch [398080/756895]\n",
      "loss: 0.018482, accuracy: 93.087%, batch [399360/756895]\n",
      "loss: 0.020437, accuracy: 93.016%, batch [400640/756895]\n",
      "loss: 0.021469, accuracy: 93.053%, batch [401920/756895]\n",
      "loss: 0.020700, accuracy: 92.969%, batch [403200/756895]\n",
      "loss: 0.016590, accuracy: 93.171%, batch [404480/756895]\n",
      "loss: 0.016816, accuracy: 93.022%, batch [405760/756895]\n",
      "loss: 0.021097, accuracy: 92.952%, batch [407040/756895]\n",
      "loss: 0.020894, accuracy: 92.992%, batch [408320/756895]\n",
      "loss: 0.018494, accuracy: 93.058%, batch [409600/756895]\n",
      "loss: 0.015844, accuracy: 93.265%, batch [410880/756895]\n",
      "loss: 0.017916, accuracy: 92.940%, batch [412160/756895]\n",
      "loss: 0.020394, accuracy: 92.904%, batch [413440/756895]\n",
      "loss: 0.018197, accuracy: 93.001%, batch [414720/756895]\n",
      "loss: 0.026452, accuracy: 92.750%, batch [416000/756895]\n",
      "loss: 0.021578, accuracy: 92.922%, batch [417280/756895]\n",
      "loss: 0.016306, accuracy: 93.172%, batch [418560/756895]\n",
      "loss: 0.017136, accuracy: 93.155%, batch [419840/756895]\n",
      "loss: 0.018634, accuracy: 92.939%, batch [421120/756895]\n",
      "loss: 0.018335, accuracy: 92.802%, batch [422400/756895]\n",
      "loss: 0.019822, accuracy: 93.077%, batch [423680/756895]\n",
      "loss: 0.020110, accuracy: 92.942%, batch [424960/756895]\n",
      "loss: 0.020410, accuracy: 92.996%, batch [426240/756895]\n",
      "loss: 0.018823, accuracy: 92.876%, batch [427520/756895]\n",
      "loss: 0.017435, accuracy: 93.301%, batch [428800/756895]\n",
      "loss: 0.015442, accuracy: 93.308%, batch [430080/756895]\n",
      "loss: 0.019994, accuracy: 93.199%, batch [431360/756895]\n",
      "loss: 0.019871, accuracy: 92.889%, batch [432640/756895]\n",
      "loss: 0.023970, accuracy: 93.161%, batch [433920/756895]\n",
      "loss: 0.018749, accuracy: 93.042%, batch [435200/756895]\n",
      "loss: 0.019836, accuracy: 93.049%, batch [436480/756895]\n",
      "loss: 0.026083, accuracy: 92.782%, batch [437760/756895]\n",
      "loss: 0.019771, accuracy: 93.231%, batch [439040/756895]\n",
      "loss: 0.013122, accuracy: 93.268%, batch [440320/756895]\n",
      "loss: 0.015193, accuracy: 93.216%, batch [441600/756895]\n",
      "loss: 0.014626, accuracy: 93.252%, batch [442880/756895]\n",
      "loss: 0.016061, accuracy: 93.203%, batch [444160/756895]\n",
      "loss: 0.021891, accuracy: 92.773%, batch [445440/756895]\n",
      "loss: 0.020769, accuracy: 93.073%, batch [446720/756895]\n",
      "loss: 0.020738, accuracy: 93.142%, batch [448000/756895]\n",
      "loss: 0.017915, accuracy: 92.997%, batch [449280/756895]\n",
      "loss: 0.015331, accuracy: 93.154%, batch [450560/756895]\n",
      "loss: 0.019109, accuracy: 92.779%, batch [451840/756895]\n",
      "loss: 0.017414, accuracy: 93.251%, batch [453120/756895]\n",
      "loss: 0.019423, accuracy: 93.042%, batch [454400/756895]\n",
      "loss: 0.017585, accuracy: 93.140%, batch [455680/756895]\n",
      "loss: 0.017076, accuracy: 93.097%, batch [456960/756895]\n",
      "loss: 0.023817, accuracy: 92.692%, batch [458240/756895]\n",
      "loss: 0.025494, accuracy: 92.915%, batch [459520/756895]\n",
      "loss: 0.019393, accuracy: 93.073%, batch [460800/756895]\n",
      "loss: 0.022224, accuracy: 92.776%, batch [462080/756895]\n",
      "loss: 0.016790, accuracy: 93.096%, batch [463360/756895]\n",
      "loss: 0.020496, accuracy: 92.958%, batch [464640/756895]\n",
      "loss: 0.016374, accuracy: 93.251%, batch [465920/756895]\n",
      "loss: 0.020314, accuracy: 92.992%, batch [467200/756895]\n",
      "loss: 0.017831, accuracy: 93.174%, batch [468480/756895]\n",
      "loss: 0.020229, accuracy: 92.987%, batch [469760/756895]\n",
      "loss: 0.020766, accuracy: 92.943%, batch [471040/756895]\n",
      "loss: 0.021144, accuracy: 92.856%, batch [472320/756895]\n",
      "loss: 0.014909, accuracy: 93.165%, batch [473600/756895]\n",
      "loss: 0.017664, accuracy: 93.096%, batch [474880/756895]\n",
      "loss: 0.019265, accuracy: 93.138%, batch [476160/756895]\n",
      "loss: 0.017860, accuracy: 93.169%, batch [477440/756895]\n",
      "loss: 0.018474, accuracy: 93.205%, batch [478720/756895]\n",
      "loss: 0.027398, accuracy: 92.716%, batch [480000/756895]\n",
      "loss: 0.019711, accuracy: 93.110%, batch [481280/756895]\n",
      "loss: 0.021615, accuracy: 93.012%, batch [482560/756895]\n",
      "loss: 0.017923, accuracy: 93.139%, batch [483840/756895]\n",
      "loss: 0.015783, accuracy: 93.224%, batch [485120/756895]\n",
      "loss: 0.022200, accuracy: 92.896%, batch [486400/756895]\n",
      "loss: 0.018267, accuracy: 93.208%, batch [487680/756895]\n",
      "loss: 0.017646, accuracy: 93.159%, batch [488960/756895]\n",
      "loss: 0.017158, accuracy: 93.296%, batch [490240/756895]\n",
      "loss: 0.018365, accuracy: 93.016%, batch [491520/756895]\n",
      "loss: 0.015488, accuracy: 93.182%, batch [492800/756895]\n",
      "loss: 0.016183, accuracy: 93.152%, batch [494080/756895]\n",
      "loss: 0.016678, accuracy: 93.150%, batch [495360/756895]\n",
      "loss: 0.016408, accuracy: 93.017%, batch [496640/756895]\n",
      "loss: 0.013835, accuracy: 93.474%, batch [497920/756895]\n",
      "loss: 0.022309, accuracy: 92.787%, batch [499200/756895]\n",
      "loss: 0.018112, accuracy: 92.968%, batch [500480/756895]\n",
      "loss: 0.028183, accuracy: 92.925%, batch [501760/756895]\n",
      "loss: 0.017990, accuracy: 93.233%, batch [503040/756895]\n",
      "loss: 0.017486, accuracy: 93.187%, batch [504320/756895]\n",
      "loss: 0.018716, accuracy: 93.210%, batch [505600/756895]\n",
      "loss: 0.015452, accuracy: 93.330%, batch [506880/756895]\n",
      "loss: 0.023182, accuracy: 92.872%, batch [508160/756895]\n",
      "loss: 0.017190, accuracy: 93.323%, batch [509440/756895]\n",
      "loss: 0.017987, accuracy: 93.074%, batch [510720/756895]\n",
      "loss: 0.023215, accuracy: 92.902%, batch [512000/756895]\n",
      "loss: 0.018796, accuracy: 93.045%, batch [513280/756895]\n",
      "loss: 0.020750, accuracy: 93.174%, batch [514560/756895]\n",
      "loss: 0.020143, accuracy: 93.029%, batch [515840/756895]\n",
      "loss: 0.014294, accuracy: 93.174%, batch [517120/756895]\n",
      "loss: 0.015456, accuracy: 93.259%, batch [518400/756895]\n",
      "loss: 0.016374, accuracy: 93.251%, batch [519680/756895]\n",
      "loss: 0.014084, accuracy: 93.340%, batch [520960/756895]\n",
      "loss: 0.014978, accuracy: 93.244%, batch [522240/756895]\n",
      "loss: 0.019309, accuracy: 93.106%, batch [523520/756895]\n",
      "loss: 0.019954, accuracy: 93.036%, batch [524800/756895]\n",
      "loss: 0.019624, accuracy: 92.944%, batch [526080/756895]\n",
      "loss: 0.018005, accuracy: 93.055%, batch [527360/756895]\n",
      "loss: 0.017166, accuracy: 93.177%, batch [528640/756895]\n",
      "loss: 0.015749, accuracy: 93.181%, batch [529920/756895]\n",
      "loss: 0.015787, accuracy: 93.076%, batch [531200/756895]\n",
      "loss: 0.019348, accuracy: 93.191%, batch [532480/756895]\n",
      "loss: 0.019717, accuracy: 93.015%, batch [533760/756895]\n",
      "loss: 0.017164, accuracy: 93.044%, batch [535040/756895]\n",
      "loss: 0.015894, accuracy: 92.981%, batch [536320/756895]\n",
      "loss: 0.014910, accuracy: 93.154%, batch [537600/756895]\n",
      "loss: 0.016235, accuracy: 93.199%, batch [538880/756895]\n",
      "loss: 0.015104, accuracy: 93.207%, batch [540160/756895]\n",
      "loss: 0.018862, accuracy: 93.091%, batch [541440/756895]\n",
      "loss: 0.018425, accuracy: 93.154%, batch [542720/756895]\n",
      "loss: 0.027018, accuracy: 92.797%, batch [544000/756895]\n",
      "loss: 0.020041, accuracy: 92.983%, batch [545280/756895]\n",
      "loss: 0.015664, accuracy: 93.173%, batch [546560/756895]\n",
      "loss: 0.019820, accuracy: 92.950%, batch [547840/756895]\n",
      "loss: 0.018646, accuracy: 93.041%, batch [549120/756895]\n",
      "loss: 0.026581, accuracy: 92.997%, batch [550400/756895]\n",
      "loss: 0.018255, accuracy: 93.144%, batch [551680/756895]\n",
      "loss: 0.017223, accuracy: 93.112%, batch [552960/756895]\n",
      "loss: 0.015990, accuracy: 93.219%, batch [554240/756895]\n",
      "loss: 0.018909, accuracy: 92.930%, batch [555520/756895]\n",
      "loss: 0.020519, accuracy: 92.887%, batch [556800/756895]\n",
      "loss: 0.018711, accuracy: 93.036%, batch [558080/756895]\n",
      "loss: 0.014971, accuracy: 93.354%, batch [559360/756895]\n",
      "loss: 0.019071, accuracy: 93.052%, batch [560640/756895]\n",
      "loss: 0.015183, accuracy: 92.937%, batch [561920/756895]\n",
      "loss: 0.020753, accuracy: 93.234%, batch [563200/756895]\n",
      "loss: 0.015807, accuracy: 93.228%, batch [564480/756895]\n",
      "loss: 0.021017, accuracy: 92.990%, batch [565760/756895]\n",
      "loss: 0.015077, accuracy: 93.202%, batch [567040/756895]\n",
      "loss: 0.019256, accuracy: 93.176%, batch [568320/756895]\n",
      "loss: 0.017784, accuracy: 93.165%, batch [569600/756895]\n",
      "loss: 0.016742, accuracy: 93.130%, batch [570880/756895]\n",
      "loss: 0.019397, accuracy: 93.149%, batch [572160/756895]\n",
      "loss: 0.019922, accuracy: 92.972%, batch [573440/756895]\n",
      "loss: 0.015753, accuracy: 93.278%, batch [574720/756895]\n",
      "loss: 0.016643, accuracy: 93.182%, batch [576000/756895]\n",
      "loss: 0.015928, accuracy: 93.176%, batch [577280/756895]\n",
      "loss: 0.018629, accuracy: 93.242%, batch [578560/756895]\n",
      "loss: 0.015939, accuracy: 93.012%, batch [579840/756895]\n",
      "loss: 0.015986, accuracy: 93.340%, batch [581120/756895]\n",
      "loss: 0.017533, accuracy: 93.065%, batch [582400/756895]\n",
      "loss: 0.017545, accuracy: 93.016%, batch [583680/756895]\n",
      "loss: 0.015068, accuracy: 93.159%, batch [584960/756895]\n",
      "loss: 0.016332, accuracy: 93.071%, batch [586240/756895]\n",
      "loss: 0.020923, accuracy: 92.951%, batch [587520/756895]\n",
      "loss: 0.018145, accuracy: 93.055%, batch [588800/756895]\n",
      "loss: 0.017813, accuracy: 93.158%, batch [590080/756895]\n",
      "loss: 0.018073, accuracy: 93.172%, batch [591360/756895]\n",
      "loss: 0.023975, accuracy: 92.806%, batch [592640/756895]\n",
      "loss: 0.031828, accuracy: 92.821%, batch [593920/756895]\n",
      "loss: 0.019547, accuracy: 93.144%, batch [595200/756895]\n",
      "loss: 0.018224, accuracy: 93.092%, batch [596480/756895]\n",
      "loss: 0.021423, accuracy: 92.965%, batch [597760/756895]\n",
      "loss: 0.019132, accuracy: 93.023%, batch [599040/756895]\n",
      "loss: 0.018811, accuracy: 93.092%, batch [600320/756895]\n",
      "loss: 0.016416, accuracy: 93.327%, batch [601600/756895]\n",
      "loss: 0.016401, accuracy: 93.204%, batch [602880/756895]\n",
      "loss: 0.017880, accuracy: 93.008%, batch [604160/756895]\n",
      "loss: 0.017658, accuracy: 93.205%, batch [605440/756895]\n",
      "loss: 0.021522, accuracy: 92.991%, batch [606720/756895]\n",
      "loss: 0.013835, accuracy: 93.198%, batch [608000/756895]\n",
      "loss: 0.018283, accuracy: 93.058%, batch [609280/756895]\n",
      "loss: 0.019725, accuracy: 93.006%, batch [610560/756895]\n",
      "loss: 0.020628, accuracy: 93.088%, batch [611840/756895]\n",
      "loss: 0.015992, accuracy: 93.186%, batch [613120/756895]\n",
      "loss: 0.014980, accuracy: 93.086%, batch [614400/756895]\n",
      "loss: 0.015638, accuracy: 93.235%, batch [615680/756895]\n",
      "loss: 0.017736, accuracy: 93.271%, batch [616960/756895]\n",
      "loss: 0.019391, accuracy: 92.962%, batch [618240/756895]\n",
      "loss: 0.021788, accuracy: 93.044%, batch [619520/756895]\n",
      "loss: 0.016482, accuracy: 93.266%, batch [620800/756895]\n",
      "loss: 0.013566, accuracy: 93.267%, batch [622080/756895]\n",
      "loss: 0.015256, accuracy: 93.170%, batch [623360/756895]\n",
      "loss: 0.018433, accuracy: 93.002%, batch [624640/756895]\n",
      "loss: 0.018080, accuracy: 93.055%, batch [625920/756895]\n",
      "loss: 0.018695, accuracy: 93.128%, batch [627200/756895]\n",
      "loss: 0.016383, accuracy: 93.215%, batch [628480/756895]\n",
      "loss: 0.018876, accuracy: 92.961%, batch [629760/756895]\n",
      "loss: 0.016622, accuracy: 93.053%, batch [631040/756895]\n",
      "loss: 0.016816, accuracy: 93.106%, batch [632320/756895]\n",
      "loss: 0.019863, accuracy: 93.055%, batch [633600/756895]\n",
      "loss: 0.020325, accuracy: 93.098%, batch [634880/756895]\n",
      "loss: 0.017914, accuracy: 92.966%, batch [636160/756895]\n",
      "loss: 0.019624, accuracy: 93.146%, batch [637440/756895]\n",
      "loss: 0.016132, accuracy: 93.240%, batch [638720/756895]\n",
      "loss: 0.021072, accuracy: 92.863%, batch [640000/756895]\n",
      "loss: 0.023860, accuracy: 92.962%, batch [641280/756895]\n",
      "loss: 0.020269, accuracy: 93.053%, batch [642560/756895]\n",
      "loss: 0.015810, accuracy: 93.253%, batch [643840/756895]\n",
      "loss: 0.018189, accuracy: 92.937%, batch [645120/756895]\n",
      "loss: 0.017043, accuracy: 93.087%, batch [646400/756895]\n",
      "loss: 0.012776, accuracy: 93.411%, batch [647680/756895]\n",
      "loss: 0.017769, accuracy: 93.227%, batch [648960/756895]\n",
      "loss: 0.018230, accuracy: 93.154%, batch [650240/756895]\n",
      "loss: 0.017994, accuracy: 93.127%, batch [651520/756895]\n",
      "loss: 0.016772, accuracy: 93.250%, batch [652800/756895]\n",
      "loss: 0.015636, accuracy: 93.174%, batch [654080/756895]\n",
      "loss: 0.015213, accuracy: 93.147%, batch [655360/756895]\n",
      "loss: 0.022008, accuracy: 92.906%, batch [656640/756895]\n",
      "loss: 0.016526, accuracy: 93.133%, batch [657920/756895]\n",
      "loss: 0.016782, accuracy: 93.072%, batch [659200/756895]\n",
      "loss: 0.021451, accuracy: 92.966%, batch [660480/756895]\n",
      "loss: 0.021895, accuracy: 93.075%, batch [661760/756895]\n",
      "loss: 0.022172, accuracy: 92.924%, batch [663040/756895]\n",
      "loss: 0.024086, accuracy: 92.998%, batch [664320/756895]\n",
      "loss: 0.022528, accuracy: 93.078%, batch [665600/756895]\n",
      "loss: 0.019327, accuracy: 93.043%, batch [666880/756895]\n",
      "loss: 0.017778, accuracy: 93.168%, batch [668160/756895]\n",
      "loss: 0.020173, accuracy: 92.927%, batch [669440/756895]\n",
      "loss: 0.021542, accuracy: 93.028%, batch [670720/756895]\n",
      "loss: 0.016096, accuracy: 93.163%, batch [672000/756895]\n",
      "loss: 0.017953, accuracy: 93.061%, batch [673280/756895]\n",
      "loss: 0.020396, accuracy: 92.699%, batch [674560/756895]\n",
      "loss: 0.015576, accuracy: 93.286%, batch [675840/756895]\n",
      "loss: 0.017595, accuracy: 93.062%, batch [677120/756895]\n",
      "loss: 0.016348, accuracy: 93.110%, batch [678400/756895]\n",
      "loss: 0.020143, accuracy: 92.879%, batch [679680/756895]\n",
      "loss: 0.015294, accuracy: 93.244%, batch [680960/756895]\n",
      "loss: 0.020126, accuracy: 92.978%, batch [682240/756895]\n",
      "loss: 0.021601, accuracy: 92.887%, batch [683520/756895]\n",
      "loss: 0.019126, accuracy: 92.942%, batch [684800/756895]\n",
      "loss: 0.021251, accuracy: 92.752%, batch [686080/756895]\n",
      "loss: 0.021489, accuracy: 93.087%, batch [687360/756895]\n",
      "loss: 0.016712, accuracy: 93.128%, batch [688640/756895]\n",
      "loss: 0.017133, accuracy: 93.053%, batch [689920/756895]\n",
      "loss: 0.021718, accuracy: 92.893%, batch [691200/756895]\n",
      "loss: 0.020002, accuracy: 92.889%, batch [692480/756895]\n",
      "loss: 0.017725, accuracy: 93.089%, batch [693760/756895]\n",
      "loss: 0.014788, accuracy: 93.175%, batch [695040/756895]\n",
      "loss: 0.019910, accuracy: 93.230%, batch [696320/756895]\n",
      "loss: 0.018276, accuracy: 92.944%, batch [697600/756895]\n",
      "loss: 0.015756, accuracy: 93.203%, batch [698880/756895]\n",
      "loss: 0.015490, accuracy: 93.100%, batch [700160/756895]\n",
      "loss: 0.014200, accuracy: 93.296%, batch [701440/756895]\n",
      "loss: 0.016017, accuracy: 93.269%, batch [702720/756895]\n",
      "loss: 0.020075, accuracy: 93.051%, batch [704000/756895]\n",
      "loss: 0.018976, accuracy: 93.047%, batch [705280/756895]\n",
      "loss: 0.019246, accuracy: 93.236%, batch [706560/756895]\n",
      "loss: 0.023248, accuracy: 92.926%, batch [707840/756895]\n",
      "loss: 0.022211, accuracy: 92.870%, batch [709120/756895]\n",
      "loss: 0.018803, accuracy: 92.962%, batch [710400/756895]\n",
      "loss: 0.020375, accuracy: 92.959%, batch [711680/756895]\n",
      "loss: 0.023165, accuracy: 92.780%, batch [712960/756895]\n",
      "loss: 0.019600, accuracy: 92.787%, batch [714240/756895]\n",
      "loss: 0.017182, accuracy: 93.173%, batch [715520/756895]\n",
      "loss: 0.023677, accuracy: 92.744%, batch [716800/756895]\n",
      "loss: 0.018425, accuracy: 93.197%, batch [718080/756895]\n",
      "loss: 0.016446, accuracy: 92.911%, batch [719360/756895]\n",
      "loss: 0.023239, accuracy: 93.088%, batch [720640/756895]\n",
      "loss: 0.022243, accuracy: 93.066%, batch [721920/756895]\n",
      "loss: 0.020363, accuracy: 92.744%, batch [723200/756895]\n",
      "loss: 0.018916, accuracy: 92.922%, batch [724480/756895]\n",
      "loss: 0.021228, accuracy: 92.824%, batch [725760/756895]\n",
      "loss: 0.017701, accuracy: 93.218%, batch [727040/756895]\n",
      "loss: 0.017383, accuracy: 93.138%, batch [728320/756895]\n",
      "loss: 0.020550, accuracy: 93.009%, batch [729600/756895]\n",
      "loss: 0.018425, accuracy: 93.186%, batch [730880/756895]\n",
      "loss: 0.026678, accuracy: 92.728%, batch [732160/756895]\n",
      "loss: 0.016865, accuracy: 93.151%, batch [733440/756895]\n",
      "loss: 0.020810, accuracy: 92.848%, batch [734720/756895]\n",
      "loss: 0.018673, accuracy: 93.042%, batch [736000/756895]\n",
      "loss: 0.020342, accuracy: 93.219%, batch [737280/756895]\n",
      "loss: 0.016836, accuracy: 93.168%, batch [738560/756895]\n",
      "loss: 0.017273, accuracy: 93.336%, batch [739840/756895]\n",
      "loss: 0.019916, accuracy: 93.033%, batch [741120/756895]\n",
      "loss: 0.020909, accuracy: 93.075%, batch [742400/756895]\n",
      "loss: 0.018101, accuracy: 93.044%, batch [743680/756895]\n",
      "loss: 0.015598, accuracy: 93.210%, batch [744960/756895]\n",
      "loss: 0.020373, accuracy: 92.912%, batch [746240/756895]\n",
      "loss: 0.019013, accuracy: 93.099%, batch [747520/756895]\n",
      "loss: 0.017471, accuracy: 93.194%, batch [748800/756895]\n",
      "loss: 0.018897, accuracy: 93.081%, batch [750080/756895]\n",
      "loss: 0.025646, accuracy: 93.185%, batch [751360/756895]\n",
      "loss: 0.019186, accuracy: 93.052%, batch [752640/756895]\n",
      "loss: 0.019210, accuracy: 93.042%, batch [753920/756895]\n",
      "loss: 0.019142, accuracy: 93.150%, batch [755200/756895]\n",
      "loss: 0.026060, accuracy: 92.879%, batch [756480/756895]\n",
      "Test avg loss: 0.019418, test avg accuracy: 93.032% \n",
      "\n",
      "Test avg loss: 0.019359, test avg accuracy: 93.054% \n",
      "\n",
      "Epoch 114\n",
      "------------------------\n",
      "loss: 0.019028, accuracy: 92.961%, batch [    0/756895]\n",
      "loss: 0.014446, accuracy: 93.245%, batch [ 1280/756895]\n",
      "loss: 0.018320, accuracy: 92.950%, batch [ 2560/756895]\n",
      "loss: 0.015957, accuracy: 93.196%, batch [ 3840/756895]\n",
      "loss: 0.016768, accuracy: 93.115%, batch [ 5120/756895]\n",
      "loss: 0.018831, accuracy: 93.252%, batch [ 6400/756895]\n",
      "loss: 0.022105, accuracy: 93.081%, batch [ 7680/756895]\n",
      "loss: 0.018050, accuracy: 93.006%, batch [ 8960/756895]\n",
      "loss: 0.020409, accuracy: 93.072%, batch [10240/756895]\n",
      "loss: 0.017252, accuracy: 93.089%, batch [11520/756895]\n",
      "loss: 0.019838, accuracy: 92.924%, batch [12800/756895]\n",
      "loss: 0.013632, accuracy: 93.307%, batch [14080/756895]\n",
      "loss: 0.016718, accuracy: 93.214%, batch [15360/756895]\n",
      "loss: 0.017289, accuracy: 93.133%, batch [16640/756895]\n",
      "loss: 0.018244, accuracy: 93.031%, batch [17920/756895]\n",
      "loss: 0.020018, accuracy: 93.158%, batch [19200/756895]\n",
      "loss: 0.017447, accuracy: 93.174%, batch [20480/756895]\n",
      "loss: 0.021043, accuracy: 93.073%, batch [21760/756895]\n",
      "loss: 0.015486, accuracy: 93.173%, batch [23040/756895]\n",
      "loss: 0.018544, accuracy: 92.972%, batch [24320/756895]\n",
      "loss: 0.017428, accuracy: 93.067%, batch [25600/756895]\n",
      "loss: 0.018465, accuracy: 93.068%, batch [26880/756895]\n",
      "loss: 0.025326, accuracy: 92.884%, batch [28160/756895]\n",
      "loss: 0.024060, accuracy: 93.107%, batch [29440/756895]\n",
      "loss: 0.018698, accuracy: 93.155%, batch [30720/756895]\n",
      "loss: 0.014283, accuracy: 93.233%, batch [32000/756895]\n",
      "loss: 0.016528, accuracy: 93.155%, batch [33280/756895]\n",
      "loss: 0.016828, accuracy: 93.284%, batch [34560/756895]\n",
      "loss: 0.024027, accuracy: 93.031%, batch [35840/756895]\n",
      "loss: 0.017599, accuracy: 93.095%, batch [37120/756895]\n",
      "loss: 0.022162, accuracy: 92.727%, batch [38400/756895]\n",
      "loss: 0.020512, accuracy: 92.944%, batch [39680/756895]\n",
      "loss: 0.019541, accuracy: 93.134%, batch [40960/756895]\n",
      "loss: 0.018455, accuracy: 93.098%, batch [42240/756895]\n",
      "loss: 0.017482, accuracy: 92.960%, batch [43520/756895]\n",
      "loss: 0.016623, accuracy: 93.017%, batch [44800/756895]\n",
      "loss: 0.017704, accuracy: 93.226%, batch [46080/756895]\n",
      "loss: 0.018865, accuracy: 93.069%, batch [47360/756895]\n",
      "loss: 0.021957, accuracy: 93.018%, batch [48640/756895]\n",
      "loss: 0.017613, accuracy: 93.148%, batch [49920/756895]\n",
      "loss: 0.020040, accuracy: 92.885%, batch [51200/756895]\n",
      "loss: 0.017325, accuracy: 93.237%, batch [52480/756895]\n",
      "loss: 0.018011, accuracy: 93.322%, batch [53760/756895]\n",
      "loss: 0.016429, accuracy: 93.343%, batch [55040/756895]\n",
      "loss: 0.016589, accuracy: 93.061%, batch [56320/756895]\n",
      "loss: 0.022825, accuracy: 92.798%, batch [57600/756895]\n",
      "loss: 0.019093, accuracy: 92.978%, batch [58880/756895]\n",
      "loss: 0.020363, accuracy: 93.147%, batch [60160/756895]\n",
      "loss: 0.016321, accuracy: 93.020%, batch [61440/756895]\n",
      "loss: 0.020528, accuracy: 92.987%, batch [62720/756895]\n",
      "loss: 0.020797, accuracy: 93.007%, batch [64000/756895]\n",
      "loss: 0.021458, accuracy: 92.885%, batch [65280/756895]\n",
      "loss: 0.018826, accuracy: 93.135%, batch [66560/756895]\n",
      "loss: 0.021002, accuracy: 93.123%, batch [67840/756895]\n",
      "loss: 0.017394, accuracy: 93.033%, batch [69120/756895]\n",
      "loss: 0.018157, accuracy: 93.062%, batch [70400/756895]\n",
      "loss: 0.016581, accuracy: 93.116%, batch [71680/756895]\n",
      "loss: 0.019133, accuracy: 92.982%, batch [72960/756895]\n",
      "loss: 0.026179, accuracy: 92.677%, batch [74240/756895]\n",
      "loss: 0.017826, accuracy: 93.099%, batch [75520/756895]\n",
      "loss: 0.018043, accuracy: 93.089%, batch [76800/756895]\n",
      "loss: 0.016866, accuracy: 93.131%, batch [78080/756895]\n",
      "loss: 0.020506, accuracy: 92.770%, batch [79360/756895]\n",
      "loss: 0.019430, accuracy: 92.982%, batch [80640/756895]\n",
      "loss: 0.019559, accuracy: 93.008%, batch [81920/756895]\n",
      "loss: 0.022707, accuracy: 93.066%, batch [83200/756895]\n",
      "loss: 0.014698, accuracy: 93.084%, batch [84480/756895]\n",
      "loss: 0.017503, accuracy: 93.198%, batch [85760/756895]\n",
      "loss: 0.017558, accuracy: 93.053%, batch [87040/756895]\n",
      "loss: 0.020855, accuracy: 93.008%, batch [88320/756895]\n",
      "loss: 0.022114, accuracy: 92.956%, batch [89600/756895]\n",
      "loss: 0.016545, accuracy: 93.060%, batch [90880/756895]\n",
      "loss: 0.021928, accuracy: 92.918%, batch [92160/756895]\n",
      "loss: 0.016640, accuracy: 93.119%, batch [93440/756895]\n",
      "loss: 0.018097, accuracy: 93.228%, batch [94720/756895]\n",
      "loss: 0.019996, accuracy: 93.116%, batch [96000/756895]\n",
      "loss: 0.018750, accuracy: 92.947%, batch [97280/756895]\n",
      "loss: 0.016566, accuracy: 93.211%, batch [98560/756895]\n",
      "loss: 0.023572, accuracy: 93.023%, batch [99840/756895]\n",
      "loss: 0.016362, accuracy: 93.103%, batch [101120/756895]\n",
      "loss: 0.019776, accuracy: 93.052%, batch [102400/756895]\n",
      "loss: 0.016041, accuracy: 93.195%, batch [103680/756895]\n",
      "loss: 0.030637, accuracy: 92.896%, batch [104960/756895]\n",
      "loss: 0.025726, accuracy: 92.933%, batch [106240/756895]\n",
      "loss: 0.014600, accuracy: 93.181%, batch [107520/756895]\n",
      "loss: 0.018062, accuracy: 93.105%, batch [108800/756895]\n",
      "loss: 0.018537, accuracy: 93.010%, batch [110080/756895]\n",
      "loss: 0.021176, accuracy: 92.771%, batch [111360/756895]\n",
      "loss: 0.016600, accuracy: 93.123%, batch [112640/756895]\n",
      "loss: 0.020426, accuracy: 92.890%, batch [113920/756895]\n",
      "loss: 0.015232, accuracy: 93.185%, batch [115200/756895]\n",
      "loss: 0.016319, accuracy: 93.156%, batch [116480/756895]\n",
      "loss: 0.026982, accuracy: 92.904%, batch [117760/756895]\n",
      "loss: 0.020205, accuracy: 93.087%, batch [119040/756895]\n",
      "loss: 0.017123, accuracy: 93.245%, batch [120320/756895]\n",
      "loss: 0.018692, accuracy: 93.163%, batch [121600/756895]\n",
      "loss: 0.017816, accuracy: 93.125%, batch [122880/756895]\n",
      "loss: 0.021345, accuracy: 92.954%, batch [124160/756895]\n",
      "loss: 0.017704, accuracy: 93.211%, batch [125440/756895]\n",
      "loss: 0.018870, accuracy: 93.070%, batch [126720/756895]\n",
      "loss: 0.018029, accuracy: 93.127%, batch [128000/756895]\n",
      "loss: 0.017545, accuracy: 93.183%, batch [129280/756895]\n",
      "loss: 0.020927, accuracy: 92.840%, batch [130560/756895]\n",
      "loss: 0.018843, accuracy: 92.938%, batch [131840/756895]\n",
      "loss: 0.014954, accuracy: 93.230%, batch [133120/756895]\n",
      "loss: 0.016036, accuracy: 93.116%, batch [134400/756895]\n",
      "loss: 0.017999, accuracy: 93.064%, batch [135680/756895]\n",
      "loss: 0.015626, accuracy: 93.270%, batch [136960/756895]\n",
      "loss: 0.020001, accuracy: 92.930%, batch [138240/756895]\n",
      "loss: 0.021275, accuracy: 93.027%, batch [139520/756895]\n",
      "loss: 0.018381, accuracy: 93.051%, batch [140800/756895]\n",
      "loss: 0.016438, accuracy: 93.142%, batch [142080/756895]\n",
      "loss: 0.018112, accuracy: 92.948%, batch [143360/756895]\n",
      "loss: 0.016843, accuracy: 93.188%, batch [144640/756895]\n",
      "loss: 0.014667, accuracy: 93.418%, batch [145920/756895]\n",
      "loss: 0.019223, accuracy: 93.277%, batch [147200/756895]\n",
      "loss: 0.016586, accuracy: 93.039%, batch [148480/756895]\n",
      "loss: 0.017621, accuracy: 93.093%, batch [149760/756895]\n",
      "loss: 0.021363, accuracy: 92.958%, batch [151040/756895]\n",
      "loss: 0.021307, accuracy: 92.684%, batch [152320/756895]\n",
      "loss: 0.019106, accuracy: 92.975%, batch [153600/756895]\n",
      "loss: 0.024814, accuracy: 93.027%, batch [154880/756895]\n",
      "loss: 0.020729, accuracy: 93.084%, batch [156160/756895]\n",
      "loss: 0.018879, accuracy: 93.082%, batch [157440/756895]\n",
      "loss: 0.017777, accuracy: 93.086%, batch [158720/756895]\n",
      "loss: 0.016945, accuracy: 93.057%, batch [160000/756895]\n",
      "loss: 0.023778, accuracy: 92.750%, batch [161280/756895]\n",
      "loss: 0.017703, accuracy: 93.054%, batch [162560/756895]\n",
      "loss: 0.019900, accuracy: 92.913%, batch [163840/756895]\n",
      "loss: 0.018076, accuracy: 93.012%, batch [165120/756895]\n",
      "loss: 0.016049, accuracy: 93.318%, batch [166400/756895]\n",
      "loss: 0.015463, accuracy: 93.143%, batch [167680/756895]\n",
      "loss: 0.016989, accuracy: 93.040%, batch [168960/756895]\n",
      "loss: 0.018939, accuracy: 93.024%, batch [170240/756895]\n",
      "loss: 0.021678, accuracy: 92.982%, batch [171520/756895]\n",
      "loss: 0.015479, accuracy: 93.217%, batch [172800/756895]\n",
      "loss: 0.018889, accuracy: 92.908%, batch [174080/756895]\n",
      "loss: 0.020455, accuracy: 93.026%, batch [175360/756895]\n",
      "loss: 0.018821, accuracy: 92.893%, batch [176640/756895]\n",
      "loss: 0.016611, accuracy: 93.145%, batch [177920/756895]\n",
      "loss: 0.017159, accuracy: 93.051%, batch [179200/756895]\n",
      "loss: 0.021506, accuracy: 92.799%, batch [180480/756895]\n",
      "loss: 0.018396, accuracy: 92.984%, batch [181760/756895]\n",
      "loss: 0.016226, accuracy: 93.241%, batch [183040/756895]\n",
      "loss: 0.018031, accuracy: 93.091%, batch [184320/756895]\n",
      "loss: 0.017231, accuracy: 93.029%, batch [185600/756895]\n",
      "loss: 0.020930, accuracy: 92.890%, batch [186880/756895]\n",
      "loss: 0.020036, accuracy: 92.966%, batch [188160/756895]\n",
      "loss: 0.019353, accuracy: 93.114%, batch [189440/756895]\n",
      "loss: 0.018082, accuracy: 93.122%, batch [190720/756895]\n",
      "loss: 0.021404, accuracy: 92.834%, batch [192000/756895]\n",
      "loss: 0.019992, accuracy: 93.122%, batch [193280/756895]\n",
      "loss: 0.023909, accuracy: 92.789%, batch [194560/756895]\n",
      "loss: 0.017038, accuracy: 93.102%, batch [195840/756895]\n",
      "loss: 0.020934, accuracy: 92.956%, batch [197120/756895]\n",
      "loss: 0.018616, accuracy: 92.963%, batch [198400/756895]\n",
      "loss: 0.022046, accuracy: 92.957%, batch [199680/756895]\n",
      "loss: 0.020864, accuracy: 93.008%, batch [200960/756895]\n",
      "loss: 0.024442, accuracy: 92.821%, batch [202240/756895]\n",
      "loss: 0.019925, accuracy: 92.991%, batch [203520/756895]\n",
      "loss: 0.022109, accuracy: 93.071%, batch [204800/756895]\n",
      "loss: 0.017104, accuracy: 93.189%, batch [206080/756895]\n",
      "loss: 0.018036, accuracy: 93.088%, batch [207360/756895]\n",
      "loss: 0.019898, accuracy: 92.974%, batch [208640/756895]\n",
      "loss: 0.018800, accuracy: 93.119%, batch [209920/756895]\n",
      "loss: 0.019738, accuracy: 92.915%, batch [211200/756895]\n",
      "loss: 0.016215, accuracy: 93.207%, batch [212480/756895]\n",
      "loss: 0.017362, accuracy: 93.015%, batch [213760/756895]\n",
      "loss: 0.017382, accuracy: 93.037%, batch [215040/756895]\n",
      "loss: 0.020853, accuracy: 93.110%, batch [216320/756895]\n",
      "loss: 0.017195, accuracy: 93.030%, batch [217600/756895]\n",
      "loss: 0.016037, accuracy: 93.073%, batch [218880/756895]\n",
      "loss: 0.018759, accuracy: 92.999%, batch [220160/756895]\n",
      "loss: 0.021092, accuracy: 93.084%, batch [221440/756895]\n",
      "loss: 0.021403, accuracy: 92.909%, batch [222720/756895]\n",
      "loss: 0.015173, accuracy: 93.095%, batch [224000/756895]\n",
      "loss: 0.016074, accuracy: 93.184%, batch [225280/756895]\n",
      "loss: 0.016616, accuracy: 93.215%, batch [226560/756895]\n",
      "loss: 0.020209, accuracy: 92.969%, batch [227840/756895]\n",
      "loss: 0.017489, accuracy: 93.135%, batch [229120/756895]\n",
      "loss: 0.019450, accuracy: 93.027%, batch [230400/756895]\n",
      "loss: 0.022634, accuracy: 93.058%, batch [231680/756895]\n",
      "loss: 0.019663, accuracy: 93.008%, batch [232960/756895]\n",
      "loss: 0.021555, accuracy: 92.801%, batch [234240/756895]\n",
      "loss: 0.017429, accuracy: 93.139%, batch [235520/756895]\n",
      "loss: 0.019717, accuracy: 92.952%, batch [236800/756895]\n",
      "loss: 0.020231, accuracy: 93.078%, batch [238080/756895]\n",
      "loss: 0.022558, accuracy: 92.860%, batch [239360/756895]\n",
      "loss: 0.018183, accuracy: 92.922%, batch [240640/756895]\n",
      "loss: 0.021063, accuracy: 93.150%, batch [241920/756895]\n",
      "loss: 0.024334, accuracy: 92.992%, batch [243200/756895]\n",
      "loss: 0.020150, accuracy: 93.196%, batch [244480/756895]\n",
      "loss: 0.020427, accuracy: 93.205%, batch [245760/756895]\n",
      "loss: 0.017628, accuracy: 93.149%, batch [247040/756895]\n",
      "loss: 0.021035, accuracy: 93.007%, batch [248320/756895]\n",
      "loss: 0.017887, accuracy: 93.201%, batch [249600/756895]\n",
      "loss: 0.015972, accuracy: 93.241%, batch [250880/756895]\n",
      "loss: 0.026442, accuracy: 92.883%, batch [252160/756895]\n",
      "loss: 0.019319, accuracy: 93.095%, batch [253440/756895]\n",
      "loss: 0.021465, accuracy: 92.891%, batch [254720/756895]\n",
      "loss: 0.018983, accuracy: 93.052%, batch [256000/756895]\n",
      "loss: 0.021771, accuracy: 92.955%, batch [257280/756895]\n",
      "loss: 0.026194, accuracy: 92.879%, batch [258560/756895]\n",
      "loss: 0.021039, accuracy: 93.079%, batch [259840/756895]\n",
      "loss: 0.014186, accuracy: 93.118%, batch [261120/756895]\n",
      "loss: 0.022153, accuracy: 92.954%, batch [262400/756895]\n",
      "loss: 0.021405, accuracy: 92.958%, batch [263680/756895]\n",
      "loss: 0.024792, accuracy: 92.874%, batch [264960/756895]\n",
      "loss: 0.020757, accuracy: 93.070%, batch [266240/756895]\n",
      "loss: 0.019942, accuracy: 92.955%, batch [267520/756895]\n",
      "loss: 0.019912, accuracy: 93.074%, batch [268800/756895]\n",
      "loss: 0.023146, accuracy: 92.969%, batch [270080/756895]\n",
      "loss: 0.020372, accuracy: 92.892%, batch [271360/756895]\n",
      "loss: 0.017374, accuracy: 93.141%, batch [272640/756895]\n",
      "loss: 0.027873, accuracy: 92.741%, batch [273920/756895]\n",
      "loss: 0.017609, accuracy: 93.147%, batch [275200/756895]\n",
      "loss: 0.015634, accuracy: 92.897%, batch [276480/756895]\n",
      "loss: 0.020245, accuracy: 93.125%, batch [277760/756895]\n",
      "loss: 0.018744, accuracy: 93.146%, batch [279040/756895]\n",
      "loss: 0.016731, accuracy: 93.168%, batch [280320/756895]\n",
      "loss: 0.018615, accuracy: 92.940%, batch [281600/756895]\n",
      "loss: 0.019463, accuracy: 93.146%, batch [282880/756895]\n",
      "loss: 0.022086, accuracy: 92.926%, batch [284160/756895]\n",
      "loss: 0.022649, accuracy: 92.974%, batch [285440/756895]\n",
      "loss: 0.019965, accuracy: 93.015%, batch [286720/756895]\n",
      "loss: 0.027677, accuracy: 92.824%, batch [288000/756895]\n",
      "loss: 0.015699, accuracy: 93.192%, batch [289280/756895]\n",
      "loss: 0.017639, accuracy: 93.089%, batch [290560/756895]\n",
      "loss: 0.016382, accuracy: 93.107%, batch [291840/756895]\n",
      "loss: 0.015957, accuracy: 93.131%, batch [293120/756895]\n",
      "loss: 0.019646, accuracy: 93.013%, batch [294400/756895]\n",
      "loss: 0.017392, accuracy: 93.054%, batch [295680/756895]\n",
      "loss: 0.021859, accuracy: 92.745%, batch [296960/756895]\n",
      "loss: 0.018758, accuracy: 93.058%, batch [298240/756895]\n",
      "loss: 0.018302, accuracy: 93.065%, batch [299520/756895]\n",
      "loss: 0.019705, accuracy: 93.001%, batch [300800/756895]\n",
      "loss: 0.016621, accuracy: 93.149%, batch [302080/756895]\n",
      "loss: 0.018923, accuracy: 92.972%, batch [303360/756895]\n",
      "loss: 0.017860, accuracy: 92.991%, batch [304640/756895]\n",
      "loss: 0.017535, accuracy: 93.205%, batch [305920/756895]\n",
      "loss: 0.016944, accuracy: 93.010%, batch [307200/756895]\n",
      "loss: 0.016594, accuracy: 93.083%, batch [308480/756895]\n",
      "loss: 0.014822, accuracy: 93.033%, batch [309760/756895]\n",
      "loss: 0.018538, accuracy: 93.067%, batch [311040/756895]\n",
      "loss: 0.019627, accuracy: 92.889%, batch [312320/756895]\n",
      "loss: 0.016687, accuracy: 93.233%, batch [313600/756895]\n",
      "loss: 0.016777, accuracy: 93.066%, batch [314880/756895]\n",
      "loss: 0.014077, accuracy: 93.372%, batch [316160/756895]\n",
      "loss: 0.015172, accuracy: 93.312%, batch [317440/756895]\n",
      "loss: 0.020236, accuracy: 92.777%, batch [318720/756895]\n",
      "loss: 0.018345, accuracy: 93.257%, batch [320000/756895]\n",
      "loss: 0.022506, accuracy: 92.910%, batch [321280/756895]\n",
      "loss: 0.016582, accuracy: 93.209%, batch [322560/756895]\n",
      "loss: 0.021225, accuracy: 92.932%, batch [323840/756895]\n",
      "loss: 0.019071, accuracy: 93.138%, batch [325120/756895]\n",
      "loss: 0.015768, accuracy: 93.307%, batch [326400/756895]\n",
      "loss: 0.018840, accuracy: 92.967%, batch [327680/756895]\n",
      "loss: 0.019348, accuracy: 92.857%, batch [328960/756895]\n",
      "loss: 0.017922, accuracy: 93.136%, batch [330240/756895]\n",
      "loss: 0.019615, accuracy: 93.133%, batch [331520/756895]\n",
      "loss: 0.019714, accuracy: 92.964%, batch [332800/756895]\n",
      "loss: 0.019935, accuracy: 92.939%, batch [334080/756895]\n",
      "loss: 0.015537, accuracy: 93.075%, batch [335360/756895]\n",
      "loss: 0.016228, accuracy: 93.212%, batch [336640/756895]\n",
      "loss: 0.020206, accuracy: 92.989%, batch [337920/756895]\n",
      "loss: 0.019416, accuracy: 92.826%, batch [339200/756895]\n",
      "loss: 0.014201, accuracy: 93.255%, batch [340480/756895]\n",
      "loss: 0.016451, accuracy: 93.120%, batch [341760/756895]\n",
      "loss: 0.022352, accuracy: 93.280%, batch [343040/756895]\n",
      "loss: 0.016753, accuracy: 93.159%, batch [344320/756895]\n",
      "loss: 0.019132, accuracy: 93.007%, batch [345600/756895]\n",
      "loss: 0.018529, accuracy: 93.035%, batch [346880/756895]\n",
      "loss: 0.018169, accuracy: 92.980%, batch [348160/756895]\n",
      "loss: 0.020942, accuracy: 93.114%, batch [349440/756895]\n",
      "loss: 0.019968, accuracy: 92.920%, batch [350720/756895]\n",
      "loss: 0.018551, accuracy: 93.086%, batch [352000/756895]\n",
      "loss: 0.023460, accuracy: 92.699%, batch [353280/756895]\n",
      "loss: 0.021209, accuracy: 92.911%, batch [354560/756895]\n",
      "loss: 0.020086, accuracy: 92.847%, batch [355840/756895]\n",
      "loss: 0.023902, accuracy: 92.923%, batch [357120/756895]\n",
      "loss: 0.017683, accuracy: 93.106%, batch [358400/756895]\n",
      "loss: 0.018931, accuracy: 93.053%, batch [359680/756895]\n",
      "loss: 0.016940, accuracy: 93.249%, batch [360960/756895]\n",
      "loss: 0.017048, accuracy: 93.154%, batch [362240/756895]\n",
      "loss: 0.019062, accuracy: 92.961%, batch [363520/756895]\n",
      "loss: 0.019536, accuracy: 93.033%, batch [364800/756895]\n",
      "loss: 0.020741, accuracy: 92.835%, batch [366080/756895]\n",
      "loss: 0.020990, accuracy: 93.083%, batch [367360/756895]\n",
      "loss: 0.021649, accuracy: 92.918%, batch [368640/756895]\n",
      "loss: 0.021483, accuracy: 92.978%, batch [369920/756895]\n",
      "loss: 0.016778, accuracy: 93.099%, batch [371200/756895]\n",
      "loss: 0.019553, accuracy: 93.211%, batch [372480/756895]\n",
      "loss: 0.022877, accuracy: 92.896%, batch [373760/756895]\n",
      "loss: 0.020883, accuracy: 93.148%, batch [375040/756895]\n",
      "loss: 0.017476, accuracy: 93.172%, batch [376320/756895]\n",
      "loss: 0.021246, accuracy: 93.116%, batch [377600/756895]\n",
      "loss: 0.015575, accuracy: 93.256%, batch [378880/756895]\n",
      "loss: 0.023999, accuracy: 92.738%, batch [380160/756895]\n",
      "loss: 0.016369, accuracy: 93.170%, batch [381440/756895]\n",
      "loss: 0.016093, accuracy: 93.289%, batch [382720/756895]\n",
      "loss: 0.024955, accuracy: 92.931%, batch [384000/756895]\n",
      "loss: 0.020461, accuracy: 93.144%, batch [385280/756895]\n",
      "loss: 0.014304, accuracy: 93.258%, batch [386560/756895]\n",
      "loss: 0.020337, accuracy: 93.097%, batch [387840/756895]\n",
      "loss: 0.016068, accuracy: 93.277%, batch [389120/756895]\n",
      "loss: 0.021218, accuracy: 93.005%, batch [390400/756895]\n",
      "loss: 0.018124, accuracy: 93.188%, batch [391680/756895]\n",
      "loss: 0.016806, accuracy: 93.159%, batch [392960/756895]\n",
      "loss: 0.023681, accuracy: 92.974%, batch [394240/756895]\n",
      "loss: 0.018398, accuracy: 93.155%, batch [395520/756895]\n",
      "loss: 0.018032, accuracy: 93.159%, batch [396800/756895]\n",
      "loss: 0.015071, accuracy: 93.054%, batch [398080/756895]\n",
      "loss: 0.017535, accuracy: 93.226%, batch [399360/756895]\n",
      "loss: 0.023223, accuracy: 93.027%, batch [400640/756895]\n",
      "loss: 0.020760, accuracy: 93.059%, batch [401920/756895]\n",
      "loss: 0.015049, accuracy: 93.216%, batch [403200/756895]\n",
      "loss: 0.019051, accuracy: 93.055%, batch [404480/756895]\n",
      "loss: 0.016896, accuracy: 93.062%, batch [405760/756895]\n",
      "loss: 0.021015, accuracy: 92.852%, batch [407040/756895]\n",
      "loss: 0.017168, accuracy: 93.097%, batch [408320/756895]\n",
      "loss: 0.022004, accuracy: 92.824%, batch [409600/756895]\n",
      "loss: 0.016852, accuracy: 92.943%, batch [410880/756895]\n",
      "loss: 0.020809, accuracy: 93.156%, batch [412160/756895]\n",
      "loss: 0.015222, accuracy: 93.188%, batch [413440/756895]\n",
      "loss: 0.015500, accuracy: 93.027%, batch [414720/756895]\n",
      "loss: 0.017334, accuracy: 93.029%, batch [416000/756895]\n",
      "loss: 0.019958, accuracy: 93.092%, batch [417280/756895]\n",
      "loss: 0.019302, accuracy: 92.773%, batch [418560/756895]\n",
      "loss: 0.017512, accuracy: 93.034%, batch [419840/756895]\n",
      "loss: 0.017837, accuracy: 93.058%, batch [421120/756895]\n",
      "loss: 0.024060, accuracy: 92.951%, batch [422400/756895]\n",
      "loss: 0.017538, accuracy: 93.094%, batch [423680/756895]\n",
      "loss: 0.026618, accuracy: 92.864%, batch [424960/756895]\n",
      "loss: 0.019811, accuracy: 92.993%, batch [426240/756895]\n",
      "loss: 0.019280, accuracy: 92.946%, batch [427520/756895]\n",
      "loss: 0.020207, accuracy: 92.891%, batch [428800/756895]\n",
      "loss: 0.018459, accuracy: 92.882%, batch [430080/756895]\n",
      "loss: 0.019527, accuracy: 93.000%, batch [431360/756895]\n",
      "loss: 0.025189, accuracy: 92.852%, batch [432640/756895]\n",
      "loss: 0.019724, accuracy: 92.891%, batch [433920/756895]\n",
      "loss: 0.019021, accuracy: 92.931%, batch [435200/756895]\n",
      "loss: 0.015867, accuracy: 93.400%, batch [436480/756895]\n",
      "loss: 0.017488, accuracy: 93.254%, batch [437760/756895]\n",
      "loss: 0.014886, accuracy: 93.151%, batch [439040/756895]\n",
      "loss: 0.019295, accuracy: 93.043%, batch [440320/756895]\n",
      "loss: 0.021471, accuracy: 93.068%, batch [441600/756895]\n",
      "loss: 0.017953, accuracy: 93.103%, batch [442880/756895]\n",
      "loss: 0.017286, accuracy: 93.182%, batch [444160/756895]\n",
      "loss: 0.015605, accuracy: 93.241%, batch [445440/756895]\n",
      "loss: 0.014941, accuracy: 93.226%, batch [446720/756895]\n",
      "loss: 0.014960, accuracy: 93.197%, batch [448000/756895]\n",
      "loss: 0.018754, accuracy: 93.131%, batch [449280/756895]\n",
      "loss: 0.018766, accuracy: 93.096%, batch [450560/756895]\n",
      "loss: 0.020723, accuracy: 92.943%, batch [451840/756895]\n",
      "loss: 0.018076, accuracy: 92.981%, batch [453120/756895]\n",
      "loss: 0.019551, accuracy: 93.133%, batch [454400/756895]\n",
      "loss: 0.025164, accuracy: 92.946%, batch [455680/756895]\n",
      "loss: 0.020207, accuracy: 93.027%, batch [456960/756895]\n",
      "loss: 0.020031, accuracy: 93.065%, batch [458240/756895]\n",
      "loss: 0.016710, accuracy: 93.101%, batch [459520/756895]\n",
      "loss: 0.018723, accuracy: 93.106%, batch [460800/756895]\n",
      "loss: 0.014645, accuracy: 93.343%, batch [462080/756895]\n",
      "loss: 0.017241, accuracy: 92.897%, batch [463360/756895]\n",
      "loss: 0.016494, accuracy: 93.016%, batch [464640/756895]\n",
      "loss: 0.017533, accuracy: 92.938%, batch [465920/756895]\n",
      "loss: 0.025920, accuracy: 92.981%, batch [467200/756895]\n",
      "loss: 0.017405, accuracy: 93.114%, batch [468480/756895]\n",
      "loss: 0.019614, accuracy: 93.162%, batch [469760/756895]\n",
      "loss: 0.025927, accuracy: 92.778%, batch [471040/756895]\n",
      "loss: 0.018288, accuracy: 92.970%, batch [472320/756895]\n",
      "loss: 0.020204, accuracy: 93.038%, batch [473600/756895]\n",
      "loss: 0.017709, accuracy: 93.195%, batch [474880/756895]\n",
      "loss: 0.016511, accuracy: 93.183%, batch [476160/756895]\n",
      "loss: 0.019054, accuracy: 93.148%, batch [477440/756895]\n",
      "loss: 0.019421, accuracy: 93.109%, batch [478720/756895]\n",
      "loss: 0.020322, accuracy: 92.905%, batch [480000/756895]\n",
      "loss: 0.018864, accuracy: 93.129%, batch [481280/756895]\n",
      "loss: 0.017836, accuracy: 93.207%, batch [482560/756895]\n",
      "loss: 0.016672, accuracy: 93.074%, batch [483840/756895]\n",
      "loss: 0.015880, accuracy: 93.212%, batch [485120/756895]\n",
      "loss: 0.024563, accuracy: 92.799%, batch [486400/756895]\n",
      "loss: 0.020752, accuracy: 93.021%, batch [487680/756895]\n",
      "loss: 0.017534, accuracy: 93.130%, batch [488960/756895]\n",
      "loss: 0.015382, accuracy: 93.041%, batch [490240/756895]\n",
      "loss: 0.014451, accuracy: 93.229%, batch [491520/756895]\n",
      "loss: 0.019174, accuracy: 92.936%, batch [492800/756895]\n",
      "loss: 0.015930, accuracy: 93.232%, batch [494080/756895]\n",
      "loss: 0.020407, accuracy: 92.954%, batch [495360/756895]\n",
      "loss: 0.019376, accuracy: 93.120%, batch [496640/756895]\n",
      "loss: 0.021805, accuracy: 92.962%, batch [497920/756895]\n",
      "loss: 0.019234, accuracy: 92.924%, batch [499200/756895]\n",
      "loss: 0.017741, accuracy: 93.046%, batch [500480/756895]\n",
      "loss: 0.018513, accuracy: 93.188%, batch [501760/756895]\n",
      "loss: 0.025609, accuracy: 93.044%, batch [503040/756895]\n",
      "loss: 0.021470, accuracy: 93.057%, batch [504320/756895]\n",
      "loss: 0.020834, accuracy: 92.886%, batch [505600/756895]\n",
      "loss: 0.021128, accuracy: 92.950%, batch [506880/756895]\n",
      "loss: 0.026519, accuracy: 93.071%, batch [508160/756895]\n",
      "loss: 0.020394, accuracy: 93.032%, batch [509440/756895]\n",
      "loss: 0.018560, accuracy: 93.100%, batch [510720/756895]\n",
      "loss: 0.019039, accuracy: 93.090%, batch [512000/756895]\n",
      "loss: 0.019794, accuracy: 92.921%, batch [513280/756895]\n",
      "loss: 0.016398, accuracy: 93.277%, batch [514560/756895]\n",
      "loss: 0.021237, accuracy: 93.065%, batch [515840/756895]\n",
      "loss: 0.015276, accuracy: 93.152%, batch [517120/756895]\n",
      "loss: 0.021390, accuracy: 92.969%, batch [518400/756895]\n",
      "loss: 0.018848, accuracy: 93.150%, batch [519680/756895]\n",
      "loss: 0.023757, accuracy: 92.903%, batch [520960/756895]\n",
      "loss: 0.021302, accuracy: 93.145%, batch [522240/756895]\n",
      "loss: 0.019328, accuracy: 92.878%, batch [523520/756895]\n",
      "loss: 0.017290, accuracy: 93.104%, batch [524800/756895]\n",
      "loss: 0.015903, accuracy: 93.203%, batch [526080/756895]\n",
      "loss: 0.017877, accuracy: 93.133%, batch [527360/756895]\n",
      "loss: 0.023413, accuracy: 93.009%, batch [528640/756895]\n",
      "loss: 0.019064, accuracy: 93.063%, batch [529920/756895]\n",
      "loss: 0.023810, accuracy: 92.909%, batch [531200/756895]\n",
      "loss: 0.016665, accuracy: 93.204%, batch [532480/756895]\n",
      "loss: 0.021284, accuracy: 93.014%, batch [533760/756895]\n",
      "loss: 0.018121, accuracy: 92.957%, batch [535040/756895]\n",
      "loss: 0.017706, accuracy: 93.186%, batch [536320/756895]\n",
      "loss: 0.018751, accuracy: 93.127%, batch [537600/756895]\n",
      "loss: 0.015997, accuracy: 93.201%, batch [538880/756895]\n",
      "loss: 0.020228, accuracy: 93.021%, batch [540160/756895]\n",
      "loss: 0.017688, accuracy: 93.136%, batch [541440/756895]\n",
      "loss: 0.018048, accuracy: 93.214%, batch [542720/756895]\n",
      "loss: 0.019162, accuracy: 93.159%, batch [544000/756895]\n",
      "loss: 0.023369, accuracy: 92.999%, batch [545280/756895]\n",
      "loss: 0.016921, accuracy: 93.101%, batch [546560/756895]\n",
      "loss: 0.019838, accuracy: 93.030%, batch [547840/756895]\n",
      "loss: 0.018964, accuracy: 92.947%, batch [549120/756895]\n",
      "loss: 0.020914, accuracy: 93.055%, batch [550400/756895]\n",
      "loss: 0.023843, accuracy: 92.867%, batch [551680/756895]\n",
      "loss: 0.018288, accuracy: 93.016%, batch [552960/756895]\n",
      "loss: 0.022926, accuracy: 92.983%, batch [554240/756895]\n",
      "loss: 0.023440, accuracy: 92.993%, batch [555520/756895]\n",
      "loss: 0.017257, accuracy: 93.109%, batch [556800/756895]\n",
      "loss: 0.020436, accuracy: 93.004%, batch [558080/756895]\n",
      "loss: 0.020643, accuracy: 92.927%, batch [559360/756895]\n",
      "loss: 0.014824, accuracy: 93.331%, batch [560640/756895]\n",
      "loss: 0.020033, accuracy: 93.022%, batch [561920/756895]\n",
      "loss: 0.017474, accuracy: 93.125%, batch [563200/756895]\n",
      "loss: 0.018047, accuracy: 93.091%, batch [564480/756895]\n",
      "loss: 0.018140, accuracy: 93.118%, batch [565760/756895]\n",
      "loss: 0.018437, accuracy: 93.084%, batch [567040/756895]\n",
      "loss: 0.022987, accuracy: 92.918%, batch [568320/756895]\n",
      "loss: 0.016896, accuracy: 93.217%, batch [569600/756895]\n",
      "loss: 0.019329, accuracy: 93.054%, batch [570880/756895]\n",
      "loss: 0.020425, accuracy: 93.226%, batch [572160/756895]\n",
      "loss: 0.018853, accuracy: 92.971%, batch [573440/756895]\n",
      "loss: 0.020748, accuracy: 93.085%, batch [574720/756895]\n",
      "loss: 0.016630, accuracy: 93.227%, batch [576000/756895]\n",
      "loss: 0.018126, accuracy: 93.105%, batch [577280/756895]\n",
      "loss: 0.019191, accuracy: 93.248%, batch [578560/756895]\n",
      "loss: 0.016839, accuracy: 93.215%, batch [579840/756895]\n",
      "loss: 0.014509, accuracy: 93.308%, batch [581120/756895]\n",
      "loss: 0.016953, accuracy: 93.069%, batch [582400/756895]\n",
      "loss: 0.020733, accuracy: 93.019%, batch [583680/756895]\n",
      "loss: 0.017253, accuracy: 92.949%, batch [584960/756895]\n",
      "loss: 0.017106, accuracy: 93.262%, batch [586240/756895]\n",
      "loss: 0.018140, accuracy: 92.933%, batch [587520/756895]\n",
      "loss: 0.016251, accuracy: 93.132%, batch [588800/756895]\n",
      "loss: 0.016779, accuracy: 93.172%, batch [590080/756895]\n",
      "loss: 0.017582, accuracy: 92.921%, batch [591360/756895]\n",
      "loss: 0.022927, accuracy: 92.765%, batch [592640/756895]\n",
      "loss: 0.016327, accuracy: 93.160%, batch [593920/756895]\n",
      "loss: 0.019485, accuracy: 92.940%, batch [595200/756895]\n",
      "loss: 0.021959, accuracy: 93.104%, batch [596480/756895]\n",
      "loss: 0.017318, accuracy: 92.977%, batch [597760/756895]\n",
      "loss: 0.014902, accuracy: 93.225%, batch [599040/756895]\n",
      "loss: 0.027172, accuracy: 93.029%, batch [600320/756895]\n",
      "loss: 0.017125, accuracy: 92.994%, batch [601600/756895]\n",
      "loss: 0.019302, accuracy: 93.240%, batch [602880/756895]\n",
      "loss: 0.018417, accuracy: 93.213%, batch [604160/756895]\n",
      "loss: 0.017047, accuracy: 93.122%, batch [605440/756895]\n",
      "loss: 0.021416, accuracy: 92.915%, batch [606720/756895]\n",
      "loss: 0.018295, accuracy: 93.172%, batch [608000/756895]\n",
      "loss: 0.016305, accuracy: 93.261%, batch [609280/756895]\n",
      "loss: 0.018894, accuracy: 93.301%, batch [610560/756895]\n",
      "loss: 0.018172, accuracy: 92.923%, batch [611840/756895]\n",
      "loss: 0.018422, accuracy: 93.040%, batch [613120/756895]\n",
      "loss: 0.023022, accuracy: 92.846%, batch [614400/756895]\n",
      "loss: 0.016542, accuracy: 93.187%, batch [615680/756895]\n",
      "loss: 0.023948, accuracy: 92.797%, batch [616960/756895]\n",
      "loss: 0.018416, accuracy: 92.953%, batch [618240/756895]\n",
      "loss: 0.018513, accuracy: 92.986%, batch [619520/756895]\n",
      "loss: 0.015609, accuracy: 93.110%, batch [620800/756895]\n",
      "loss: 0.022129, accuracy: 92.987%, batch [622080/756895]\n",
      "loss: 0.016213, accuracy: 93.244%, batch [623360/756895]\n",
      "loss: 0.017058, accuracy: 93.162%, batch [624640/756895]\n",
      "loss: 0.020422, accuracy: 92.979%, batch [625920/756895]\n",
      "loss: 0.017924, accuracy: 92.982%, batch [627200/756895]\n",
      "loss: 0.021437, accuracy: 92.951%, batch [628480/756895]\n",
      "loss: 0.017049, accuracy: 93.042%, batch [629760/756895]\n",
      "loss: 0.017694, accuracy: 93.044%, batch [631040/756895]\n",
      "loss: 0.015164, accuracy: 93.202%, batch [632320/756895]\n",
      "loss: 0.017638, accuracy: 93.041%, batch [633600/756895]\n",
      "loss: 0.017089, accuracy: 92.936%, batch [634880/756895]\n",
      "loss: 0.018296, accuracy: 93.121%, batch [636160/756895]\n",
      "loss: 0.021956, accuracy: 93.031%, batch [637440/756895]\n",
      "loss: 0.016246, accuracy: 93.124%, batch [638720/756895]\n",
      "loss: 0.016750, accuracy: 93.091%, batch [640000/756895]\n",
      "loss: 0.017447, accuracy: 93.120%, batch [641280/756895]\n",
      "loss: 0.018448, accuracy: 93.069%, batch [642560/756895]\n",
      "loss: 0.019015, accuracy: 92.933%, batch [643840/756895]\n",
      "loss: 0.016942, accuracy: 93.062%, batch [645120/756895]\n",
      "loss: 0.022132, accuracy: 93.095%, batch [646400/756895]\n",
      "loss: 0.020620, accuracy: 92.902%, batch [647680/756895]\n",
      "loss: 0.014488, accuracy: 93.276%, batch [648960/756895]\n",
      "loss: 0.019669, accuracy: 93.182%, batch [650240/756895]\n",
      "loss: 0.016297, accuracy: 93.294%, batch [651520/756895]\n",
      "loss: 0.025764, accuracy: 92.915%, batch [652800/756895]\n",
      "loss: 0.017310, accuracy: 93.130%, batch [654080/756895]\n",
      "loss: 0.017908, accuracy: 93.185%, batch [655360/756895]\n",
      "loss: 0.018444, accuracy: 93.045%, batch [656640/756895]\n",
      "loss: 0.021650, accuracy: 93.064%, batch [657920/756895]\n",
      "loss: 0.017854, accuracy: 93.191%, batch [659200/756895]\n",
      "loss: 0.016103, accuracy: 93.129%, batch [660480/756895]\n",
      "loss: 0.020384, accuracy: 93.022%, batch [661760/756895]\n",
      "loss: 0.019826, accuracy: 92.957%, batch [663040/756895]\n",
      "loss: 0.014392, accuracy: 93.479%, batch [664320/756895]\n",
      "loss: 0.016131, accuracy: 93.202%, batch [665600/756895]\n",
      "loss: 0.018234, accuracy: 93.091%, batch [666880/756895]\n",
      "loss: 0.019282, accuracy: 93.157%, batch [668160/756895]\n",
      "loss: 0.015449, accuracy: 93.278%, batch [669440/756895]\n",
      "loss: 0.017309, accuracy: 93.179%, batch [670720/756895]\n",
      "loss: 0.016257, accuracy: 93.103%, batch [672000/756895]\n",
      "loss: 0.021971, accuracy: 92.884%, batch [673280/756895]\n",
      "loss: 0.016908, accuracy: 93.120%, batch [674560/756895]\n",
      "loss: 0.029004, accuracy: 92.842%, batch [675840/756895]\n",
      "loss: 0.017101, accuracy: 93.185%, batch [677120/756895]\n",
      "loss: 0.016662, accuracy: 93.146%, batch [678400/756895]\n",
      "loss: 0.019235, accuracy: 93.112%, batch [679680/756895]\n",
      "loss: 0.020314, accuracy: 93.147%, batch [680960/756895]\n",
      "loss: 0.016891, accuracy: 93.124%, batch [682240/756895]\n",
      "loss: 0.018228, accuracy: 93.027%, batch [683520/756895]\n",
      "loss: 0.017775, accuracy: 93.088%, batch [684800/756895]\n",
      "loss: 0.020852, accuracy: 92.892%, batch [686080/756895]\n",
      "loss: 0.015373, accuracy: 93.254%, batch [687360/756895]\n",
      "loss: 0.016923, accuracy: 93.088%, batch [688640/756895]\n",
      "loss: 0.016032, accuracy: 93.199%, batch [689920/756895]\n",
      "loss: 0.017486, accuracy: 92.984%, batch [691200/756895]\n",
      "loss: 0.017013, accuracy: 93.005%, batch [692480/756895]\n",
      "loss: 0.018347, accuracy: 93.109%, batch [693760/756895]\n",
      "loss: 0.018563, accuracy: 93.083%, batch [695040/756895]\n",
      "loss: 0.017801, accuracy: 93.054%, batch [696320/756895]\n",
      "loss: 0.018685, accuracy: 93.064%, batch [697600/756895]\n",
      "loss: 0.017728, accuracy: 93.126%, batch [698880/756895]\n",
      "loss: 0.016650, accuracy: 93.177%, batch [700160/756895]\n",
      "loss: 0.024254, accuracy: 92.899%, batch [701440/756895]\n",
      "loss: 0.017780, accuracy: 93.161%, batch [702720/756895]\n",
      "loss: 0.016856, accuracy: 93.109%, batch [704000/756895]\n",
      "loss: 0.015244, accuracy: 93.309%, batch [705280/756895]\n",
      "loss: 0.018559, accuracy: 92.957%, batch [706560/756895]\n",
      "loss: 0.017614, accuracy: 93.250%, batch [707840/756895]\n",
      "loss: 0.020889, accuracy: 93.150%, batch [709120/756895]\n",
      "loss: 0.028877, accuracy: 92.740%, batch [710400/756895]\n",
      "loss: 0.014816, accuracy: 93.332%, batch [711680/756895]\n",
      "loss: 0.018584, accuracy: 93.008%, batch [712960/756895]\n",
      "loss: 0.016006, accuracy: 93.216%, batch [714240/756895]\n",
      "loss: 0.020690, accuracy: 93.058%, batch [715520/756895]\n",
      "loss: 0.018905, accuracy: 93.134%, batch [716800/756895]\n",
      "loss: 0.015445, accuracy: 93.303%, batch [718080/756895]\n",
      "loss: 0.018909, accuracy: 93.125%, batch [719360/756895]\n",
      "loss: 0.015312, accuracy: 93.241%, batch [720640/756895]\n",
      "loss: 0.020256, accuracy: 92.954%, batch [721920/756895]\n",
      "loss: 0.014746, accuracy: 93.064%, batch [723200/756895]\n",
      "loss: 0.023350, accuracy: 92.719%, batch [724480/756895]\n",
      "loss: 0.018046, accuracy: 93.071%, batch [725760/756895]\n",
      "loss: 0.021327, accuracy: 92.886%, batch [727040/756895]\n",
      "loss: 0.017737, accuracy: 93.015%, batch [728320/756895]\n",
      "loss: 0.017367, accuracy: 93.113%, batch [729600/756895]\n",
      "loss: 0.021169, accuracy: 92.927%, batch [730880/756895]\n",
      "loss: 0.019373, accuracy: 92.943%, batch [732160/756895]\n",
      "loss: 0.021150, accuracy: 92.822%, batch [733440/756895]\n",
      "loss: 0.019929, accuracy: 93.198%, batch [734720/756895]\n",
      "loss: 0.015568, accuracy: 93.293%, batch [736000/756895]\n",
      "loss: 0.020668, accuracy: 93.085%, batch [737280/756895]\n",
      "loss: 0.020454, accuracy: 93.181%, batch [738560/756895]\n",
      "loss: 0.021607, accuracy: 92.920%, batch [739840/756895]\n",
      "loss: 0.021728, accuracy: 92.956%, batch [741120/756895]\n",
      "loss: 0.015655, accuracy: 93.259%, batch [742400/756895]\n",
      "loss: 0.018920, accuracy: 93.079%, batch [743680/756895]\n",
      "loss: 0.018991, accuracy: 93.079%, batch [744960/756895]\n",
      "loss: 0.015630, accuracy: 93.265%, batch [746240/756895]\n",
      "loss: 0.023543, accuracy: 92.945%, batch [747520/756895]\n",
      "loss: 0.019121, accuracy: 92.966%, batch [748800/756895]\n",
      "loss: 0.016729, accuracy: 93.198%, batch [750080/756895]\n",
      "loss: 0.017237, accuracy: 93.094%, batch [751360/756895]\n",
      "loss: 0.015545, accuracy: 93.138%, batch [752640/756895]\n",
      "loss: 0.015882, accuracy: 93.075%, batch [753920/756895]\n",
      "loss: 0.021353, accuracy: 92.849%, batch [755200/756895]\n",
      "loss: 0.019168, accuracy: 92.993%, batch [756480/756895]\n",
      "Test avg loss: 0.019749, test avg accuracy: 93.017% \n",
      "\n",
      "Test avg loss: 0.019497, test avg accuracy: 93.038% \n",
      "\n",
      "Epoch 115\n",
      "------------------------\n",
      "loss: 0.016237, accuracy: 93.168%, batch [    0/756895]\n",
      "loss: 0.019972, accuracy: 93.102%, batch [ 1280/756895]\n",
      "loss: 0.018132, accuracy: 93.064%, batch [ 2560/756895]\n",
      "loss: 0.018263, accuracy: 93.178%, batch [ 3840/756895]\n",
      "loss: 0.017563, accuracy: 93.136%, batch [ 5120/756895]\n",
      "loss: 0.015823, accuracy: 93.126%, batch [ 6400/756895]\n",
      "loss: 0.015328, accuracy: 93.095%, batch [ 7680/756895]\n",
      "loss: 0.014955, accuracy: 93.126%, batch [ 8960/756895]\n",
      "loss: 0.018448, accuracy: 92.962%, batch [10240/756895]\n",
      "loss: 0.021186, accuracy: 93.069%, batch [11520/756895]\n",
      "loss: 0.022465, accuracy: 92.865%, batch [12800/756895]\n",
      "loss: 0.023348, accuracy: 92.981%, batch [14080/756895]\n",
      "loss: 0.017812, accuracy: 93.102%, batch [15360/756895]\n",
      "loss: 0.018982, accuracy: 93.168%, batch [16640/756895]\n",
      "loss: 0.015380, accuracy: 93.009%, batch [17920/756895]\n",
      "loss: 0.025466, accuracy: 92.918%, batch [19200/756895]\n",
      "loss: 0.016879, accuracy: 93.003%, batch [20480/756895]\n",
      "loss: 0.017551, accuracy: 93.152%, batch [21760/756895]\n",
      "loss: 0.020058, accuracy: 92.872%, batch [23040/756895]\n",
      "loss: 0.023100, accuracy: 92.911%, batch [24320/756895]\n",
      "loss: 0.022218, accuracy: 92.900%, batch [25600/756895]\n",
      "loss: 0.017645, accuracy: 92.991%, batch [26880/756895]\n",
      "loss: 0.016856, accuracy: 93.124%, batch [28160/756895]\n",
      "loss: 0.019633, accuracy: 92.905%, batch [29440/756895]\n",
      "loss: 0.019165, accuracy: 93.152%, batch [30720/756895]\n",
      "loss: 0.015988, accuracy: 93.175%, batch [32000/756895]\n",
      "loss: 0.015919, accuracy: 93.171%, batch [33280/756895]\n",
      "loss: 0.026803, accuracy: 92.779%, batch [34560/756895]\n",
      "loss: 0.016740, accuracy: 93.084%, batch [35840/756895]\n",
      "loss: 0.019232, accuracy: 93.108%, batch [37120/756895]\n",
      "loss: 0.027758, accuracy: 92.779%, batch [38400/756895]\n",
      "loss: 0.013609, accuracy: 93.373%, batch [39680/756895]\n",
      "loss: 0.017367, accuracy: 93.213%, batch [40960/756895]\n",
      "loss: 0.024051, accuracy: 92.794%, batch [42240/756895]\n",
      "loss: 0.017280, accuracy: 93.066%, batch [43520/756895]\n",
      "loss: 0.025404, accuracy: 92.886%, batch [44800/756895]\n",
      "loss: 0.025622, accuracy: 92.903%, batch [46080/756895]\n",
      "loss: 0.026344, accuracy: 92.760%, batch [47360/756895]\n",
      "loss: 0.017610, accuracy: 93.107%, batch [48640/756895]\n",
      "loss: 0.016482, accuracy: 93.173%, batch [49920/756895]\n",
      "loss: 0.015297, accuracy: 93.132%, batch [51200/756895]\n",
      "loss: 0.014961, accuracy: 93.233%, batch [52480/756895]\n",
      "loss: 0.017391, accuracy: 93.159%, batch [53760/756895]\n",
      "loss: 0.017469, accuracy: 93.203%, batch [55040/756895]\n",
      "loss: 0.015263, accuracy: 93.177%, batch [56320/756895]\n",
      "loss: 0.022097, accuracy: 92.900%, batch [57600/756895]\n",
      "loss: 0.018337, accuracy: 93.040%, batch [58880/756895]\n",
      "loss: 0.015860, accuracy: 93.016%, batch [60160/756895]\n",
      "loss: 0.021562, accuracy: 92.980%, batch [61440/756895]\n",
      "loss: 0.021203, accuracy: 92.940%, batch [62720/756895]\n",
      "loss: 0.015528, accuracy: 93.339%, batch [64000/756895]\n",
      "loss: 0.015056, accuracy: 93.113%, batch [65280/756895]\n",
      "loss: 0.019336, accuracy: 93.010%, batch [66560/756895]\n",
      "loss: 0.022433, accuracy: 92.969%, batch [67840/756895]\n",
      "loss: 0.018467, accuracy: 92.935%, batch [69120/756895]\n",
      "loss: 0.020954, accuracy: 92.902%, batch [70400/756895]\n",
      "loss: 0.016365, accuracy: 93.138%, batch [71680/756895]\n",
      "loss: 0.023232, accuracy: 92.674%, batch [72960/756895]\n",
      "loss: 0.019914, accuracy: 93.051%, batch [74240/756895]\n",
      "loss: 0.018958, accuracy: 93.143%, batch [75520/756895]\n",
      "loss: 0.016845, accuracy: 93.090%, batch [76800/756895]\n",
      "loss: 0.018033, accuracy: 93.139%, batch [78080/756895]\n",
      "loss: 0.022287, accuracy: 93.014%, batch [79360/756895]\n",
      "loss: 0.021147, accuracy: 92.880%, batch [80640/756895]\n",
      "loss: 0.018326, accuracy: 93.033%, batch [81920/756895]\n",
      "loss: 0.021520, accuracy: 93.003%, batch [83200/756895]\n",
      "loss: 0.017716, accuracy: 93.209%, batch [84480/756895]\n",
      "loss: 0.020266, accuracy: 93.003%, batch [85760/756895]\n",
      "loss: 0.018092, accuracy: 92.991%, batch [87040/756895]\n",
      "loss: 0.016803, accuracy: 93.129%, batch [88320/756895]\n",
      "loss: 0.019290, accuracy: 92.937%, batch [89600/756895]\n",
      "loss: 0.020708, accuracy: 93.194%, batch [90880/756895]\n",
      "loss: 0.017254, accuracy: 92.975%, batch [92160/756895]\n",
      "loss: 0.017771, accuracy: 93.325%, batch [93440/756895]\n",
      "loss: 0.015111, accuracy: 93.252%, batch [94720/756895]\n",
      "loss: 0.021226, accuracy: 92.965%, batch [96000/756895]\n",
      "loss: 0.023166, accuracy: 92.933%, batch [97280/756895]\n",
      "loss: 0.015703, accuracy: 93.202%, batch [98560/756895]\n",
      "loss: 0.016521, accuracy: 93.281%, batch [99840/756895]\n",
      "loss: 0.017589, accuracy: 93.172%, batch [101120/756895]\n",
      "loss: 0.013943, accuracy: 93.480%, batch [102400/756895]\n",
      "loss: 0.014199, accuracy: 93.230%, batch [103680/756895]\n",
      "loss: 0.015871, accuracy: 93.271%, batch [104960/756895]\n",
      "loss: 0.016795, accuracy: 93.092%, batch [106240/756895]\n",
      "loss: 0.019826, accuracy: 93.129%, batch [107520/756895]\n",
      "loss: 0.016797, accuracy: 93.161%, batch [108800/756895]\n",
      "loss: 0.017500, accuracy: 92.946%, batch [110080/756895]\n",
      "loss: 0.017247, accuracy: 93.116%, batch [111360/756895]\n",
      "loss: 0.020690, accuracy: 92.907%, batch [112640/756895]\n",
      "loss: 0.017352, accuracy: 93.144%, batch [113920/756895]\n",
      "loss: 0.017408, accuracy: 93.070%, batch [115200/756895]\n",
      "loss: 0.019931, accuracy: 93.077%, batch [116480/756895]\n",
      "loss: 0.017706, accuracy: 93.053%, batch [117760/756895]\n",
      "loss: 0.016608, accuracy: 93.123%, batch [119040/756895]\n",
      "loss: 0.019364, accuracy: 93.241%, batch [120320/756895]\n",
      "loss: 0.014222, accuracy: 93.260%, batch [121600/756895]\n",
      "loss: 0.017773, accuracy: 93.178%, batch [122880/756895]\n",
      "loss: 0.017719, accuracy: 92.964%, batch [124160/756895]\n",
      "loss: 0.018239, accuracy: 92.894%, batch [125440/756895]\n",
      "loss: 0.019443, accuracy: 93.141%, batch [126720/756895]\n",
      "loss: 0.016248, accuracy: 92.927%, batch [128000/756895]\n",
      "loss: 0.016100, accuracy: 93.125%, batch [129280/756895]\n",
      "loss: 0.020371, accuracy: 92.971%, batch [130560/756895]\n",
      "loss: 0.018698, accuracy: 93.247%, batch [131840/756895]\n",
      "loss: 0.023329, accuracy: 92.964%, batch [133120/756895]\n",
      "loss: 0.021237, accuracy: 93.230%, batch [134400/756895]\n",
      "loss: 0.016262, accuracy: 93.251%, batch [135680/756895]\n",
      "loss: 0.015315, accuracy: 93.080%, batch [136960/756895]\n",
      "loss: 0.023571, accuracy: 93.085%, batch [138240/756895]\n",
      "loss: 0.023293, accuracy: 93.095%, batch [139520/756895]\n",
      "loss: 0.016747, accuracy: 93.095%, batch [140800/756895]\n",
      "loss: 0.029554, accuracy: 92.914%, batch [142080/756895]\n",
      "loss: 0.018187, accuracy: 92.963%, batch [143360/756895]\n",
      "loss: 0.018961, accuracy: 93.160%, batch [144640/756895]\n",
      "loss: 0.017017, accuracy: 93.071%, batch [145920/756895]\n",
      "loss: 0.018873, accuracy: 93.186%, batch [147200/756895]\n",
      "loss: 0.017619, accuracy: 93.110%, batch [148480/756895]\n",
      "loss: 0.017235, accuracy: 93.149%, batch [149760/756895]\n",
      "loss: 0.017910, accuracy: 93.032%, batch [151040/756895]\n",
      "loss: 0.015831, accuracy: 93.212%, batch [152320/756895]\n",
      "loss: 0.017715, accuracy: 93.249%, batch [153600/756895]\n",
      "loss: 0.017090, accuracy: 93.120%, batch [154880/756895]\n",
      "loss: 0.018028, accuracy: 93.036%, batch [156160/756895]\n",
      "loss: 0.022245, accuracy: 92.948%, batch [157440/756895]\n",
      "loss: 0.019383, accuracy: 93.135%, batch [158720/756895]\n",
      "loss: 0.014474, accuracy: 93.280%, batch [160000/756895]\n",
      "loss: 0.026112, accuracy: 93.020%, batch [161280/756895]\n",
      "loss: 0.018082, accuracy: 93.161%, batch [162560/756895]\n",
      "loss: 0.023171, accuracy: 92.695%, batch [163840/756895]\n",
      "loss: 0.018105, accuracy: 92.934%, batch [165120/756895]\n",
      "loss: 0.015186, accuracy: 93.158%, batch [166400/756895]\n",
      "loss: 0.017577, accuracy: 92.882%, batch [167680/756895]\n",
      "loss: 0.015039, accuracy: 93.014%, batch [168960/756895]\n",
      "loss: 0.024555, accuracy: 93.064%, batch [170240/756895]\n",
      "loss: 0.018959, accuracy: 92.938%, batch [171520/756895]\n",
      "loss: 0.023844, accuracy: 93.039%, batch [172800/756895]\n",
      "loss: 0.019361, accuracy: 93.054%, batch [174080/756895]\n",
      "loss: 0.016415, accuracy: 93.355%, batch [175360/756895]\n",
      "loss: 0.017138, accuracy: 93.033%, batch [176640/756895]\n",
      "loss: 0.022974, accuracy: 92.837%, batch [177920/756895]\n",
      "loss: 0.028224, accuracy: 92.807%, batch [179200/756895]\n",
      "loss: 0.015924, accuracy: 93.105%, batch [180480/756895]\n",
      "loss: 0.023147, accuracy: 92.763%, batch [181760/756895]\n",
      "loss: 0.018181, accuracy: 93.064%, batch [183040/756895]\n",
      "loss: 0.016475, accuracy: 93.033%, batch [184320/756895]\n",
      "loss: 0.014200, accuracy: 93.256%, batch [185600/756895]\n",
      "loss: 0.018440, accuracy: 93.148%, batch [186880/756895]\n",
      "loss: 0.021253, accuracy: 92.803%, batch [188160/756895]\n",
      "loss: 0.017002, accuracy: 93.107%, batch [189440/756895]\n",
      "loss: 0.018381, accuracy: 92.952%, batch [190720/756895]\n",
      "loss: 0.015407, accuracy: 93.001%, batch [192000/756895]\n",
      "loss: 0.020057, accuracy: 93.069%, batch [193280/756895]\n",
      "loss: 0.016992, accuracy: 92.966%, batch [194560/756895]\n",
      "loss: 0.020144, accuracy: 93.167%, batch [195840/756895]\n",
      "loss: 0.017190, accuracy: 93.013%, batch [197120/756895]\n",
      "loss: 0.015528, accuracy: 93.139%, batch [198400/756895]\n",
      "loss: 0.023803, accuracy: 93.004%, batch [199680/756895]\n",
      "loss: 0.023010, accuracy: 93.030%, batch [200960/756895]\n",
      "loss: 0.017529, accuracy: 92.986%, batch [202240/756895]\n",
      "loss: 0.022727, accuracy: 92.901%, batch [203520/756895]\n",
      "loss: 0.019056, accuracy: 92.971%, batch [204800/756895]\n",
      "loss: 0.016933, accuracy: 93.160%, batch [206080/756895]\n",
      "loss: 0.018062, accuracy: 93.043%, batch [207360/756895]\n",
      "loss: 0.017609, accuracy: 93.157%, batch [208640/756895]\n",
      "loss: 0.023756, accuracy: 92.849%, batch [209920/756895]\n",
      "loss: 0.018474, accuracy: 92.989%, batch [211200/756895]\n",
      "loss: 0.018386, accuracy: 93.017%, batch [212480/756895]\n",
      "loss: 0.020717, accuracy: 92.924%, batch [213760/756895]\n",
      "loss: 0.019789, accuracy: 93.113%, batch [215040/756895]\n",
      "loss: 0.020945, accuracy: 92.964%, batch [216320/756895]\n",
      "loss: 0.018444, accuracy: 93.042%, batch [217600/756895]\n",
      "loss: 0.019165, accuracy: 93.158%, batch [218880/756895]\n",
      "loss: 0.015795, accuracy: 93.223%, batch [220160/756895]\n",
      "loss: 0.015817, accuracy: 93.144%, batch [221440/756895]\n",
      "loss: 0.016678, accuracy: 93.208%, batch [222720/756895]\n",
      "loss: 0.018225, accuracy: 93.000%, batch [224000/756895]\n",
      "loss: 0.016699, accuracy: 93.151%, batch [225280/756895]\n",
      "loss: 0.016408, accuracy: 93.201%, batch [226560/756895]\n",
      "loss: 0.017582, accuracy: 93.191%, batch [227840/756895]\n",
      "loss: 0.020670, accuracy: 92.934%, batch [229120/756895]\n",
      "loss: 0.025081, accuracy: 92.815%, batch [230400/756895]\n",
      "loss: 0.015572, accuracy: 93.193%, batch [231680/756895]\n",
      "loss: 0.023984, accuracy: 93.125%, batch [232960/756895]\n",
      "loss: 0.013736, accuracy: 93.284%, batch [234240/756895]\n",
      "loss: 0.020105, accuracy: 92.997%, batch [235520/756895]\n",
      "loss: 0.016580, accuracy: 93.173%, batch [236800/756895]\n",
      "loss: 0.026808, accuracy: 92.841%, batch [238080/756895]\n",
      "loss: 0.017813, accuracy: 93.143%, batch [239360/756895]\n",
      "loss: 0.016701, accuracy: 93.132%, batch [240640/756895]\n",
      "loss: 0.018031, accuracy: 93.178%, batch [241920/756895]\n",
      "loss: 0.018298, accuracy: 93.050%, batch [243200/756895]\n",
      "loss: 0.015744, accuracy: 93.218%, batch [244480/756895]\n",
      "loss: 0.016016, accuracy: 93.046%, batch [245760/756895]\n",
      "loss: 0.019750, accuracy: 93.127%, batch [247040/756895]\n",
      "loss: 0.022182, accuracy: 92.791%, batch [248320/756895]\n",
      "loss: 0.019687, accuracy: 93.141%, batch [249600/756895]\n",
      "loss: 0.020462, accuracy: 92.745%, batch [250880/756895]\n",
      "loss: 0.018879, accuracy: 93.053%, batch [252160/756895]\n",
      "loss: 0.016391, accuracy: 93.020%, batch [253440/756895]\n",
      "loss: 0.017574, accuracy: 93.035%, batch [254720/756895]\n",
      "loss: 0.022570, accuracy: 92.927%, batch [256000/756895]\n",
      "loss: 0.021819, accuracy: 93.015%, batch [257280/756895]\n",
      "loss: 0.017043, accuracy: 93.179%, batch [258560/756895]\n",
      "loss: 0.017245, accuracy: 93.171%, batch [259840/756895]\n",
      "loss: 0.022957, accuracy: 92.748%, batch [261120/756895]\n",
      "loss: 0.026861, accuracy: 92.692%, batch [262400/756895]\n",
      "loss: 0.020233, accuracy: 93.203%, batch [263680/756895]\n",
      "loss: 0.027467, accuracy: 92.563%, batch [264960/756895]\n",
      "loss: 0.026372, accuracy: 92.868%, batch [266240/756895]\n",
      "loss: 0.017328, accuracy: 93.010%, batch [267520/756895]\n",
      "loss: 0.021364, accuracy: 92.825%, batch [268800/756895]\n",
      "loss: 0.022871, accuracy: 92.811%, batch [270080/756895]\n",
      "loss: 0.016493, accuracy: 93.169%, batch [271360/756895]\n",
      "loss: 0.016943, accuracy: 93.106%, batch [272640/756895]\n",
      "loss: 0.014737, accuracy: 93.278%, batch [273920/756895]\n",
      "loss: 0.018792, accuracy: 93.056%, batch [275200/756895]\n",
      "loss: 0.017014, accuracy: 93.146%, batch [276480/756895]\n",
      "loss: 0.021350, accuracy: 92.913%, batch [277760/756895]\n",
      "loss: 0.016701, accuracy: 92.966%, batch [279040/756895]\n",
      "loss: 0.018469, accuracy: 93.010%, batch [280320/756895]\n",
      "loss: 0.014976, accuracy: 93.160%, batch [281600/756895]\n",
      "loss: 0.018269, accuracy: 93.225%, batch [282880/756895]\n",
      "loss: 0.021779, accuracy: 92.955%, batch [284160/756895]\n",
      "loss: 0.019618, accuracy: 93.042%, batch [285440/756895]\n",
      "loss: 0.017369, accuracy: 93.044%, batch [286720/756895]\n",
      "loss: 0.018953, accuracy: 92.887%, batch [288000/756895]\n",
      "loss: 0.017413, accuracy: 93.193%, batch [289280/756895]\n",
      "loss: 0.017938, accuracy: 93.080%, batch [290560/756895]\n",
      "loss: 0.032750, accuracy: 92.704%, batch [291840/756895]\n",
      "loss: 0.022951, accuracy: 93.011%, batch [293120/756895]\n",
      "loss: 0.020577, accuracy: 93.075%, batch [294400/756895]\n",
      "loss: 0.018712, accuracy: 93.085%, batch [295680/756895]\n",
      "loss: 0.019878, accuracy: 93.088%, batch [296960/756895]\n",
      "loss: 0.028164, accuracy: 92.885%, batch [298240/756895]\n",
      "loss: 0.017163, accuracy: 93.249%, batch [299520/756895]\n",
      "loss: 0.019738, accuracy: 93.197%, batch [300800/756895]\n",
      "loss: 0.016176, accuracy: 93.201%, batch [302080/756895]\n",
      "loss: 0.017742, accuracy: 93.346%, batch [303360/756895]\n",
      "loss: 0.015828, accuracy: 93.314%, batch [304640/756895]\n",
      "loss: 0.015094, accuracy: 93.224%, batch [305920/756895]\n",
      "loss: 0.021959, accuracy: 92.873%, batch [307200/756895]\n",
      "loss: 0.014416, accuracy: 93.179%, batch [308480/756895]\n",
      "loss: 0.019572, accuracy: 92.724%, batch [309760/756895]\n",
      "loss: 0.016872, accuracy: 93.215%, batch [311040/756895]\n",
      "loss: 0.017566, accuracy: 93.093%, batch [312320/756895]\n",
      "loss: 0.021595, accuracy: 92.947%, batch [313600/756895]\n",
      "loss: 0.024703, accuracy: 92.894%, batch [314880/756895]\n",
      "loss: 0.016063, accuracy: 93.135%, batch [316160/756895]\n",
      "loss: 0.018717, accuracy: 93.070%, batch [317440/756895]\n",
      "loss: 0.018318, accuracy: 93.096%, batch [318720/756895]\n",
      "loss: 0.019101, accuracy: 93.043%, batch [320000/756895]\n",
      "loss: 0.021127, accuracy: 92.918%, batch [321280/756895]\n",
      "loss: 0.022834, accuracy: 92.909%, batch [322560/756895]\n",
      "loss: 0.030859, accuracy: 92.865%, batch [323840/756895]\n",
      "loss: 0.018292, accuracy: 93.029%, batch [325120/756895]\n",
      "loss: 0.016655, accuracy: 93.066%, batch [326400/756895]\n",
      "loss: 0.016992, accuracy: 93.113%, batch [327680/756895]\n",
      "loss: 0.017487, accuracy: 92.944%, batch [328960/756895]\n",
      "loss: 0.021855, accuracy: 92.982%, batch [330240/756895]\n",
      "loss: 0.021364, accuracy: 92.955%, batch [331520/756895]\n",
      "loss: 0.021610, accuracy: 92.868%, batch [332800/756895]\n",
      "loss: 0.016207, accuracy: 93.006%, batch [334080/756895]\n",
      "loss: 0.020862, accuracy: 93.040%, batch [335360/756895]\n",
      "loss: 0.022719, accuracy: 92.851%, batch [336640/756895]\n",
      "loss: 0.019516, accuracy: 92.962%, batch [337920/756895]\n",
      "loss: 0.018711, accuracy: 92.968%, batch [339200/756895]\n",
      "loss: 0.023704, accuracy: 92.942%, batch [340480/756895]\n",
      "loss: 0.020722, accuracy: 93.010%, batch [341760/756895]\n",
      "loss: 0.018398, accuracy: 93.049%, batch [343040/756895]\n",
      "loss: 0.016507, accuracy: 93.135%, batch [344320/756895]\n",
      "loss: 0.015495, accuracy: 93.020%, batch [345600/756895]\n",
      "loss: 0.017606, accuracy: 93.050%, batch [346880/756895]\n",
      "loss: 0.021402, accuracy: 92.743%, batch [348160/756895]\n",
      "loss: 0.017475, accuracy: 92.978%, batch [349440/756895]\n",
      "loss: 0.018687, accuracy: 92.955%, batch [350720/756895]\n",
      "loss: 0.021882, accuracy: 93.140%, batch [352000/756895]\n",
      "loss: 0.019332, accuracy: 93.073%, batch [353280/756895]\n",
      "loss: 0.025183, accuracy: 92.757%, batch [354560/756895]\n",
      "loss: 0.025551, accuracy: 92.986%, batch [355840/756895]\n",
      "loss: 0.019136, accuracy: 93.004%, batch [357120/756895]\n",
      "loss: 0.020949, accuracy: 93.102%, batch [358400/756895]\n",
      "loss: 0.019491, accuracy: 93.075%, batch [359680/756895]\n",
      "loss: 0.017985, accuracy: 92.965%, batch [360960/756895]\n",
      "loss: 0.018090, accuracy: 93.085%, batch [362240/756895]\n",
      "loss: 0.019842, accuracy: 93.069%, batch [363520/756895]\n",
      "loss: 0.016993, accuracy: 93.167%, batch [364800/756895]\n",
      "loss: 0.018710, accuracy: 93.166%, batch [366080/756895]\n",
      "loss: 0.019002, accuracy: 93.125%, batch [367360/756895]\n",
      "loss: 0.016563, accuracy: 93.056%, batch [368640/756895]\n",
      "loss: 0.018007, accuracy: 93.028%, batch [369920/756895]\n",
      "loss: 0.018972, accuracy: 93.059%, batch [371200/756895]\n",
      "loss: 0.017494, accuracy: 92.817%, batch [372480/756895]\n",
      "loss: 0.024134, accuracy: 93.002%, batch [373760/756895]\n",
      "loss: 0.018680, accuracy: 93.335%, batch [375040/756895]\n",
      "loss: 0.020265, accuracy: 93.067%, batch [376320/756895]\n",
      "loss: 0.013854, accuracy: 93.183%, batch [377600/756895]\n",
      "loss: 0.019316, accuracy: 93.176%, batch [378880/756895]\n",
      "loss: 0.027076, accuracy: 92.678%, batch [380160/756895]\n",
      "loss: 0.018140, accuracy: 92.980%, batch [381440/756895]\n",
      "loss: 0.018032, accuracy: 93.148%, batch [382720/756895]\n",
      "loss: 0.017264, accuracy: 93.133%, batch [384000/756895]\n",
      "loss: 0.017592, accuracy: 93.119%, batch [385280/756895]\n",
      "loss: 0.017313, accuracy: 93.159%, batch [386560/756895]\n",
      "loss: 0.017664, accuracy: 93.075%, batch [387840/756895]\n",
      "loss: 0.021449, accuracy: 93.279%, batch [389120/756895]\n",
      "loss: 0.016857, accuracy: 93.169%, batch [390400/756895]\n",
      "loss: 0.015628, accuracy: 93.231%, batch [391680/756895]\n",
      "loss: 0.019343, accuracy: 92.888%, batch [392960/756895]\n",
      "loss: 0.016849, accuracy: 93.105%, batch [394240/756895]\n",
      "loss: 0.015831, accuracy: 93.044%, batch [395520/756895]\n",
      "loss: 0.020474, accuracy: 93.038%, batch [396800/756895]\n",
      "loss: 0.017431, accuracy: 93.200%, batch [398080/756895]\n",
      "loss: 0.022174, accuracy: 92.995%, batch [399360/756895]\n",
      "loss: 0.021317, accuracy: 92.873%, batch [400640/756895]\n",
      "loss: 0.022190, accuracy: 92.943%, batch [401920/756895]\n",
      "loss: 0.016991, accuracy: 93.146%, batch [403200/756895]\n",
      "loss: 0.017664, accuracy: 93.064%, batch [404480/756895]\n",
      "loss: 0.017414, accuracy: 93.377%, batch [405760/756895]\n",
      "loss: 0.019351, accuracy: 93.184%, batch [407040/756895]\n",
      "loss: 0.028110, accuracy: 93.247%, batch [408320/756895]\n",
      "loss: 0.014975, accuracy: 93.161%, batch [409600/756895]\n",
      "loss: 0.019433, accuracy: 92.991%, batch [410880/756895]\n",
      "loss: 0.023886, accuracy: 93.015%, batch [412160/756895]\n",
      "loss: 0.016684, accuracy: 93.125%, batch [413440/756895]\n",
      "loss: 0.018798, accuracy: 93.210%, batch [414720/756895]\n",
      "loss: 0.016819, accuracy: 93.196%, batch [416000/756895]\n",
      "loss: 0.020633, accuracy: 92.983%, batch [417280/756895]\n",
      "loss: 0.018281, accuracy: 93.170%, batch [418560/756895]\n",
      "loss: 0.013467, accuracy: 93.378%, batch [419840/756895]\n",
      "loss: 0.021700, accuracy: 93.135%, batch [421120/756895]\n",
      "loss: 0.015396, accuracy: 93.107%, batch [422400/756895]\n",
      "loss: 0.019238, accuracy: 92.949%, batch [423680/756895]\n",
      "loss: 0.021021, accuracy: 92.962%, batch [424960/756895]\n",
      "loss: 0.018542, accuracy: 93.166%, batch [426240/756895]\n",
      "loss: 0.018978, accuracy: 92.867%, batch [427520/756895]\n",
      "loss: 0.023824, accuracy: 92.786%, batch [428800/756895]\n",
      "loss: 0.018147, accuracy: 93.008%, batch [430080/756895]\n",
      "loss: 0.017019, accuracy: 93.110%, batch [431360/756895]\n",
      "loss: 0.021879, accuracy: 92.702%, batch [432640/756895]\n",
      "loss: 0.019057, accuracy: 93.116%, batch [433920/756895]\n",
      "loss: 0.016430, accuracy: 93.027%, batch [435200/756895]\n",
      "loss: 0.020454, accuracy: 92.968%, batch [436480/756895]\n",
      "loss: 0.019566, accuracy: 92.931%, batch [437760/756895]\n",
      "loss: 0.018640, accuracy: 93.027%, batch [439040/756895]\n",
      "loss: 0.018820, accuracy: 93.118%, batch [440320/756895]\n",
      "loss: 0.018180, accuracy: 93.096%, batch [441600/756895]\n",
      "loss: 0.015039, accuracy: 93.320%, batch [442880/756895]\n",
      "loss: 0.015883, accuracy: 93.308%, batch [444160/756895]\n",
      "loss: 0.019222, accuracy: 93.305%, batch [445440/756895]\n",
      "loss: 0.013610, accuracy: 93.256%, batch [446720/756895]\n",
      "loss: 0.017882, accuracy: 92.964%, batch [448000/756895]\n",
      "loss: 0.016741, accuracy: 93.053%, batch [449280/756895]\n",
      "loss: 0.020712, accuracy: 93.235%, batch [450560/756895]\n",
      "loss: 0.019364, accuracy: 92.934%, batch [451840/756895]\n",
      "loss: 0.017250, accuracy: 93.165%, batch [453120/756895]\n",
      "loss: 0.023452, accuracy: 93.073%, batch [454400/756895]\n",
      "loss: 0.020490, accuracy: 92.847%, batch [455680/756895]\n",
      "loss: 0.015642, accuracy: 93.103%, batch [456960/756895]\n",
      "loss: 0.027002, accuracy: 92.779%, batch [458240/756895]\n",
      "loss: 0.017291, accuracy: 93.091%, batch [459520/756895]\n",
      "loss: 0.023717, accuracy: 92.850%, batch [460800/756895]\n",
      "loss: 0.016480, accuracy: 93.114%, batch [462080/756895]\n",
      "loss: 0.019316, accuracy: 93.260%, batch [463360/756895]\n",
      "loss: 0.019033, accuracy: 93.202%, batch [464640/756895]\n",
      "loss: 0.017878, accuracy: 92.923%, batch [465920/756895]\n",
      "loss: 0.017113, accuracy: 93.079%, batch [467200/756895]\n",
      "loss: 0.019039, accuracy: 93.035%, batch [468480/756895]\n",
      "loss: 0.020520, accuracy: 92.998%, batch [469760/756895]\n",
      "loss: 0.017203, accuracy: 93.112%, batch [471040/756895]\n",
      "loss: 0.023442, accuracy: 92.924%, batch [472320/756895]\n",
      "loss: 0.014567, accuracy: 93.266%, batch [473600/756895]\n",
      "loss: 0.018151, accuracy: 93.093%, batch [474880/756895]\n",
      "loss: 0.014399, accuracy: 93.206%, batch [476160/756895]\n",
      "loss: 0.019332, accuracy: 93.122%, batch [477440/756895]\n",
      "loss: 0.015757, accuracy: 93.175%, batch [478720/756895]\n",
      "loss: 0.020056, accuracy: 93.160%, batch [480000/756895]\n",
      "loss: 0.019050, accuracy: 92.996%, batch [481280/756895]\n",
      "loss: 0.016874, accuracy: 93.189%, batch [482560/756895]\n",
      "loss: 0.024649, accuracy: 92.714%, batch [483840/756895]\n",
      "loss: 0.019379, accuracy: 93.153%, batch [485120/756895]\n",
      "loss: 0.022835, accuracy: 92.992%, batch [486400/756895]\n",
      "loss: 0.018159, accuracy: 93.080%, batch [487680/756895]\n",
      "loss: 0.020343, accuracy: 92.926%, batch [488960/756895]\n",
      "loss: 0.017688, accuracy: 93.199%, batch [490240/756895]\n",
      "loss: 0.017259, accuracy: 93.289%, batch [491520/756895]\n",
      "loss: 0.016809, accuracy: 93.174%, batch [492800/756895]\n",
      "loss: 0.017060, accuracy: 93.256%, batch [494080/756895]\n",
      "loss: 0.019975, accuracy: 92.939%, batch [495360/756895]\n",
      "loss: 0.017269, accuracy: 93.133%, batch [496640/756895]\n",
      "loss: 0.016953, accuracy: 93.120%, batch [497920/756895]\n",
      "loss: 0.016382, accuracy: 93.176%, batch [499200/756895]\n",
      "loss: 0.016345, accuracy: 93.141%, batch [500480/756895]\n",
      "loss: 0.018521, accuracy: 92.986%, batch [501760/756895]\n",
      "loss: 0.020922, accuracy: 93.057%, batch [503040/756895]\n",
      "loss: 0.017131, accuracy: 93.177%, batch [504320/756895]\n",
      "loss: 0.022950, accuracy: 93.019%, batch [505600/756895]\n",
      "loss: 0.017120, accuracy: 93.119%, batch [506880/756895]\n",
      "loss: 0.023526, accuracy: 92.868%, batch [508160/756895]\n",
      "loss: 0.019659, accuracy: 93.035%, batch [509440/756895]\n",
      "loss: 0.016791, accuracy: 93.185%, batch [510720/756895]\n",
      "loss: 0.013891, accuracy: 93.299%, batch [512000/756895]\n",
      "loss: 0.015084, accuracy: 93.198%, batch [513280/756895]\n",
      "loss: 0.021200, accuracy: 93.032%, batch [514560/756895]\n",
      "loss: 0.018297, accuracy: 93.021%, batch [515840/756895]\n",
      "loss: 0.017928, accuracy: 93.259%, batch [517120/756895]\n",
      "loss: 0.015464, accuracy: 93.150%, batch [518400/756895]\n",
      "loss: 0.018309, accuracy: 92.987%, batch [519680/756895]\n",
      "loss: 0.018484, accuracy: 93.058%, batch [520960/756895]\n",
      "loss: 0.017299, accuracy: 93.194%, batch [522240/756895]\n",
      "loss: 0.019002, accuracy: 93.182%, batch [523520/756895]\n",
      "loss: 0.018782, accuracy: 93.008%, batch [524800/756895]\n",
      "loss: 0.017559, accuracy: 93.165%, batch [526080/756895]\n",
      "loss: 0.017082, accuracy: 93.113%, batch [527360/756895]\n",
      "loss: 0.020749, accuracy: 93.176%, batch [528640/756895]\n",
      "loss: 0.024953, accuracy: 92.840%, batch [529920/756895]\n",
      "loss: 0.019924, accuracy: 93.017%, batch [531200/756895]\n",
      "loss: 0.015730, accuracy: 93.118%, batch [532480/756895]\n",
      "loss: 0.019446, accuracy: 93.080%, batch [533760/756895]\n",
      "loss: 0.017633, accuracy: 93.028%, batch [535040/756895]\n",
      "loss: 0.017847, accuracy: 93.125%, batch [536320/756895]\n",
      "loss: 0.019805, accuracy: 92.997%, batch [537600/756895]\n",
      "loss: 0.016472, accuracy: 93.121%, batch [538880/756895]\n",
      "loss: 0.018866, accuracy: 92.841%, batch [540160/756895]\n",
      "loss: 0.021015, accuracy: 93.017%, batch [541440/756895]\n",
      "loss: 0.018810, accuracy: 93.080%, batch [542720/756895]\n",
      "loss: 0.018058, accuracy: 93.119%, batch [544000/756895]\n",
      "loss: 0.015162, accuracy: 93.376%, batch [545280/756895]\n",
      "loss: 0.019407, accuracy: 93.030%, batch [546560/756895]\n",
      "loss: 0.025068, accuracy: 92.811%, batch [547840/756895]\n",
      "loss: 0.019672, accuracy: 92.903%, batch [549120/756895]\n",
      "loss: 0.020311, accuracy: 92.940%, batch [550400/756895]\n",
      "loss: 0.018916, accuracy: 93.006%, batch [551680/756895]\n",
      "loss: 0.014301, accuracy: 93.296%, batch [552960/756895]\n",
      "loss: 0.020849, accuracy: 92.916%, batch [554240/756895]\n",
      "loss: 0.015976, accuracy: 92.933%, batch [555520/756895]\n",
      "loss: 0.018060, accuracy: 93.201%, batch [556800/756895]\n",
      "loss: 0.021991, accuracy: 92.857%, batch [558080/756895]\n",
      "loss: 0.015226, accuracy: 93.244%, batch [559360/756895]\n",
      "loss: 0.022561, accuracy: 93.080%, batch [560640/756895]\n",
      "loss: 0.014945, accuracy: 93.273%, batch [561920/756895]\n",
      "loss: 0.018878, accuracy: 93.086%, batch [563200/756895]\n",
      "loss: 0.016658, accuracy: 93.171%, batch [564480/756895]\n",
      "loss: 0.017417, accuracy: 93.043%, batch [565760/756895]\n",
      "loss: 0.016705, accuracy: 93.080%, batch [567040/756895]\n",
      "loss: 0.021506, accuracy: 93.006%, batch [568320/756895]\n",
      "loss: 0.017607, accuracy: 93.192%, batch [569600/756895]\n",
      "loss: 0.017921, accuracy: 93.213%, batch [570880/756895]\n",
      "loss: 0.020358, accuracy: 92.985%, batch [572160/756895]\n",
      "loss: 0.019647, accuracy: 93.238%, batch [573440/756895]\n",
      "loss: 0.018022, accuracy: 93.057%, batch [574720/756895]\n",
      "loss: 0.019095, accuracy: 93.090%, batch [576000/756895]\n",
      "loss: 0.017978, accuracy: 93.098%, batch [577280/756895]\n",
      "loss: 0.016967, accuracy: 93.270%, batch [578560/756895]\n",
      "loss: 0.018652, accuracy: 93.036%, batch [579840/756895]\n",
      "loss: 0.017656, accuracy: 93.109%, batch [581120/756895]\n",
      "loss: 0.016804, accuracy: 93.267%, batch [582400/756895]\n",
      "loss: 0.018750, accuracy: 92.920%, batch [583680/756895]\n",
      "loss: 0.018126, accuracy: 93.249%, batch [584960/756895]\n",
      "loss: 0.020123, accuracy: 93.016%, batch [586240/756895]\n",
      "loss: 0.017807, accuracy: 93.214%, batch [587520/756895]\n",
      "loss: 0.027722, accuracy: 92.834%, batch [588800/756895]\n",
      "loss: 0.020188, accuracy: 93.111%, batch [590080/756895]\n",
      "loss: 0.020118, accuracy: 93.156%, batch [591360/756895]\n",
      "loss: 0.016980, accuracy: 93.152%, batch [592640/756895]\n",
      "loss: 0.016836, accuracy: 93.151%, batch [593920/756895]\n",
      "loss: 0.018975, accuracy: 93.071%, batch [595200/756895]\n",
      "loss: 0.020936, accuracy: 93.232%, batch [596480/756895]\n",
      "loss: 0.021417, accuracy: 93.086%, batch [597760/756895]\n",
      "loss: 0.018979, accuracy: 93.165%, batch [599040/756895]\n",
      "loss: 0.020733, accuracy: 93.018%, batch [600320/756895]\n",
      "loss: 0.024307, accuracy: 92.920%, batch [601600/756895]\n",
      "loss: 0.019607, accuracy: 93.017%, batch [602880/756895]\n",
      "loss: 0.018682, accuracy: 93.267%, batch [604160/756895]\n",
      "loss: 0.017792, accuracy: 92.860%, batch [605440/756895]\n",
      "loss: 0.018923, accuracy: 93.194%, batch [606720/756895]\n",
      "loss: 0.017982, accuracy: 93.173%, batch [608000/756895]\n",
      "loss: 0.020211, accuracy: 93.008%, batch [609280/756895]\n",
      "loss: 0.016038, accuracy: 93.264%, batch [610560/756895]\n",
      "loss: 0.020516, accuracy: 92.980%, batch [611840/756895]\n",
      "loss: 0.017573, accuracy: 93.166%, batch [613120/756895]\n",
      "loss: 0.019633, accuracy: 93.037%, batch [614400/756895]\n",
      "loss: 0.017237, accuracy: 93.111%, batch [615680/756895]\n",
      "loss: 0.020615, accuracy: 93.009%, batch [616960/756895]\n",
      "loss: 0.016863, accuracy: 93.126%, batch [618240/756895]\n",
      "loss: 0.020479, accuracy: 93.028%, batch [619520/756895]\n",
      "loss: 0.018176, accuracy: 93.085%, batch [620800/756895]\n",
      "loss: 0.018606, accuracy: 93.032%, batch [622080/756895]\n",
      "loss: 0.018977, accuracy: 93.014%, batch [623360/756895]\n",
      "loss: 0.021915, accuracy: 93.021%, batch [624640/756895]\n",
      "loss: 0.018912, accuracy: 93.055%, batch [625920/756895]\n",
      "loss: 0.020228, accuracy: 93.070%, batch [627200/756895]\n",
      "loss: 0.019044, accuracy: 93.122%, batch [628480/756895]\n",
      "loss: 0.020533, accuracy: 92.927%, batch [629760/756895]\n",
      "loss: 0.022410, accuracy: 92.905%, batch [631040/756895]\n",
      "loss: 0.021632, accuracy: 92.998%, batch [632320/756895]\n",
      "loss: 0.016888, accuracy: 93.179%, batch [633600/756895]\n",
      "loss: 0.016263, accuracy: 93.126%, batch [634880/756895]\n",
      "loss: 0.014750, accuracy: 93.172%, batch [636160/756895]\n",
      "loss: 0.022311, accuracy: 93.013%, batch [637440/756895]\n",
      "loss: 0.015196, accuracy: 93.224%, batch [638720/756895]\n",
      "loss: 0.024541, accuracy: 92.898%, batch [640000/756895]\n",
      "loss: 0.014616, accuracy: 93.256%, batch [641280/756895]\n",
      "loss: 0.025203, accuracy: 92.974%, batch [642560/756895]\n",
      "loss: 0.022884, accuracy: 92.955%, batch [643840/756895]\n",
      "loss: 0.021972, accuracy: 92.847%, batch [645120/756895]\n",
      "loss: 0.017192, accuracy: 93.033%, batch [646400/756895]\n",
      "loss: 0.017204, accuracy: 93.051%, batch [647680/756895]\n",
      "loss: 0.024415, accuracy: 92.899%, batch [648960/756895]\n",
      "loss: 0.020067, accuracy: 92.985%, batch [650240/756895]\n",
      "loss: 0.023483, accuracy: 93.058%, batch [651520/756895]\n",
      "loss: 0.019622, accuracy: 93.062%, batch [652800/756895]\n",
      "loss: 0.019266, accuracy: 93.046%, batch [654080/756895]\n",
      "loss: 0.021732, accuracy: 92.924%, batch [655360/756895]\n",
      "loss: 0.016873, accuracy: 93.214%, batch [656640/756895]\n",
      "loss: 0.018082, accuracy: 93.071%, batch [657920/756895]\n",
      "loss: 0.017995, accuracy: 92.914%, batch [659200/756895]\n",
      "loss: 0.016800, accuracy: 93.319%, batch [660480/756895]\n",
      "loss: 0.020022, accuracy: 93.316%, batch [661760/756895]\n",
      "loss: 0.016502, accuracy: 92.992%, batch [663040/756895]\n",
      "loss: 0.018234, accuracy: 93.185%, batch [664320/756895]\n",
      "loss: 0.022710, accuracy: 92.968%, batch [665600/756895]\n",
      "loss: 0.021358, accuracy: 93.022%, batch [666880/756895]\n",
      "loss: 0.016694, accuracy: 93.115%, batch [668160/756895]\n",
      "loss: 0.016947, accuracy: 93.145%, batch [669440/756895]\n",
      "loss: 0.017737, accuracy: 92.922%, batch [670720/756895]\n",
      "loss: 0.019508, accuracy: 92.996%, batch [672000/756895]\n",
      "loss: 0.023294, accuracy: 92.772%, batch [673280/756895]\n",
      "loss: 0.023843, accuracy: 92.844%, batch [674560/756895]\n",
      "loss: 0.016780, accuracy: 93.103%, batch [675840/756895]\n",
      "loss: 0.016122, accuracy: 93.129%, batch [677120/756895]\n",
      "loss: 0.017057, accuracy: 92.990%, batch [678400/756895]\n",
      "loss: 0.017520, accuracy: 92.992%, batch [679680/756895]\n",
      "loss: 0.018184, accuracy: 93.089%, batch [680960/756895]\n",
      "loss: 0.017675, accuracy: 92.987%, batch [682240/756895]\n",
      "loss: 0.019341, accuracy: 92.885%, batch [683520/756895]\n",
      "loss: 0.016700, accuracy: 92.841%, batch [684800/756895]\n",
      "loss: 0.020030, accuracy: 93.046%, batch [686080/756895]\n",
      "loss: 0.023905, accuracy: 92.900%, batch [687360/756895]\n",
      "loss: 0.014940, accuracy: 93.314%, batch [688640/756895]\n",
      "loss: 0.018340, accuracy: 92.938%, batch [689920/756895]\n",
      "loss: 0.016828, accuracy: 92.995%, batch [691200/756895]\n",
      "loss: 0.018936, accuracy: 93.066%, batch [692480/756895]\n",
      "loss: 0.024405, accuracy: 93.193%, batch [693760/756895]\n",
      "loss: 0.022176, accuracy: 92.919%, batch [695040/756895]\n",
      "loss: 0.020101, accuracy: 92.971%, batch [696320/756895]\n",
      "loss: 0.017374, accuracy: 93.070%, batch [697600/756895]\n",
      "loss: 0.017602, accuracy: 93.083%, batch [698880/756895]\n",
      "loss: 0.020514, accuracy: 93.151%, batch [700160/756895]\n",
      "loss: 0.019905, accuracy: 92.999%, batch [701440/756895]\n",
      "loss: 0.015389, accuracy: 93.144%, batch [702720/756895]\n",
      "loss: 0.024709, accuracy: 92.886%, batch [704000/756895]\n",
      "loss: 0.018831, accuracy: 93.181%, batch [705280/756895]\n",
      "loss: 0.019566, accuracy: 93.024%, batch [706560/756895]\n",
      "loss: 0.015382, accuracy: 93.130%, batch [707840/756895]\n",
      "loss: 0.018456, accuracy: 93.236%, batch [709120/756895]\n",
      "loss: 0.021368, accuracy: 92.895%, batch [710400/756895]\n",
      "loss: 0.016592, accuracy: 93.113%, batch [711680/756895]\n",
      "loss: 0.018136, accuracy: 92.991%, batch [712960/756895]\n",
      "loss: 0.016251, accuracy: 93.239%, batch [714240/756895]\n",
      "loss: 0.016483, accuracy: 93.145%, batch [715520/756895]\n",
      "loss: 0.016228, accuracy: 93.186%, batch [716800/756895]\n",
      "loss: 0.016181, accuracy: 93.238%, batch [718080/756895]\n",
      "loss: 0.029716, accuracy: 92.609%, batch [719360/756895]\n",
      "loss: 0.020371, accuracy: 93.031%, batch [720640/756895]\n",
      "loss: 0.019008, accuracy: 93.120%, batch [721920/756895]\n",
      "loss: 0.020479, accuracy: 93.006%, batch [723200/756895]\n",
      "loss: 0.019623, accuracy: 92.892%, batch [724480/756895]\n",
      "loss: 0.022115, accuracy: 93.031%, batch [725760/756895]\n",
      "loss: 0.016733, accuracy: 93.132%, batch [727040/756895]\n",
      "loss: 0.020185, accuracy: 92.803%, batch [728320/756895]\n",
      "loss: 0.017836, accuracy: 93.112%, batch [729600/756895]\n",
      "loss: 0.019057, accuracy: 92.946%, batch [730880/756895]\n",
      "loss: 0.020017, accuracy: 93.023%, batch [732160/756895]\n",
      "loss: 0.016529, accuracy: 93.130%, batch [733440/756895]\n",
      "loss: 0.019940, accuracy: 92.975%, batch [734720/756895]\n",
      "loss: 0.015887, accuracy: 93.156%, batch [736000/756895]\n",
      "loss: 0.017114, accuracy: 93.157%, batch [737280/756895]\n",
      "loss: 0.019607, accuracy: 92.730%, batch [738560/756895]\n",
      "loss: 0.020146, accuracy: 93.054%, batch [739840/756895]\n",
      "loss: 0.017862, accuracy: 93.141%, batch [741120/756895]\n",
      "loss: 0.016355, accuracy: 93.324%, batch [742400/756895]\n",
      "loss: 0.014849, accuracy: 93.198%, batch [743680/756895]\n",
      "loss: 0.020094, accuracy: 93.047%, batch [744960/756895]\n",
      "loss: 0.020318, accuracy: 93.039%, batch [746240/756895]\n",
      "loss: 0.017560, accuracy: 93.062%, batch [747520/756895]\n",
      "loss: 0.016639, accuracy: 93.102%, batch [748800/756895]\n",
      "loss: 0.016766, accuracy: 93.175%, batch [750080/756895]\n",
      "loss: 0.018845, accuracy: 93.218%, batch [751360/756895]\n",
      "loss: 0.015809, accuracy: 93.251%, batch [752640/756895]\n",
      "loss: 0.018613, accuracy: 92.999%, batch [753920/756895]\n",
      "loss: 0.020107, accuracy: 93.127%, batch [755200/756895]\n",
      "loss: 0.021425, accuracy: 93.031%, batch [756480/756895]\n",
      "Test avg loss: 0.019171, test avg accuracy: 93.040% \n",
      "\n",
      "Test avg loss: 0.019050, test avg accuracy: 93.058% \n",
      "\n",
      "Epoch 116\n",
      "------------------------\n",
      "loss: 0.016623, accuracy: 93.142%, batch [    0/756895]\n",
      "loss: 0.016951, accuracy: 93.074%, batch [ 1280/756895]\n",
      "loss: 0.019911, accuracy: 92.969%, batch [ 2560/756895]\n",
      "loss: 0.016987, accuracy: 92.996%, batch [ 3840/756895]\n",
      "loss: 0.015197, accuracy: 93.186%, batch [ 5120/756895]\n",
      "loss: 0.017742, accuracy: 93.255%, batch [ 6400/756895]\n",
      "loss: 0.015568, accuracy: 93.072%, batch [ 7680/756895]\n",
      "loss: 0.017804, accuracy: 93.042%, batch [ 8960/756895]\n",
      "loss: 0.016987, accuracy: 93.010%, batch [10240/756895]\n",
      "loss: 0.023576, accuracy: 92.835%, batch [11520/756895]\n",
      "loss: 0.016806, accuracy: 93.188%, batch [12800/756895]\n",
      "loss: 0.019504, accuracy: 93.003%, batch [14080/756895]\n",
      "loss: 0.018339, accuracy: 93.049%, batch [15360/756895]\n",
      "loss: 0.017100, accuracy: 93.139%, batch [16640/756895]\n",
      "loss: 0.017897, accuracy: 93.054%, batch [17920/756895]\n",
      "loss: 0.014278, accuracy: 93.268%, batch [19200/756895]\n",
      "loss: 0.021523, accuracy: 92.860%, batch [20480/756895]\n",
      "loss: 0.022028, accuracy: 93.086%, batch [21760/756895]\n",
      "loss: 0.016258, accuracy: 93.159%, batch [23040/756895]\n",
      "loss: 0.016602, accuracy: 93.050%, batch [24320/756895]\n",
      "loss: 0.020141, accuracy: 92.958%, batch [25600/756895]\n",
      "loss: 0.021838, accuracy: 93.138%, batch [26880/756895]\n",
      "loss: 0.018873, accuracy: 93.118%, batch [28160/756895]\n",
      "loss: 0.016603, accuracy: 93.260%, batch [29440/756895]\n",
      "loss: 0.014333, accuracy: 93.331%, batch [30720/756895]\n",
      "loss: 0.016186, accuracy: 92.931%, batch [32000/756895]\n",
      "loss: 0.020279, accuracy: 92.939%, batch [33280/756895]\n",
      "loss: 0.018708, accuracy: 93.100%, batch [34560/756895]\n",
      "loss: 0.015284, accuracy: 93.261%, batch [35840/756895]\n",
      "loss: 0.020914, accuracy: 93.101%, batch [37120/756895]\n",
      "loss: 0.015704, accuracy: 93.165%, batch [38400/756895]\n",
      "loss: 0.017105, accuracy: 93.122%, batch [39680/756895]\n",
      "loss: 0.015235, accuracy: 93.159%, batch [40960/756895]\n",
      "loss: 0.014258, accuracy: 93.225%, batch [42240/756895]\n",
      "loss: 0.020409, accuracy: 92.963%, batch [43520/756895]\n",
      "loss: 0.019446, accuracy: 93.204%, batch [44800/756895]\n",
      "loss: 0.017579, accuracy: 93.040%, batch [46080/756895]\n",
      "loss: 0.017667, accuracy: 92.961%, batch [47360/756895]\n",
      "loss: 0.027445, accuracy: 92.929%, batch [48640/756895]\n",
      "loss: 0.022623, accuracy: 92.840%, batch [49920/756895]\n",
      "loss: 0.020225, accuracy: 93.056%, batch [51200/756895]\n",
      "loss: 0.017994, accuracy: 93.077%, batch [52480/756895]\n",
      "loss: 0.017599, accuracy: 93.056%, batch [53760/756895]\n",
      "loss: 0.020447, accuracy: 93.014%, batch [55040/756895]\n",
      "loss: 0.018936, accuracy: 93.162%, batch [56320/756895]\n",
      "loss: 0.015624, accuracy: 93.132%, batch [57600/756895]\n",
      "loss: 0.014746, accuracy: 93.260%, batch [58880/756895]\n",
      "loss: 0.017429, accuracy: 93.073%, batch [60160/756895]\n",
      "loss: 0.017492, accuracy: 93.130%, batch [61440/756895]\n",
      "loss: 0.015781, accuracy: 93.247%, batch [62720/756895]\n",
      "loss: 0.019197, accuracy: 93.143%, batch [64000/756895]\n",
      "loss: 0.023527, accuracy: 93.048%, batch [65280/756895]\n",
      "loss: 0.026234, accuracy: 92.900%, batch [66560/756895]\n",
      "loss: 0.017917, accuracy: 93.117%, batch [67840/756895]\n",
      "loss: 0.017835, accuracy: 93.152%, batch [69120/756895]\n",
      "loss: 0.019266, accuracy: 92.928%, batch [70400/756895]\n",
      "loss: 0.019182, accuracy: 93.145%, batch [71680/756895]\n",
      "loss: 0.016839, accuracy: 93.117%, batch [72960/756895]\n",
      "loss: 0.018166, accuracy: 92.910%, batch [74240/756895]\n",
      "loss: 0.027756, accuracy: 92.795%, batch [75520/756895]\n",
      "loss: 0.020271, accuracy: 93.222%, batch [76800/756895]\n",
      "loss: 0.016400, accuracy: 93.081%, batch [78080/756895]\n",
      "loss: 0.020003, accuracy: 93.100%, batch [79360/756895]\n",
      "loss: 0.020996, accuracy: 92.970%, batch [80640/756895]\n",
      "loss: 0.020354, accuracy: 93.131%, batch [81920/756895]\n",
      "loss: 0.020665, accuracy: 93.026%, batch [83200/756895]\n",
      "loss: 0.019406, accuracy: 92.940%, batch [84480/756895]\n",
      "loss: 0.015280, accuracy: 93.190%, batch [85760/756895]\n",
      "loss: 0.020032, accuracy: 93.214%, batch [87040/756895]\n",
      "loss: 0.019882, accuracy: 93.078%, batch [88320/756895]\n",
      "loss: 0.021288, accuracy: 92.899%, batch [89600/756895]\n",
      "loss: 0.017057, accuracy: 93.057%, batch [90880/756895]\n",
      "loss: 0.016928, accuracy: 93.170%, batch [92160/756895]\n",
      "loss: 0.017174, accuracy: 93.220%, batch [93440/756895]\n",
      "loss: 0.024523, accuracy: 92.879%, batch [94720/756895]\n",
      "loss: 0.019763, accuracy: 92.929%, batch [96000/756895]\n",
      "loss: 0.016041, accuracy: 93.313%, batch [97280/756895]\n",
      "loss: 0.020367, accuracy: 92.961%, batch [98560/756895]\n",
      "loss: 0.020728, accuracy: 92.949%, batch [99840/756895]\n",
      "loss: 0.016710, accuracy: 93.058%, batch [101120/756895]\n",
      "loss: 0.018922, accuracy: 93.090%, batch [102400/756895]\n",
      "loss: 0.016485, accuracy: 93.260%, batch [103680/756895]\n",
      "loss: 0.031483, accuracy: 92.564%, batch [104960/756895]\n",
      "loss: 0.018272, accuracy: 93.061%, batch [106240/756895]\n",
      "loss: 0.019809, accuracy: 93.229%, batch [107520/756895]\n",
      "loss: 0.017613, accuracy: 93.002%, batch [108800/756895]\n",
      "loss: 0.013447, accuracy: 93.325%, batch [110080/756895]\n",
      "loss: 0.020656, accuracy: 92.822%, batch [111360/756895]\n",
      "loss: 0.017502, accuracy: 93.294%, batch [112640/756895]\n",
      "loss: 0.017999, accuracy: 93.146%, batch [113920/756895]\n",
      "loss: 0.018344, accuracy: 93.157%, batch [115200/756895]\n",
      "loss: 0.022509, accuracy: 93.036%, batch [116480/756895]\n",
      "loss: 0.019370, accuracy: 93.079%, batch [117760/756895]\n",
      "loss: 0.018417, accuracy: 93.177%, batch [119040/756895]\n",
      "loss: 0.014033, accuracy: 93.205%, batch [120320/756895]\n",
      "loss: 0.018182, accuracy: 93.014%, batch [121600/756895]\n",
      "loss: 0.015575, accuracy: 93.233%, batch [122880/756895]\n",
      "loss: 0.015384, accuracy: 93.263%, batch [124160/756895]\n",
      "loss: 0.020762, accuracy: 93.071%, batch [125440/756895]\n",
      "loss: 0.017238, accuracy: 93.085%, batch [126720/756895]\n",
      "loss: 0.015747, accuracy: 93.110%, batch [128000/756895]\n",
      "loss: 0.019663, accuracy: 93.126%, batch [129280/756895]\n",
      "loss: 0.017850, accuracy: 93.231%, batch [130560/756895]\n",
      "loss: 0.014335, accuracy: 93.294%, batch [131840/756895]\n",
      "loss: 0.017087, accuracy: 93.023%, batch [133120/756895]\n",
      "loss: 0.016645, accuracy: 93.112%, batch [134400/756895]\n",
      "loss: 0.021741, accuracy: 92.976%, batch [135680/756895]\n",
      "loss: 0.018531, accuracy: 93.116%, batch [136960/756895]\n",
      "loss: 0.016778, accuracy: 93.014%, batch [138240/756895]\n",
      "loss: 0.019732, accuracy: 92.961%, batch [139520/756895]\n",
      "loss: 0.023359, accuracy: 93.009%, batch [140800/756895]\n",
      "loss: 0.018237, accuracy: 92.984%, batch [142080/756895]\n",
      "loss: 0.018413, accuracy: 92.982%, batch [143360/756895]\n",
      "loss: 0.017432, accuracy: 93.169%, batch [144640/756895]\n",
      "loss: 0.016012, accuracy: 93.148%, batch [145920/756895]\n",
      "loss: 0.017705, accuracy: 93.187%, batch [147200/756895]\n",
      "loss: 0.017225, accuracy: 93.188%, batch [148480/756895]\n",
      "loss: 0.014515, accuracy: 93.201%, batch [149760/756895]\n",
      "loss: 0.015058, accuracy: 93.095%, batch [151040/756895]\n",
      "loss: 0.015852, accuracy: 93.150%, batch [152320/756895]\n",
      "loss: 0.017017, accuracy: 93.163%, batch [153600/756895]\n",
      "loss: 0.026178, accuracy: 92.735%, batch [154880/756895]\n",
      "loss: 0.018461, accuracy: 92.993%, batch [156160/756895]\n",
      "loss: 0.015745, accuracy: 93.203%, batch [157440/756895]\n",
      "loss: 0.020559, accuracy: 93.182%, batch [158720/756895]\n",
      "loss: 0.021499, accuracy: 93.108%, batch [160000/756895]\n",
      "loss: 0.018518, accuracy: 93.052%, batch [161280/756895]\n",
      "loss: 0.015658, accuracy: 93.382%, batch [162560/756895]\n",
      "loss: 0.015957, accuracy: 93.404%, batch [163840/756895]\n",
      "loss: 0.017962, accuracy: 93.052%, batch [165120/756895]\n",
      "loss: 0.020435, accuracy: 92.951%, batch [166400/756895]\n",
      "loss: 0.027844, accuracy: 92.847%, batch [167680/756895]\n",
      "loss: 0.018155, accuracy: 93.142%, batch [168960/756895]\n",
      "loss: 0.021239, accuracy: 92.959%, batch [170240/756895]\n",
      "loss: 0.016633, accuracy: 93.329%, batch [171520/756895]\n",
      "loss: 0.016381, accuracy: 93.109%, batch [172800/756895]\n",
      "loss: 0.018748, accuracy: 93.039%, batch [174080/756895]\n",
      "loss: 0.018441, accuracy: 92.831%, batch [175360/756895]\n",
      "loss: 0.019593, accuracy: 93.086%, batch [176640/756895]\n",
      "loss: 0.012893, accuracy: 93.433%, batch [177920/756895]\n",
      "loss: 0.016391, accuracy: 93.136%, batch [179200/756895]\n",
      "loss: 0.019860, accuracy: 93.062%, batch [180480/756895]\n",
      "loss: 0.020415, accuracy: 93.030%, batch [181760/756895]\n",
      "loss: 0.020689, accuracy: 93.029%, batch [183040/756895]\n",
      "loss: 0.016836, accuracy: 93.292%, batch [184320/756895]\n",
      "loss: 0.025745, accuracy: 92.895%, batch [185600/756895]\n",
      "loss: 0.018482, accuracy: 93.031%, batch [186880/756895]\n",
      "loss: 0.018286, accuracy: 93.244%, batch [188160/756895]\n",
      "loss: 0.017007, accuracy: 93.201%, batch [189440/756895]\n",
      "loss: 0.019707, accuracy: 93.122%, batch [190720/756895]\n",
      "loss: 0.020668, accuracy: 93.094%, batch [192000/756895]\n",
      "loss: 0.017176, accuracy: 93.068%, batch [193280/756895]\n",
      "loss: 0.016341, accuracy: 93.258%, batch [194560/756895]\n",
      "loss: 0.018704, accuracy: 93.150%, batch [195840/756895]\n",
      "loss: 0.021862, accuracy: 93.112%, batch [197120/756895]\n",
      "loss: 0.017015, accuracy: 93.068%, batch [198400/756895]\n",
      "loss: 0.017578, accuracy: 93.213%, batch [199680/756895]\n",
      "loss: 0.016553, accuracy: 93.213%, batch [200960/756895]\n",
      "loss: 0.017727, accuracy: 93.054%, batch [202240/756895]\n",
      "loss: 0.016211, accuracy: 93.077%, batch [203520/756895]\n",
      "loss: 0.017792, accuracy: 93.150%, batch [204800/756895]\n",
      "loss: 0.018432, accuracy: 92.983%, batch [206080/756895]\n",
      "loss: 0.017563, accuracy: 93.097%, batch [207360/756895]\n",
      "loss: 0.016115, accuracy: 93.263%, batch [208640/756895]\n",
      "loss: 0.018570, accuracy: 93.172%, batch [209920/756895]\n",
      "loss: 0.022178, accuracy: 92.943%, batch [211200/756895]\n",
      "loss: 0.021404, accuracy: 92.887%, batch [212480/756895]\n",
      "loss: 0.020610, accuracy: 93.017%, batch [213760/756895]\n",
      "loss: 0.015997, accuracy: 93.046%, batch [215040/756895]\n",
      "loss: 0.015764, accuracy: 92.968%, batch [216320/756895]\n",
      "loss: 0.019485, accuracy: 93.260%, batch [217600/756895]\n",
      "loss: 0.017086, accuracy: 93.193%, batch [218880/756895]\n",
      "loss: 0.017313, accuracy: 93.156%, batch [220160/756895]\n",
      "loss: 0.021857, accuracy: 93.081%, batch [221440/756895]\n",
      "loss: 0.015824, accuracy: 93.147%, batch [222720/756895]\n",
      "loss: 0.018397, accuracy: 93.079%, batch [224000/756895]\n",
      "loss: 0.017481, accuracy: 92.930%, batch [225280/756895]\n",
      "loss: 0.016788, accuracy: 93.180%, batch [226560/756895]\n",
      "loss: 0.018993, accuracy: 93.084%, batch [227840/756895]\n",
      "loss: 0.015576, accuracy: 93.173%, batch [229120/756895]\n",
      "loss: 0.014276, accuracy: 93.205%, batch [230400/756895]\n",
      "loss: 0.017321, accuracy: 93.206%, batch [231680/756895]\n",
      "loss: 0.019188, accuracy: 93.006%, batch [232960/756895]\n",
      "loss: 0.016931, accuracy: 93.108%, batch [234240/756895]\n",
      "loss: 0.018540, accuracy: 92.961%, batch [235520/756895]\n",
      "loss: 0.016503, accuracy: 93.101%, batch [236800/756895]\n",
      "loss: 0.016786, accuracy: 93.212%, batch [238080/756895]\n",
      "loss: 0.014188, accuracy: 93.270%, batch [239360/756895]\n",
      "loss: 0.017737, accuracy: 92.968%, batch [240640/756895]\n",
      "loss: 0.025490, accuracy: 92.905%, batch [241920/756895]\n",
      "loss: 0.017244, accuracy: 93.013%, batch [243200/756895]\n",
      "loss: 0.022160, accuracy: 92.923%, batch [244480/756895]\n",
      "loss: 0.017597, accuracy: 93.134%, batch [245760/756895]\n",
      "loss: 0.018363, accuracy: 93.099%, batch [247040/756895]\n",
      "loss: 0.021950, accuracy: 92.794%, batch [248320/756895]\n",
      "loss: 0.016500, accuracy: 93.254%, batch [249600/756895]\n",
      "loss: 0.019986, accuracy: 92.892%, batch [250880/756895]\n",
      "loss: 0.017426, accuracy: 93.160%, batch [252160/756895]\n",
      "loss: 0.019536, accuracy: 92.994%, batch [253440/756895]\n",
      "loss: 0.020070, accuracy: 92.862%, batch [254720/756895]\n",
      "loss: 0.019552, accuracy: 93.137%, batch [256000/756895]\n",
      "loss: 0.015732, accuracy: 93.217%, batch [257280/756895]\n",
      "loss: 0.017600, accuracy: 92.881%, batch [258560/756895]\n",
      "loss: 0.023425, accuracy: 92.946%, batch [259840/756895]\n",
      "loss: 0.017794, accuracy: 93.032%, batch [261120/756895]\n",
      "loss: 0.017385, accuracy: 93.102%, batch [262400/756895]\n",
      "loss: 0.021185, accuracy: 93.055%, batch [263680/756895]\n",
      "loss: 0.018254, accuracy: 93.141%, batch [264960/756895]\n",
      "loss: 0.021293, accuracy: 92.939%, batch [266240/756895]\n",
      "loss: 0.014348, accuracy: 93.237%, batch [267520/756895]\n",
      "loss: 0.021735, accuracy: 92.904%, batch [268800/756895]\n",
      "loss: 0.017799, accuracy: 93.028%, batch [270080/756895]\n",
      "loss: 0.016908, accuracy: 93.160%, batch [271360/756895]\n",
      "loss: 0.023425, accuracy: 92.998%, batch [272640/756895]\n",
      "loss: 0.018442, accuracy: 93.159%, batch [273920/756895]\n",
      "loss: 0.022909, accuracy: 92.891%, batch [275200/756895]\n",
      "loss: 0.018549, accuracy: 93.092%, batch [276480/756895]\n",
      "loss: 0.024003, accuracy: 92.873%, batch [277760/756895]\n",
      "loss: 0.019605, accuracy: 93.139%, batch [279040/756895]\n",
      "loss: 0.022499, accuracy: 93.040%, batch [280320/756895]\n",
      "loss: 0.016915, accuracy: 93.035%, batch [281600/756895]\n",
      "loss: 0.017787, accuracy: 93.135%, batch [282880/756895]\n",
      "loss: 0.018521, accuracy: 92.921%, batch [284160/756895]\n",
      "loss: 0.019044, accuracy: 93.119%, batch [285440/756895]\n",
      "loss: 0.014897, accuracy: 93.168%, batch [286720/756895]\n",
      "loss: 0.015633, accuracy: 93.169%, batch [288000/756895]\n",
      "loss: 0.018751, accuracy: 93.039%, batch [289280/756895]\n",
      "loss: 0.016580, accuracy: 93.296%, batch [290560/756895]\n",
      "loss: 0.020915, accuracy: 93.014%, batch [291840/756895]\n",
      "loss: 0.022113, accuracy: 92.763%, batch [293120/756895]\n",
      "loss: 0.016514, accuracy: 93.024%, batch [294400/756895]\n",
      "loss: 0.017826, accuracy: 93.267%, batch [295680/756895]\n",
      "loss: 0.017758, accuracy: 93.057%, batch [296960/756895]\n",
      "loss: 0.017992, accuracy: 93.052%, batch [298240/756895]\n",
      "loss: 0.020840, accuracy: 93.096%, batch [299520/756895]\n",
      "loss: 0.020196, accuracy: 93.235%, batch [300800/756895]\n",
      "loss: 0.018107, accuracy: 92.949%, batch [302080/756895]\n",
      "loss: 0.021175, accuracy: 92.940%, batch [303360/756895]\n",
      "loss: 0.018586, accuracy: 93.135%, batch [304640/756895]\n",
      "loss: 0.016795, accuracy: 93.191%, batch [305920/756895]\n",
      "loss: 0.017425, accuracy: 93.043%, batch [307200/756895]\n",
      "loss: 0.022053, accuracy: 92.988%, batch [308480/756895]\n",
      "loss: 0.018327, accuracy: 93.055%, batch [309760/756895]\n",
      "loss: 0.020334, accuracy: 93.007%, batch [311040/756895]\n",
      "loss: 0.024048, accuracy: 92.978%, batch [312320/756895]\n",
      "loss: 0.017153, accuracy: 92.961%, batch [313600/756895]\n",
      "loss: 0.017294, accuracy: 93.010%, batch [314880/756895]\n",
      "loss: 0.019371, accuracy: 92.973%, batch [316160/756895]\n",
      "loss: 0.016520, accuracy: 93.292%, batch [317440/756895]\n",
      "loss: 0.016180, accuracy: 93.168%, batch [318720/756895]\n",
      "loss: 0.018262, accuracy: 93.231%, batch [320000/756895]\n",
      "loss: 0.021573, accuracy: 92.934%, batch [321280/756895]\n",
      "loss: 0.018165, accuracy: 93.049%, batch [322560/756895]\n",
      "loss: 0.015990, accuracy: 93.232%, batch [323840/756895]\n",
      "loss: 0.019226, accuracy: 93.148%, batch [325120/756895]\n",
      "loss: 0.027674, accuracy: 92.948%, batch [326400/756895]\n",
      "loss: 0.017843, accuracy: 93.109%, batch [327680/756895]\n",
      "loss: 0.019943, accuracy: 93.005%, batch [328960/756895]\n",
      "loss: 0.016795, accuracy: 93.192%, batch [330240/756895]\n",
      "loss: 0.015297, accuracy: 93.201%, batch [331520/756895]\n",
      "loss: 0.024340, accuracy: 93.063%, batch [332800/756895]\n",
      "loss: 0.019381, accuracy: 92.959%, batch [334080/756895]\n",
      "loss: 0.022754, accuracy: 92.820%, batch [335360/756895]\n",
      "loss: 0.032804, accuracy: 92.771%, batch [336640/756895]\n",
      "loss: 0.016893, accuracy: 93.060%, batch [337920/756895]\n",
      "loss: 0.017285, accuracy: 93.187%, batch [339200/756895]\n",
      "loss: 0.019406, accuracy: 92.973%, batch [340480/756895]\n",
      "loss: 0.019920, accuracy: 92.975%, batch [341760/756895]\n",
      "loss: 0.018128, accuracy: 93.239%, batch [343040/756895]\n",
      "loss: 0.020482, accuracy: 93.036%, batch [344320/756895]\n",
      "loss: 0.018172, accuracy: 93.078%, batch [345600/756895]\n",
      "loss: 0.017040, accuracy: 93.317%, batch [346880/756895]\n",
      "loss: 0.015395, accuracy: 93.060%, batch [348160/756895]\n",
      "loss: 0.016688, accuracy: 93.267%, batch [349440/756895]\n",
      "loss: 0.022087, accuracy: 92.939%, batch [350720/756895]\n",
      "loss: 0.015825, accuracy: 93.157%, batch [352000/756895]\n",
      "loss: 0.020325, accuracy: 92.933%, batch [353280/756895]\n",
      "loss: 0.017468, accuracy: 93.091%, batch [354560/756895]\n",
      "loss: 0.015656, accuracy: 93.118%, batch [355840/756895]\n",
      "loss: 0.024417, accuracy: 93.126%, batch [357120/756895]\n",
      "loss: 0.019304, accuracy: 93.103%, batch [358400/756895]\n",
      "loss: 0.019065, accuracy: 92.977%, batch [359680/756895]\n",
      "loss: 0.016577, accuracy: 93.251%, batch [360960/756895]\n",
      "loss: 0.019641, accuracy: 92.895%, batch [362240/756895]\n",
      "loss: 0.018103, accuracy: 93.093%, batch [363520/756895]\n",
      "loss: 0.023676, accuracy: 92.856%, batch [364800/756895]\n",
      "loss: 0.018524, accuracy: 93.075%, batch [366080/756895]\n",
      "loss: 0.015580, accuracy: 93.127%, batch [367360/756895]\n",
      "loss: 0.020240, accuracy: 93.125%, batch [368640/756895]\n",
      "loss: 0.016750, accuracy: 93.087%, batch [369920/756895]\n",
      "loss: 0.019916, accuracy: 92.950%, batch [371200/756895]\n",
      "loss: 0.020160, accuracy: 93.053%, batch [372480/756895]\n",
      "loss: 0.017286, accuracy: 93.061%, batch [373760/756895]\n",
      "loss: 0.017770, accuracy: 93.034%, batch [375040/756895]\n",
      "loss: 0.024828, accuracy: 93.045%, batch [376320/756895]\n",
      "loss: 0.019925, accuracy: 92.943%, batch [377600/756895]\n",
      "loss: 0.018247, accuracy: 93.021%, batch [378880/756895]\n",
      "loss: 0.018866, accuracy: 93.039%, batch [380160/756895]\n",
      "loss: 0.015991, accuracy: 93.016%, batch [381440/756895]\n",
      "loss: 0.015563, accuracy: 93.188%, batch [382720/756895]\n",
      "loss: 0.017538, accuracy: 93.187%, batch [384000/756895]\n",
      "loss: 0.020696, accuracy: 92.876%, batch [385280/756895]\n",
      "loss: 0.017469, accuracy: 93.074%, batch [386560/756895]\n",
      "loss: 0.021867, accuracy: 93.037%, batch [387840/756895]\n",
      "loss: 0.018707, accuracy: 93.037%, batch [389120/756895]\n",
      "loss: 0.015052, accuracy: 93.376%, batch [390400/756895]\n",
      "loss: 0.017986, accuracy: 92.980%, batch [391680/756895]\n",
      "loss: 0.019964, accuracy: 93.075%, batch [392960/756895]\n",
      "loss: 0.020806, accuracy: 93.077%, batch [394240/756895]\n",
      "loss: 0.020458, accuracy: 92.904%, batch [395520/756895]\n",
      "loss: 0.021712, accuracy: 93.073%, batch [396800/756895]\n",
      "loss: 0.023077, accuracy: 92.926%, batch [398080/756895]\n",
      "loss: 0.021735, accuracy: 93.000%, batch [399360/756895]\n",
      "loss: 0.016731, accuracy: 93.119%, batch [400640/756895]\n",
      "loss: 0.018859, accuracy: 93.297%, batch [401920/756895]\n",
      "loss: 0.021688, accuracy: 92.963%, batch [403200/756895]\n",
      "loss: 0.017891, accuracy: 93.009%, batch [404480/756895]\n",
      "loss: 0.020346, accuracy: 93.027%, batch [405760/756895]\n",
      "loss: 0.020114, accuracy: 92.971%, batch [407040/756895]\n",
      "loss: 0.018846, accuracy: 93.122%, batch [408320/756895]\n",
      "loss: 0.015550, accuracy: 93.339%, batch [409600/756895]\n",
      "loss: 0.018613, accuracy: 93.109%, batch [410880/756895]\n",
      "loss: 0.018356, accuracy: 93.153%, batch [412160/756895]\n",
      "loss: 0.025842, accuracy: 92.748%, batch [413440/756895]\n",
      "loss: 0.026646, accuracy: 92.610%, batch [414720/756895]\n",
      "loss: 0.018761, accuracy: 93.163%, batch [416000/756895]\n",
      "loss: 0.015780, accuracy: 93.385%, batch [417280/756895]\n",
      "loss: 0.015166, accuracy: 93.118%, batch [418560/756895]\n",
      "loss: 0.023186, accuracy: 92.966%, batch [419840/756895]\n",
      "loss: 0.022013, accuracy: 92.955%, batch [421120/756895]\n",
      "loss: 0.016666, accuracy: 93.057%, batch [422400/756895]\n",
      "loss: 0.015126, accuracy: 93.084%, batch [423680/756895]\n",
      "loss: 0.018053, accuracy: 92.940%, batch [424960/756895]\n",
      "loss: 0.018143, accuracy: 92.899%, batch [426240/756895]\n",
      "loss: 0.021901, accuracy: 93.125%, batch [427520/756895]\n",
      "loss: 0.019136, accuracy: 92.983%, batch [428800/756895]\n",
      "loss: 0.022506, accuracy: 92.988%, batch [430080/756895]\n",
      "loss: 0.015798, accuracy: 93.273%, batch [431360/756895]\n",
      "loss: 0.016639, accuracy: 93.253%, batch [432640/756895]\n",
      "loss: 0.023871, accuracy: 93.032%, batch [433920/756895]\n",
      "loss: 0.019301, accuracy: 92.956%, batch [435200/756895]\n",
      "loss: 0.015946, accuracy: 92.926%, batch [436480/756895]\n",
      "loss: 0.017860, accuracy: 92.990%, batch [437760/756895]\n",
      "loss: 0.019273, accuracy: 92.926%, batch [439040/756895]\n",
      "loss: 0.022078, accuracy: 92.790%, batch [440320/756895]\n",
      "loss: 0.016641, accuracy: 93.159%, batch [441600/756895]\n",
      "loss: 0.018017, accuracy: 93.192%, batch [442880/756895]\n",
      "loss: 0.013469, accuracy: 93.279%, batch [444160/756895]\n",
      "loss: 0.017841, accuracy: 93.197%, batch [445440/756895]\n",
      "loss: 0.017381, accuracy: 92.985%, batch [446720/756895]\n",
      "loss: 0.020870, accuracy: 93.048%, batch [448000/756895]\n",
      "loss: 0.020681, accuracy: 92.966%, batch [449280/756895]\n",
      "loss: 0.018318, accuracy: 93.009%, batch [450560/756895]\n",
      "loss: 0.018161, accuracy: 93.126%, batch [451840/756895]\n",
      "loss: 0.022716, accuracy: 93.096%, batch [453120/756895]\n",
      "loss: 0.019272, accuracy: 92.945%, batch [454400/756895]\n",
      "loss: 0.018934, accuracy: 93.188%, batch [455680/756895]\n",
      "loss: 0.014056, accuracy: 93.227%, batch [456960/756895]\n",
      "loss: 0.019611, accuracy: 93.113%, batch [458240/756895]\n",
      "loss: 0.016285, accuracy: 93.092%, batch [459520/756895]\n",
      "loss: 0.023507, accuracy: 92.955%, batch [460800/756895]\n",
      "loss: 0.017401, accuracy: 93.233%, batch [462080/756895]\n",
      "loss: 0.022602, accuracy: 93.123%, batch [463360/756895]\n",
      "loss: 0.018168, accuracy: 93.036%, batch [464640/756895]\n",
      "loss: 0.017092, accuracy: 93.231%, batch [465920/756895]\n",
      "loss: 0.021720, accuracy: 92.958%, batch [467200/756895]\n",
      "loss: 0.017290, accuracy: 93.136%, batch [468480/756895]\n",
      "loss: 0.017176, accuracy: 93.151%, batch [469760/756895]\n",
      "loss: 0.018948, accuracy: 93.188%, batch [471040/756895]\n",
      "loss: 0.018685, accuracy: 93.053%, batch [472320/756895]\n",
      "loss: 0.018269, accuracy: 92.946%, batch [473600/756895]\n",
      "loss: 0.018246, accuracy: 93.002%, batch [474880/756895]\n",
      "loss: 0.015583, accuracy: 93.277%, batch [476160/756895]\n",
      "loss: 0.015228, accuracy: 93.126%, batch [477440/756895]\n",
      "loss: 0.019119, accuracy: 92.924%, batch [478720/756895]\n",
      "loss: 0.016854, accuracy: 93.294%, batch [480000/756895]\n",
      "loss: 0.017925, accuracy: 93.063%, batch [481280/756895]\n",
      "loss: 0.023385, accuracy: 93.075%, batch [482560/756895]\n",
      "loss: 0.018378, accuracy: 92.964%, batch [483840/756895]\n",
      "loss: 0.014687, accuracy: 93.168%, batch [485120/756895]\n",
      "loss: 0.018887, accuracy: 92.991%, batch [486400/756895]\n",
      "loss: 0.019993, accuracy: 92.909%, batch [487680/756895]\n",
      "loss: 0.016009, accuracy: 93.036%, batch [488960/756895]\n",
      "loss: 0.017727, accuracy: 93.217%, batch [490240/756895]\n",
      "loss: 0.015180, accuracy: 93.107%, batch [491520/756895]\n",
      "loss: 0.014042, accuracy: 93.338%, batch [492800/756895]\n",
      "loss: 0.015623, accuracy: 93.153%, batch [494080/756895]\n",
      "loss: 0.020551, accuracy: 92.878%, batch [495360/756895]\n",
      "loss: 0.017043, accuracy: 93.223%, batch [496640/756895]\n",
      "loss: 0.015595, accuracy: 93.070%, batch [497920/756895]\n",
      "loss: 0.018318, accuracy: 93.052%, batch [499200/756895]\n",
      "loss: 0.020404, accuracy: 92.983%, batch [500480/756895]\n",
      "loss: 0.026648, accuracy: 93.089%, batch [501760/756895]\n",
      "loss: 0.018322, accuracy: 93.079%, batch [503040/756895]\n",
      "loss: 0.019885, accuracy: 92.965%, batch [504320/756895]\n",
      "loss: 0.019609, accuracy: 92.979%, batch [505600/756895]\n",
      "loss: 0.020249, accuracy: 93.013%, batch [506880/756895]\n",
      "loss: 0.022447, accuracy: 93.051%, batch [508160/756895]\n",
      "loss: 0.017406, accuracy: 93.135%, batch [509440/756895]\n",
      "loss: 0.018464, accuracy: 92.994%, batch [510720/756895]\n",
      "loss: 0.017562, accuracy: 93.312%, batch [512000/756895]\n",
      "loss: 0.018835, accuracy: 93.024%, batch [513280/756895]\n",
      "loss: 0.019941, accuracy: 93.244%, batch [514560/756895]\n",
      "loss: 0.018410, accuracy: 93.043%, batch [515840/756895]\n",
      "loss: 0.021761, accuracy: 92.810%, batch [517120/756895]\n",
      "loss: 0.018470, accuracy: 93.178%, batch [518400/756895]\n",
      "loss: 0.018142, accuracy: 92.934%, batch [519680/756895]\n",
      "loss: 0.017366, accuracy: 93.058%, batch [520960/756895]\n",
      "loss: 0.019924, accuracy: 93.011%, batch [522240/756895]\n",
      "loss: 0.015739, accuracy: 93.204%, batch [523520/756895]\n",
      "loss: 0.020415, accuracy: 93.130%, batch [524800/756895]\n",
      "loss: 0.017190, accuracy: 93.130%, batch [526080/756895]\n",
      "loss: 0.016877, accuracy: 93.117%, batch [527360/756895]\n",
      "loss: 0.017594, accuracy: 93.020%, batch [528640/756895]\n",
      "loss: 0.018043, accuracy: 92.959%, batch [529920/756895]\n",
      "loss: 0.018910, accuracy: 92.995%, batch [531200/756895]\n",
      "loss: 0.017914, accuracy: 93.162%, batch [532480/756895]\n",
      "loss: 0.019410, accuracy: 93.095%, batch [533760/756895]\n",
      "loss: 0.019038, accuracy: 92.948%, batch [535040/756895]\n",
      "loss: 0.018815, accuracy: 93.141%, batch [536320/756895]\n",
      "loss: 0.018704, accuracy: 93.087%, batch [537600/756895]\n",
      "loss: 0.016574, accuracy: 93.174%, batch [538880/756895]\n",
      "loss: 0.017584, accuracy: 93.097%, batch [540160/756895]\n",
      "loss: 0.017255, accuracy: 93.053%, batch [541440/756895]\n",
      "loss: 0.014860, accuracy: 93.298%, batch [542720/756895]\n",
      "loss: 0.019912, accuracy: 92.856%, batch [544000/756895]\n",
      "loss: 0.019909, accuracy: 93.092%, batch [545280/756895]\n",
      "loss: 0.017332, accuracy: 93.233%, batch [546560/756895]\n",
      "loss: 0.019140, accuracy: 93.044%, batch [547840/756895]\n",
      "loss: 0.020227, accuracy: 93.179%, batch [549120/756895]\n",
      "loss: 0.016879, accuracy: 93.199%, batch [550400/756895]\n",
      "loss: 0.016840, accuracy: 93.201%, batch [551680/756895]\n",
      "loss: 0.017596, accuracy: 93.152%, batch [552960/756895]\n",
      "loss: 0.018377, accuracy: 92.981%, batch [554240/756895]\n",
      "loss: 0.025311, accuracy: 92.796%, batch [555520/756895]\n",
      "loss: 0.018675, accuracy: 93.107%, batch [556800/756895]\n",
      "loss: 0.019125, accuracy: 92.994%, batch [558080/756895]\n",
      "loss: 0.021751, accuracy: 93.048%, batch [559360/756895]\n",
      "loss: 0.020268, accuracy: 92.937%, batch [560640/756895]\n",
      "loss: 0.020718, accuracy: 92.960%, batch [561920/756895]\n",
      "loss: 0.019009, accuracy: 93.134%, batch [563200/756895]\n",
      "loss: 0.022941, accuracy: 92.919%, batch [564480/756895]\n",
      "loss: 0.020283, accuracy: 92.923%, batch [565760/756895]\n",
      "loss: 0.018149, accuracy: 93.051%, batch [567040/756895]\n",
      "loss: 0.018789, accuracy: 93.133%, batch [568320/756895]\n",
      "loss: 0.020181, accuracy: 92.985%, batch [569600/756895]\n",
      "loss: 0.014817, accuracy: 93.153%, batch [570880/756895]\n",
      "loss: 0.017643, accuracy: 93.111%, batch [572160/756895]\n",
      "loss: 0.018249, accuracy: 93.156%, batch [573440/756895]\n",
      "loss: 0.023025, accuracy: 92.966%, batch [574720/756895]\n",
      "loss: 0.020276, accuracy: 93.096%, batch [576000/756895]\n",
      "loss: 0.020373, accuracy: 93.206%, batch [577280/756895]\n",
      "loss: 0.021799, accuracy: 93.033%, batch [578560/756895]\n",
      "loss: 0.016354, accuracy: 93.079%, batch [579840/756895]\n",
      "loss: 0.016555, accuracy: 93.025%, batch [581120/756895]\n",
      "loss: 0.017529, accuracy: 93.083%, batch [582400/756895]\n",
      "loss: 0.019238, accuracy: 93.146%, batch [583680/756895]\n",
      "loss: 0.018572, accuracy: 93.024%, batch [584960/756895]\n",
      "loss: 0.023823, accuracy: 93.016%, batch [586240/756895]\n",
      "loss: 0.020420, accuracy: 93.000%, batch [587520/756895]\n",
      "loss: 0.019504, accuracy: 93.080%, batch [588800/756895]\n",
      "loss: 0.018775, accuracy: 93.066%, batch [590080/756895]\n",
      "loss: 0.023336, accuracy: 93.012%, batch [591360/756895]\n",
      "loss: 0.018764, accuracy: 93.094%, batch [592640/756895]\n",
      "loss: 0.014668, accuracy: 93.233%, batch [593920/756895]\n",
      "loss: 0.021713, accuracy: 92.941%, batch [595200/756895]\n",
      "loss: 0.019575, accuracy: 93.062%, batch [596480/756895]\n",
      "loss: 0.018972, accuracy: 93.234%, batch [597760/756895]\n",
      "loss: 0.015762, accuracy: 93.276%, batch [599040/756895]\n",
      "loss: 0.019985, accuracy: 93.050%, batch [600320/756895]\n",
      "loss: 0.017939, accuracy: 92.954%, batch [601600/756895]\n",
      "loss: 0.025007, accuracy: 92.946%, batch [602880/756895]\n",
      "loss: 0.027485, accuracy: 93.148%, batch [604160/756895]\n",
      "loss: 0.016337, accuracy: 93.116%, batch [605440/756895]\n",
      "loss: 0.017328, accuracy: 93.081%, batch [606720/756895]\n",
      "loss: 0.018258, accuracy: 92.934%, batch [608000/756895]\n",
      "loss: 0.016328, accuracy: 93.025%, batch [609280/756895]\n",
      "loss: 0.021584, accuracy: 92.897%, batch [610560/756895]\n",
      "loss: 0.018300, accuracy: 93.071%, batch [611840/756895]\n",
      "loss: 0.016718, accuracy: 93.024%, batch [613120/756895]\n",
      "loss: 0.016085, accuracy: 93.324%, batch [614400/756895]\n",
      "loss: 0.018546, accuracy: 92.995%, batch [615680/756895]\n",
      "loss: 0.022929, accuracy: 92.960%, batch [616960/756895]\n",
      "loss: 0.017519, accuracy: 93.192%, batch [618240/756895]\n",
      "loss: 0.017969, accuracy: 92.983%, batch [619520/756895]\n",
      "loss: 0.022586, accuracy: 92.956%, batch [620800/756895]\n",
      "loss: 0.019299, accuracy: 93.000%, batch [622080/756895]\n",
      "loss: 0.017802, accuracy: 93.205%, batch [623360/756895]\n",
      "loss: 0.019878, accuracy: 92.984%, batch [624640/756895]\n",
      "loss: 0.016455, accuracy: 92.999%, batch [625920/756895]\n",
      "loss: 0.023256, accuracy: 92.742%, batch [627200/756895]\n",
      "loss: 0.021318, accuracy: 92.689%, batch [628480/756895]\n",
      "loss: 0.018499, accuracy: 92.893%, batch [629760/756895]\n",
      "loss: 0.018136, accuracy: 92.882%, batch [631040/756895]\n",
      "loss: 0.022049, accuracy: 93.197%, batch [632320/756895]\n",
      "loss: 0.018029, accuracy: 93.171%, batch [633600/756895]\n",
      "loss: 0.015631, accuracy: 93.133%, batch [634880/756895]\n",
      "loss: 0.015629, accuracy: 93.081%, batch [636160/756895]\n",
      "loss: 0.015057, accuracy: 93.230%, batch [637440/756895]\n",
      "loss: 0.015378, accuracy: 93.170%, batch [638720/756895]\n",
      "loss: 0.018027, accuracy: 92.977%, batch [640000/756895]\n",
      "loss: 0.015473, accuracy: 93.088%, batch [641280/756895]\n",
      "loss: 0.016872, accuracy: 93.254%, batch [642560/756895]\n",
      "loss: 0.018689, accuracy: 92.850%, batch [643840/756895]\n",
      "loss: 0.015135, accuracy: 93.109%, batch [645120/756895]\n",
      "loss: 0.017604, accuracy: 93.033%, batch [646400/756895]\n",
      "loss: 0.020245, accuracy: 93.028%, batch [647680/756895]\n",
      "loss: 0.016393, accuracy: 93.136%, batch [648960/756895]\n",
      "loss: 0.018241, accuracy: 93.194%, batch [650240/756895]\n",
      "loss: 0.017004, accuracy: 93.096%, batch [651520/756895]\n",
      "loss: 0.015502, accuracy: 93.135%, batch [652800/756895]\n",
      "loss: 0.014299, accuracy: 93.235%, batch [654080/756895]\n",
      "loss: 0.016690, accuracy: 93.010%, batch [655360/756895]\n",
      "loss: 0.016721, accuracy: 93.287%, batch [656640/756895]\n",
      "loss: 0.019041, accuracy: 92.969%, batch [657920/756895]\n",
      "loss: 0.020822, accuracy: 93.010%, batch [659200/756895]\n",
      "loss: 0.016461, accuracy: 92.985%, batch [660480/756895]\n",
      "loss: 0.015077, accuracy: 93.181%, batch [661760/756895]\n",
      "loss: 0.014873, accuracy: 93.279%, batch [663040/756895]\n",
      "loss: 0.013921, accuracy: 93.208%, batch [664320/756895]\n",
      "loss: 0.015868, accuracy: 93.101%, batch [665600/756895]\n",
      "loss: 0.017985, accuracy: 93.038%, batch [666880/756895]\n",
      "loss: 0.019314, accuracy: 92.988%, batch [668160/756895]\n",
      "loss: 0.017403, accuracy: 93.132%, batch [669440/756895]\n",
      "loss: 0.018920, accuracy: 93.086%, batch [670720/756895]\n",
      "loss: 0.016428, accuracy: 93.151%, batch [672000/756895]\n",
      "loss: 0.018783, accuracy: 92.976%, batch [673280/756895]\n",
      "loss: 0.026633, accuracy: 92.856%, batch [674560/756895]\n",
      "loss: 0.022025, accuracy: 93.089%, batch [675840/756895]\n",
      "loss: 0.016644, accuracy: 93.125%, batch [677120/756895]\n",
      "loss: 0.025111, accuracy: 92.781%, batch [678400/756895]\n",
      "loss: 0.020049, accuracy: 92.942%, batch [679680/756895]\n",
      "loss: 0.017600, accuracy: 93.208%, batch [680960/756895]\n",
      "loss: 0.021043, accuracy: 92.976%, batch [682240/756895]\n",
      "loss: 0.020309, accuracy: 92.779%, batch [683520/756895]\n",
      "loss: 0.018461, accuracy: 93.129%, batch [684800/756895]\n",
      "loss: 0.021985, accuracy: 92.887%, batch [686080/756895]\n",
      "loss: 0.028386, accuracy: 92.576%, batch [687360/756895]\n",
      "loss: 0.018635, accuracy: 93.044%, batch [688640/756895]\n",
      "loss: 0.022340, accuracy: 92.909%, batch [689920/756895]\n",
      "loss: 0.021085, accuracy: 92.990%, batch [691200/756895]\n",
      "loss: 0.023320, accuracy: 92.939%, batch [692480/756895]\n",
      "loss: 0.019597, accuracy: 93.032%, batch [693760/756895]\n",
      "loss: 0.017832, accuracy: 93.184%, batch [695040/756895]\n",
      "loss: 0.015906, accuracy: 93.102%, batch [696320/756895]\n",
      "loss: 0.019846, accuracy: 93.060%, batch [697600/756895]\n",
      "loss: 0.019110, accuracy: 92.992%, batch [698880/756895]\n",
      "loss: 0.020462, accuracy: 93.135%, batch [700160/756895]\n",
      "loss: 0.020042, accuracy: 93.172%, batch [701440/756895]\n",
      "loss: 0.019728, accuracy: 92.844%, batch [702720/756895]\n",
      "loss: 0.015562, accuracy: 93.114%, batch [704000/756895]\n",
      "loss: 0.021031, accuracy: 93.164%, batch [705280/756895]\n",
      "loss: 0.019630, accuracy: 92.954%, batch [706560/756895]\n",
      "loss: 0.024534, accuracy: 92.939%, batch [707840/756895]\n",
      "loss: 0.015166, accuracy: 93.295%, batch [709120/756895]\n",
      "loss: 0.016016, accuracy: 93.213%, batch [710400/756895]\n",
      "loss: 0.028494, accuracy: 92.937%, batch [711680/756895]\n",
      "loss: 0.018893, accuracy: 92.975%, batch [712960/756895]\n",
      "loss: 0.016891, accuracy: 93.063%, batch [714240/756895]\n",
      "loss: 0.017072, accuracy: 93.077%, batch [715520/756895]\n",
      "loss: 0.018005, accuracy: 93.087%, batch [716800/756895]\n",
      "loss: 0.017388, accuracy: 93.209%, batch [718080/756895]\n",
      "loss: 0.022602, accuracy: 92.889%, batch [719360/756895]\n",
      "loss: 0.018904, accuracy: 93.021%, batch [720640/756895]\n",
      "loss: 0.025517, accuracy: 92.839%, batch [721920/756895]\n",
      "loss: 0.019618, accuracy: 93.022%, batch [723200/756895]\n",
      "loss: 0.023021, accuracy: 92.964%, batch [724480/756895]\n",
      "loss: 0.020915, accuracy: 93.009%, batch [725760/756895]\n",
      "loss: 0.018264, accuracy: 93.114%, batch [727040/756895]\n",
      "loss: 0.016653, accuracy: 93.094%, batch [728320/756895]\n",
      "loss: 0.016937, accuracy: 93.011%, batch [729600/756895]\n",
      "loss: 0.023794, accuracy: 92.969%, batch [730880/756895]\n",
      "loss: 0.021049, accuracy: 92.920%, batch [732160/756895]\n",
      "loss: 0.019390, accuracy: 92.948%, batch [733440/756895]\n",
      "loss: 0.017204, accuracy: 93.320%, batch [734720/756895]\n",
      "loss: 0.015220, accuracy: 93.077%, batch [736000/756895]\n",
      "loss: 0.017334, accuracy: 93.052%, batch [737280/756895]\n",
      "loss: 0.018200, accuracy: 93.068%, batch [738560/756895]\n",
      "loss: 0.017185, accuracy: 93.195%, batch [739840/756895]\n",
      "loss: 0.015389, accuracy: 93.060%, batch [741120/756895]\n",
      "loss: 0.015915, accuracy: 93.167%, batch [742400/756895]\n",
      "loss: 0.020092, accuracy: 93.028%, batch [743680/756895]\n",
      "loss: 0.018479, accuracy: 93.031%, batch [744960/756895]\n",
      "loss: 0.017748, accuracy: 93.179%, batch [746240/756895]\n",
      "loss: 0.016865, accuracy: 93.066%, batch [747520/756895]\n",
      "loss: 0.016962, accuracy: 93.228%, batch [748800/756895]\n",
      "loss: 0.020087, accuracy: 92.941%, batch [750080/756895]\n",
      "loss: 0.017262, accuracy: 92.920%, batch [751360/756895]\n",
      "loss: 0.018961, accuracy: 93.057%, batch [752640/756895]\n",
      "loss: 0.015711, accuracy: 93.250%, batch [753920/756895]\n",
      "loss: 0.016916, accuracy: 93.199%, batch [755200/756895]\n",
      "loss: 0.014622, accuracy: 93.207%, batch [756480/756895]\n",
      "Test avg loss: 0.019690, test avg accuracy: 93.020% \n",
      "\n",
      "Test avg loss: 0.019223, test avg accuracy: 93.055% \n",
      "\n",
      "Epoch 117\n",
      "------------------------\n",
      "loss: 0.019065, accuracy: 93.153%, batch [    0/756895]\n",
      "loss: 0.017283, accuracy: 93.066%, batch [ 1280/756895]\n",
      "loss: 0.018776, accuracy: 93.033%, batch [ 2560/756895]\n",
      "loss: 0.017016, accuracy: 93.024%, batch [ 3840/756895]\n",
      "loss: 0.024755, accuracy: 93.003%, batch [ 5120/756895]\n",
      "loss: 0.020430, accuracy: 93.076%, batch [ 6400/756895]\n",
      "loss: 0.017855, accuracy: 93.198%, batch [ 7680/756895]\n",
      "loss: 0.014916, accuracy: 93.302%, batch [ 8960/756895]\n",
      "loss: 0.015201, accuracy: 93.256%, batch [10240/756895]\n",
      "loss: 0.016458, accuracy: 93.229%, batch [11520/756895]\n",
      "loss: 0.015417, accuracy: 93.160%, batch [12800/756895]\n",
      "loss: 0.020826, accuracy: 92.979%, batch [14080/756895]\n",
      "loss: 0.020819, accuracy: 93.151%, batch [15360/756895]\n",
      "loss: 0.019546, accuracy: 93.069%, batch [16640/756895]\n",
      "loss: 0.018846, accuracy: 93.153%, batch [17920/756895]\n",
      "loss: 0.017092, accuracy: 93.194%, batch [19200/756895]\n",
      "loss: 0.018546, accuracy: 93.134%, batch [20480/756895]\n",
      "loss: 0.015041, accuracy: 93.146%, batch [21760/756895]\n",
      "loss: 0.023977, accuracy: 92.948%, batch [23040/756895]\n",
      "loss: 0.016712, accuracy: 93.197%, batch [24320/756895]\n",
      "loss: 0.015348, accuracy: 93.194%, batch [25600/756895]\n",
      "loss: 0.019668, accuracy: 93.005%, batch [26880/756895]\n",
      "loss: 0.024652, accuracy: 92.798%, batch [28160/756895]\n",
      "loss: 0.022910, accuracy: 92.891%, batch [29440/756895]\n",
      "loss: 0.020405, accuracy: 92.926%, batch [30720/756895]\n",
      "loss: 0.018602, accuracy: 93.133%, batch [32000/756895]\n",
      "loss: 0.016569, accuracy: 93.184%, batch [33280/756895]\n",
      "loss: 0.019165, accuracy: 93.043%, batch [34560/756895]\n",
      "loss: 0.021485, accuracy: 93.111%, batch [35840/756895]\n",
      "loss: 0.019737, accuracy: 93.165%, batch [37120/756895]\n",
      "loss: 0.015980, accuracy: 93.168%, batch [38400/756895]\n",
      "loss: 0.016823, accuracy: 93.166%, batch [39680/756895]\n",
      "loss: 0.018003, accuracy: 93.024%, batch [40960/756895]\n",
      "loss: 0.018811, accuracy: 93.061%, batch [42240/756895]\n",
      "loss: 0.018252, accuracy: 92.843%, batch [43520/756895]\n",
      "loss: 0.014765, accuracy: 93.192%, batch [44800/756895]\n",
      "loss: 0.024307, accuracy: 92.761%, batch [46080/756895]\n",
      "loss: 0.015120, accuracy: 93.334%, batch [47360/756895]\n",
      "loss: 0.019833, accuracy: 92.960%, batch [48640/756895]\n",
      "loss: 0.022121, accuracy: 93.127%, batch [49920/756895]\n",
      "loss: 0.016424, accuracy: 93.087%, batch [51200/756895]\n",
      "loss: 0.018079, accuracy: 93.204%, batch [52480/756895]\n",
      "loss: 0.016166, accuracy: 93.133%, batch [53760/756895]\n",
      "loss: 0.017283, accuracy: 93.180%, batch [55040/756895]\n",
      "loss: 0.015061, accuracy: 93.345%, batch [56320/756895]\n",
      "loss: 0.014229, accuracy: 93.163%, batch [57600/756895]\n",
      "loss: 0.017918, accuracy: 93.050%, batch [58880/756895]\n",
      "loss: 0.021687, accuracy: 92.972%, batch [60160/756895]\n",
      "loss: 0.019142, accuracy: 93.039%, batch [61440/756895]\n",
      "loss: 0.021875, accuracy: 92.749%, batch [62720/756895]\n",
      "loss: 0.020201, accuracy: 93.050%, batch [64000/756895]\n",
      "loss: 0.015825, accuracy: 93.269%, batch [65280/756895]\n",
      "loss: 0.019031, accuracy: 93.088%, batch [66560/756895]\n",
      "loss: 0.017970, accuracy: 92.869%, batch [67840/756895]\n",
      "loss: 0.021778, accuracy: 92.734%, batch [69120/756895]\n",
      "loss: 0.017769, accuracy: 93.108%, batch [70400/756895]\n",
      "loss: 0.020379, accuracy: 93.074%, batch [71680/756895]\n",
      "loss: 0.019781, accuracy: 93.098%, batch [72960/756895]\n",
      "loss: 0.016687, accuracy: 93.154%, batch [74240/756895]\n",
      "loss: 0.020106, accuracy: 92.953%, batch [75520/756895]\n",
      "loss: 0.018593, accuracy: 93.063%, batch [76800/756895]\n",
      "loss: 0.019413, accuracy: 93.115%, batch [78080/756895]\n",
      "loss: 0.022618, accuracy: 92.850%, batch [79360/756895]\n",
      "loss: 0.024118, accuracy: 92.989%, batch [80640/756895]\n",
      "loss: 0.021938, accuracy: 92.690%, batch [81920/756895]\n",
      "loss: 0.019118, accuracy: 93.145%, batch [83200/756895]\n",
      "loss: 0.020811, accuracy: 92.845%, batch [84480/756895]\n",
      "loss: 0.017629, accuracy: 93.093%, batch [85760/756895]\n",
      "loss: 0.016168, accuracy: 93.188%, batch [87040/756895]\n",
      "loss: 0.016084, accuracy: 93.219%, batch [88320/756895]\n",
      "loss: 0.027911, accuracy: 92.733%, batch [89600/756895]\n",
      "loss: 0.020943, accuracy: 92.990%, batch [90880/756895]\n",
      "loss: 0.020425, accuracy: 93.251%, batch [92160/756895]\n",
      "loss: 0.017128, accuracy: 93.156%, batch [93440/756895]\n",
      "loss: 0.017523, accuracy: 93.153%, batch [94720/756895]\n",
      "loss: 0.018234, accuracy: 93.003%, batch [96000/756895]\n",
      "loss: 0.017720, accuracy: 92.903%, batch [97280/756895]\n",
      "loss: 0.017141, accuracy: 93.111%, batch [98560/756895]\n",
      "loss: 0.017394, accuracy: 93.166%, batch [99840/756895]\n",
      "loss: 0.017652, accuracy: 93.236%, batch [101120/756895]\n",
      "loss: 0.021486, accuracy: 92.907%, batch [102400/756895]\n",
      "loss: 0.017780, accuracy: 93.037%, batch [103680/756895]\n",
      "loss: 0.019998, accuracy: 93.071%, batch [104960/756895]\n",
      "loss: 0.018833, accuracy: 93.043%, batch [106240/756895]\n",
      "loss: 0.018600, accuracy: 93.133%, batch [107520/756895]\n",
      "loss: 0.017237, accuracy: 93.005%, batch [108800/756895]\n",
      "loss: 0.019696, accuracy: 92.909%, batch [110080/756895]\n",
      "loss: 0.016160, accuracy: 93.060%, batch [111360/756895]\n",
      "loss: 0.017781, accuracy: 93.150%, batch [112640/756895]\n",
      "loss: 0.015807, accuracy: 93.223%, batch [113920/756895]\n",
      "loss: 0.015412, accuracy: 93.172%, batch [115200/756895]\n",
      "loss: 0.022605, accuracy: 93.042%, batch [116480/756895]\n",
      "loss: 0.023818, accuracy: 92.909%, batch [117760/756895]\n",
      "loss: 0.016128, accuracy: 93.092%, batch [119040/756895]\n",
      "loss: 0.018951, accuracy: 93.055%, batch [120320/756895]\n",
      "loss: 0.020612, accuracy: 92.896%, batch [121600/756895]\n",
      "loss: 0.014492, accuracy: 93.322%, batch [122880/756895]\n",
      "loss: 0.013019, accuracy: 93.236%, batch [124160/756895]\n",
      "loss: 0.015970, accuracy: 93.130%, batch [125440/756895]\n",
      "loss: 0.014484, accuracy: 92.919%, batch [126720/756895]\n",
      "loss: 0.018093, accuracy: 93.040%, batch [128000/756895]\n",
      "loss: 0.017965, accuracy: 93.146%, batch [129280/756895]\n",
      "loss: 0.028394, accuracy: 92.839%, batch [130560/756895]\n",
      "loss: 0.017345, accuracy: 93.101%, batch [131840/756895]\n",
      "loss: 0.016363, accuracy: 93.132%, batch [133120/756895]\n",
      "loss: 0.017615, accuracy: 93.041%, batch [134400/756895]\n",
      "loss: 0.020169, accuracy: 92.984%, batch [135680/756895]\n",
      "loss: 0.015456, accuracy: 93.240%, batch [136960/756895]\n",
      "loss: 0.020184, accuracy: 92.924%, batch [138240/756895]\n",
      "loss: 0.021592, accuracy: 92.879%, batch [139520/756895]\n",
      "loss: 0.016969, accuracy: 93.043%, batch [140800/756895]\n",
      "loss: 0.021792, accuracy: 92.971%, batch [142080/756895]\n",
      "loss: 0.017816, accuracy: 93.238%, batch [143360/756895]\n",
      "loss: 0.018735, accuracy: 92.997%, batch [144640/756895]\n",
      "loss: 0.018643, accuracy: 93.084%, batch [145920/756895]\n",
      "loss: 0.018946, accuracy: 93.262%, batch [147200/756895]\n",
      "loss: 0.015752, accuracy: 93.146%, batch [148480/756895]\n",
      "loss: 0.018121, accuracy: 93.114%, batch [149760/756895]\n",
      "loss: 0.020391, accuracy: 92.918%, batch [151040/756895]\n",
      "loss: 0.022050, accuracy: 92.946%, batch [152320/756895]\n",
      "loss: 0.021065, accuracy: 93.003%, batch [153600/756895]\n",
      "loss: 0.017360, accuracy: 93.044%, batch [154880/756895]\n",
      "loss: 0.022850, accuracy: 92.792%, batch [156160/756895]\n",
      "loss: 0.015616, accuracy: 93.227%, batch [157440/756895]\n",
      "loss: 0.021488, accuracy: 92.920%, batch [158720/756895]\n",
      "loss: 0.019784, accuracy: 93.063%, batch [160000/756895]\n",
      "loss: 0.017057, accuracy: 93.113%, batch [161280/756895]\n",
      "loss: 0.015817, accuracy: 93.197%, batch [162560/756895]\n",
      "loss: 0.017048, accuracy: 93.115%, batch [163840/756895]\n",
      "loss: 0.018717, accuracy: 93.034%, batch [165120/756895]\n",
      "loss: 0.019879, accuracy: 93.108%, batch [166400/756895]\n",
      "loss: 0.016623, accuracy: 93.183%, batch [167680/756895]\n",
      "loss: 0.017174, accuracy: 93.026%, batch [168960/756895]\n",
      "loss: 0.021805, accuracy: 92.988%, batch [170240/756895]\n",
      "loss: 0.017050, accuracy: 93.040%, batch [171520/756895]\n",
      "loss: 0.028579, accuracy: 92.809%, batch [172800/756895]\n",
      "loss: 0.017128, accuracy: 93.093%, batch [174080/756895]\n",
      "loss: 0.018910, accuracy: 93.195%, batch [175360/756895]\n",
      "loss: 0.020375, accuracy: 93.103%, batch [176640/756895]\n",
      "loss: 0.017780, accuracy: 93.155%, batch [177920/756895]\n",
      "loss: 0.020544, accuracy: 92.912%, batch [179200/756895]\n",
      "loss: 0.020955, accuracy: 92.962%, batch [180480/756895]\n",
      "loss: 0.015790, accuracy: 93.256%, batch [181760/756895]\n",
      "loss: 0.017651, accuracy: 92.953%, batch [183040/756895]\n",
      "loss: 0.018360, accuracy: 92.995%, batch [184320/756895]\n",
      "loss: 0.021675, accuracy: 93.096%, batch [185600/756895]\n",
      "loss: 0.018440, accuracy: 93.038%, batch [186880/756895]\n",
      "loss: 0.021180, accuracy: 92.935%, batch [188160/756895]\n",
      "loss: 0.020562, accuracy: 92.867%, batch [189440/756895]\n",
      "loss: 0.016103, accuracy: 93.174%, batch [190720/756895]\n",
      "loss: 0.017565, accuracy: 93.102%, batch [192000/756895]\n",
      "loss: 0.019475, accuracy: 92.858%, batch [193280/756895]\n",
      "loss: 0.022076, accuracy: 92.895%, batch [194560/756895]\n",
      "loss: 0.013721, accuracy: 93.361%, batch [195840/756895]\n",
      "loss: 0.014836, accuracy: 93.209%, batch [197120/756895]\n",
      "loss: 0.015631, accuracy: 93.231%, batch [198400/756895]\n",
      "loss: 0.016082, accuracy: 93.203%, batch [199680/756895]\n",
      "loss: 0.021461, accuracy: 93.082%, batch [200960/756895]\n",
      "loss: 0.022401, accuracy: 93.016%, batch [202240/756895]\n",
      "loss: 0.018123, accuracy: 93.162%, batch [203520/756895]\n",
      "loss: 0.022557, accuracy: 93.119%, batch [204800/756895]\n",
      "loss: 0.020689, accuracy: 93.142%, batch [206080/756895]\n",
      "loss: 0.017078, accuracy: 93.241%, batch [207360/756895]\n",
      "loss: 0.017397, accuracy: 93.118%, batch [208640/756895]\n",
      "loss: 0.016068, accuracy: 93.258%, batch [209920/756895]\n",
      "loss: 0.014674, accuracy: 93.146%, batch [211200/756895]\n",
      "loss: 0.018796, accuracy: 93.052%, batch [212480/756895]\n",
      "loss: 0.015391, accuracy: 93.415%, batch [213760/756895]\n",
      "loss: 0.018902, accuracy: 93.026%, batch [215040/756895]\n",
      "loss: 0.018360, accuracy: 92.997%, batch [216320/756895]\n",
      "loss: 0.021442, accuracy: 93.060%, batch [217600/756895]\n",
      "loss: 0.019430, accuracy: 93.216%, batch [218880/756895]\n",
      "loss: 0.022888, accuracy: 92.886%, batch [220160/756895]\n",
      "loss: 0.018293, accuracy: 93.084%, batch [221440/756895]\n",
      "loss: 0.019210, accuracy: 92.964%, batch [222720/756895]\n",
      "loss: 0.021054, accuracy: 92.911%, batch [224000/756895]\n",
      "loss: 0.017852, accuracy: 93.275%, batch [225280/756895]\n",
      "loss: 0.020024, accuracy: 92.967%, batch [226560/756895]\n",
      "loss: 0.019084, accuracy: 93.086%, batch [227840/756895]\n",
      "loss: 0.019637, accuracy: 93.053%, batch [229120/756895]\n",
      "loss: 0.017490, accuracy: 93.140%, batch [230400/756895]\n",
      "loss: 0.016869, accuracy: 93.032%, batch [231680/756895]\n",
      "loss: 0.015593, accuracy: 93.095%, batch [232960/756895]\n",
      "loss: 0.017435, accuracy: 92.946%, batch [234240/756895]\n",
      "loss: 0.018514, accuracy: 93.192%, batch [235520/756895]\n",
      "loss: 0.022113, accuracy: 92.985%, batch [236800/756895]\n",
      "loss: 0.016978, accuracy: 93.070%, batch [238080/756895]\n",
      "loss: 0.016474, accuracy: 93.088%, batch [239360/756895]\n",
      "loss: 0.015794, accuracy: 93.281%, batch [240640/756895]\n",
      "loss: 0.023129, accuracy: 92.737%, batch [241920/756895]\n",
      "loss: 0.017349, accuracy: 92.974%, batch [243200/756895]\n",
      "loss: 0.014457, accuracy: 93.205%, batch [244480/756895]\n",
      "loss: 0.017690, accuracy: 93.178%, batch [245760/756895]\n",
      "loss: 0.017780, accuracy: 92.993%, batch [247040/756895]\n",
      "loss: 0.019527, accuracy: 93.129%, batch [248320/756895]\n",
      "loss: 0.018763, accuracy: 92.855%, batch [249600/756895]\n",
      "loss: 0.015094, accuracy: 93.283%, batch [250880/756895]\n",
      "loss: 0.024541, accuracy: 93.033%, batch [252160/756895]\n",
      "loss: 0.018966, accuracy: 92.855%, batch [253440/756895]\n",
      "loss: 0.015278, accuracy: 93.265%, batch [254720/756895]\n",
      "loss: 0.017079, accuracy: 93.202%, batch [256000/756895]\n",
      "loss: 0.019543, accuracy: 93.170%, batch [257280/756895]\n",
      "loss: 0.017942, accuracy: 93.028%, batch [258560/756895]\n",
      "loss: 0.018669, accuracy: 92.867%, batch [259840/756895]\n",
      "loss: 0.021687, accuracy: 92.961%, batch [261120/756895]\n",
      "loss: 0.022416, accuracy: 93.130%, batch [262400/756895]\n",
      "loss: 0.016893, accuracy: 93.237%, batch [263680/756895]\n",
      "loss: 0.016627, accuracy: 93.263%, batch [264960/756895]\n",
      "loss: 0.019159, accuracy: 93.005%, batch [266240/756895]\n",
      "loss: 0.015686, accuracy: 93.185%, batch [267520/756895]\n",
      "loss: 0.018824, accuracy: 93.227%, batch [268800/756895]\n",
      "loss: 0.018906, accuracy: 93.140%, batch [270080/756895]\n",
      "loss: 0.014828, accuracy: 93.218%, batch [271360/756895]\n",
      "loss: 0.015638, accuracy: 93.093%, batch [272640/756895]\n",
      "loss: 0.015671, accuracy: 93.181%, batch [273920/756895]\n",
      "loss: 0.021729, accuracy: 92.800%, batch [275200/756895]\n",
      "loss: 0.014588, accuracy: 93.420%, batch [276480/756895]\n",
      "loss: 0.019262, accuracy: 93.076%, batch [277760/756895]\n",
      "loss: 0.023301, accuracy: 93.004%, batch [279040/756895]\n",
      "loss: 0.024438, accuracy: 92.684%, batch [280320/756895]\n",
      "loss: 0.017452, accuracy: 93.008%, batch [281600/756895]\n",
      "loss: 0.019748, accuracy: 92.953%, batch [282880/756895]\n",
      "loss: 0.020072, accuracy: 93.026%, batch [284160/756895]\n",
      "loss: 0.027071, accuracy: 92.928%, batch [285440/756895]\n",
      "loss: 0.016981, accuracy: 93.079%, batch [286720/756895]\n",
      "loss: 0.016287, accuracy: 93.179%, batch [288000/756895]\n",
      "loss: 0.018930, accuracy: 93.100%, batch [289280/756895]\n",
      "loss: 0.017267, accuracy: 93.058%, batch [290560/756895]\n",
      "loss: 0.018200, accuracy: 93.086%, batch [291840/756895]\n",
      "loss: 0.022766, accuracy: 93.005%, batch [293120/756895]\n",
      "loss: 0.016029, accuracy: 93.128%, batch [294400/756895]\n",
      "loss: 0.016821, accuracy: 93.150%, batch [295680/756895]\n",
      "loss: 0.017753, accuracy: 93.161%, batch [296960/756895]\n",
      "loss: 0.018656, accuracy: 92.915%, batch [298240/756895]\n",
      "loss: 0.027037, accuracy: 92.894%, batch [299520/756895]\n",
      "loss: 0.021031, accuracy: 93.047%, batch [300800/756895]\n",
      "loss: 0.018311, accuracy: 93.302%, batch [302080/756895]\n",
      "loss: 0.021357, accuracy: 92.898%, batch [303360/756895]\n",
      "loss: 0.018328, accuracy: 93.214%, batch [304640/756895]\n",
      "loss: 0.016574, accuracy: 93.216%, batch [305920/756895]\n",
      "loss: 0.029861, accuracy: 92.744%, batch [307200/756895]\n",
      "loss: 0.026897, accuracy: 92.920%, batch [308480/756895]\n",
      "loss: 0.025227, accuracy: 92.673%, batch [309760/756895]\n",
      "loss: 0.023279, accuracy: 92.984%, batch [311040/756895]\n",
      "loss: 0.024382, accuracy: 92.744%, batch [312320/756895]\n",
      "loss: 0.023904, accuracy: 93.115%, batch [313600/756895]\n",
      "loss: 0.017585, accuracy: 93.151%, batch [314880/756895]\n",
      "loss: 0.017360, accuracy: 92.981%, batch [316160/756895]\n",
      "loss: 0.027413, accuracy: 92.884%, batch [317440/756895]\n",
      "loss: 0.019939, accuracy: 92.960%, batch [318720/756895]\n",
      "loss: 0.018292, accuracy: 93.056%, batch [320000/756895]\n",
      "loss: 0.021681, accuracy: 93.073%, batch [321280/756895]\n",
      "loss: 0.016791, accuracy: 93.155%, batch [322560/756895]\n",
      "loss: 0.016625, accuracy: 93.090%, batch [323840/756895]\n",
      "loss: 0.019858, accuracy: 93.179%, batch [325120/756895]\n",
      "loss: 0.022155, accuracy: 92.952%, batch [326400/756895]\n",
      "loss: 0.027471, accuracy: 92.895%, batch [327680/756895]\n",
      "loss: 0.019093, accuracy: 93.187%, batch [328960/756895]\n",
      "loss: 0.017295, accuracy: 93.124%, batch [330240/756895]\n",
      "loss: 0.020163, accuracy: 93.019%, batch [331520/756895]\n",
      "loss: 0.020969, accuracy: 92.992%, batch [332800/756895]\n",
      "loss: 0.018685, accuracy: 93.128%, batch [334080/756895]\n",
      "loss: 0.019901, accuracy: 93.169%, batch [335360/756895]\n",
      "loss: 0.015749, accuracy: 93.334%, batch [336640/756895]\n",
      "loss: 0.019663, accuracy: 93.222%, batch [337920/756895]\n",
      "loss: 0.017159, accuracy: 93.112%, batch [339200/756895]\n",
      "loss: 0.020560, accuracy: 92.988%, batch [340480/756895]\n",
      "loss: 0.017454, accuracy: 92.999%, batch [341760/756895]\n",
      "loss: 0.021073, accuracy: 93.159%, batch [343040/756895]\n",
      "loss: 0.015356, accuracy: 93.340%, batch [344320/756895]\n",
      "loss: 0.016250, accuracy: 93.138%, batch [345600/756895]\n",
      "loss: 0.020784, accuracy: 93.092%, batch [346880/756895]\n",
      "loss: 0.016219, accuracy: 93.203%, batch [348160/756895]\n",
      "loss: 0.023730, accuracy: 92.877%, batch [349440/756895]\n",
      "loss: 0.016164, accuracy: 93.126%, batch [350720/756895]\n",
      "loss: 0.018951, accuracy: 93.095%, batch [352000/756895]\n",
      "loss: 0.018268, accuracy: 93.140%, batch [353280/756895]\n",
      "loss: 0.017992, accuracy: 93.135%, batch [354560/756895]\n",
      "loss: 0.023849, accuracy: 92.891%, batch [355840/756895]\n",
      "loss: 0.018192, accuracy: 93.067%, batch [357120/756895]\n",
      "loss: 0.016812, accuracy: 93.145%, batch [358400/756895]\n",
      "loss: 0.016137, accuracy: 93.208%, batch [359680/756895]\n",
      "loss: 0.015498, accuracy: 93.060%, batch [360960/756895]\n",
      "loss: 0.018697, accuracy: 93.082%, batch [362240/756895]\n",
      "loss: 0.019820, accuracy: 93.026%, batch [363520/756895]\n",
      "loss: 0.021484, accuracy: 92.820%, batch [364800/756895]\n",
      "loss: 0.019492, accuracy: 93.270%, batch [366080/756895]\n",
      "loss: 0.022210, accuracy: 93.030%, batch [367360/756895]\n",
      "loss: 0.019107, accuracy: 93.123%, batch [368640/756895]\n",
      "loss: 0.019809, accuracy: 93.060%, batch [369920/756895]\n",
      "loss: 0.020624, accuracy: 92.947%, batch [371200/756895]\n",
      "loss: 0.016546, accuracy: 93.147%, batch [372480/756895]\n",
      "loss: 0.022656, accuracy: 92.801%, batch [373760/756895]\n",
      "loss: 0.017643, accuracy: 93.255%, batch [375040/756895]\n",
      "loss: 0.024607, accuracy: 92.768%, batch [376320/756895]\n",
      "loss: 0.018003, accuracy: 93.095%, batch [377600/756895]\n",
      "loss: 0.018221, accuracy: 93.053%, batch [378880/756895]\n",
      "loss: 0.016930, accuracy: 93.145%, batch [380160/756895]\n",
      "loss: 0.020506, accuracy: 92.925%, batch [381440/756895]\n",
      "loss: 0.020997, accuracy: 92.933%, batch [382720/756895]\n",
      "loss: 0.018511, accuracy: 93.015%, batch [384000/756895]\n",
      "loss: 0.024045, accuracy: 92.813%, batch [385280/756895]\n",
      "loss: 0.018941, accuracy: 93.164%, batch [386560/756895]\n",
      "loss: 0.024017, accuracy: 92.677%, batch [387840/756895]\n",
      "loss: 0.016068, accuracy: 93.081%, batch [389120/756895]\n",
      "loss: 0.017804, accuracy: 93.112%, batch [390400/756895]\n",
      "loss: 0.017938, accuracy: 92.938%, batch [391680/756895]\n",
      "loss: 0.015575, accuracy: 93.116%, batch [392960/756895]\n",
      "loss: 0.018482, accuracy: 93.032%, batch [394240/756895]\n",
      "loss: 0.029941, accuracy: 92.863%, batch [395520/756895]\n",
      "loss: 0.017653, accuracy: 93.175%, batch [396800/756895]\n",
      "loss: 0.024927, accuracy: 92.708%, batch [398080/756895]\n",
      "loss: 0.018617, accuracy: 93.254%, batch [399360/756895]\n",
      "loss: 0.017868, accuracy: 92.878%, batch [400640/756895]\n",
      "loss: 0.016996, accuracy: 93.034%, batch [401920/756895]\n",
      "loss: 0.015098, accuracy: 93.248%, batch [403200/756895]\n",
      "loss: 0.027878, accuracy: 92.848%, batch [404480/756895]\n",
      "loss: 0.017087, accuracy: 93.157%, batch [405760/756895]\n",
      "loss: 0.016477, accuracy: 93.109%, batch [407040/756895]\n",
      "loss: 0.022597, accuracy: 93.131%, batch [408320/756895]\n",
      "loss: 0.018625, accuracy: 93.125%, batch [409600/756895]\n",
      "loss: 0.018736, accuracy: 92.953%, batch [410880/756895]\n",
      "loss: 0.018496, accuracy: 92.984%, batch [412160/756895]\n",
      "loss: 0.022807, accuracy: 92.709%, batch [413440/756895]\n",
      "loss: 0.021183, accuracy: 92.951%, batch [414720/756895]\n",
      "loss: 0.017099, accuracy: 93.146%, batch [416000/756895]\n",
      "loss: 0.018044, accuracy: 93.054%, batch [417280/756895]\n",
      "loss: 0.016932, accuracy: 93.278%, batch [418560/756895]\n",
      "loss: 0.020248, accuracy: 93.031%, batch [419840/756895]\n",
      "loss: 0.018156, accuracy: 93.210%, batch [421120/756895]\n",
      "loss: 0.017534, accuracy: 93.052%, batch [422400/756895]\n",
      "loss: 0.019720, accuracy: 93.083%, batch [423680/756895]\n",
      "loss: 0.023283, accuracy: 92.967%, batch [424960/756895]\n",
      "loss: 0.016960, accuracy: 93.114%, batch [426240/756895]\n",
      "loss: 0.016965, accuracy: 93.068%, batch [427520/756895]\n",
      "loss: 0.021903, accuracy: 93.054%, batch [428800/756895]\n",
      "loss: 0.019279, accuracy: 93.016%, batch [430080/756895]\n",
      "loss: 0.022291, accuracy: 93.039%, batch [431360/756895]\n",
      "loss: 0.021429, accuracy: 93.001%, batch [432640/756895]\n",
      "loss: 0.015506, accuracy: 93.125%, batch [433920/756895]\n",
      "loss: 0.016388, accuracy: 93.175%, batch [435200/756895]\n",
      "loss: 0.021624, accuracy: 93.057%, batch [436480/756895]\n",
      "loss: 0.017859, accuracy: 93.060%, batch [437760/756895]\n",
      "loss: 0.016646, accuracy: 93.157%, batch [439040/756895]\n",
      "loss: 0.021759, accuracy: 92.976%, batch [440320/756895]\n",
      "loss: 0.017790, accuracy: 93.128%, batch [441600/756895]\n",
      "loss: 0.016587, accuracy: 93.330%, batch [442880/756895]\n",
      "loss: 0.016275, accuracy: 92.999%, batch [444160/756895]\n",
      "loss: 0.026436, accuracy: 92.755%, batch [445440/756895]\n",
      "loss: 0.021197, accuracy: 92.892%, batch [446720/756895]\n",
      "loss: 0.017958, accuracy: 93.037%, batch [448000/756895]\n",
      "loss: 0.017085, accuracy: 93.208%, batch [449280/756895]\n",
      "loss: 0.023919, accuracy: 92.905%, batch [450560/756895]\n",
      "loss: 0.021810, accuracy: 93.137%, batch [451840/756895]\n",
      "loss: 0.020397, accuracy: 93.020%, batch [453120/756895]\n",
      "loss: 0.019873, accuracy: 93.124%, batch [454400/756895]\n",
      "loss: 0.024924, accuracy: 92.808%, batch [455680/756895]\n",
      "loss: 0.016046, accuracy: 93.331%, batch [456960/756895]\n",
      "loss: 0.016449, accuracy: 93.065%, batch [458240/756895]\n",
      "loss: 0.017348, accuracy: 93.108%, batch [459520/756895]\n",
      "loss: 0.018189, accuracy: 92.962%, batch [460800/756895]\n",
      "loss: 0.018480, accuracy: 93.048%, batch [462080/756895]\n",
      "loss: 0.017156, accuracy: 93.355%, batch [463360/756895]\n",
      "loss: 0.017860, accuracy: 92.997%, batch [464640/756895]\n",
      "loss: 0.019953, accuracy: 93.112%, batch [465920/756895]\n",
      "loss: 0.014394, accuracy: 93.322%, batch [467200/756895]\n",
      "loss: 0.024746, accuracy: 92.882%, batch [468480/756895]\n",
      "loss: 0.018456, accuracy: 93.086%, batch [469760/756895]\n",
      "loss: 0.019649, accuracy: 93.075%, batch [471040/756895]\n",
      "loss: 0.024400, accuracy: 92.793%, batch [472320/756895]\n",
      "loss: 0.018363, accuracy: 93.097%, batch [473600/756895]\n",
      "loss: 0.016521, accuracy: 93.232%, batch [474880/756895]\n",
      "loss: 0.018060, accuracy: 93.112%, batch [476160/756895]\n",
      "loss: 0.021916, accuracy: 92.969%, batch [477440/756895]\n",
      "loss: 0.017748, accuracy: 93.180%, batch [478720/756895]\n",
      "loss: 0.017272, accuracy: 93.100%, batch [480000/756895]\n",
      "loss: 0.019725, accuracy: 93.057%, batch [481280/756895]\n",
      "loss: 0.020456, accuracy: 92.879%, batch [482560/756895]\n",
      "loss: 0.022570, accuracy: 92.913%, batch [483840/756895]\n",
      "loss: 0.014026, accuracy: 93.220%, batch [485120/756895]\n",
      "loss: 0.015744, accuracy: 93.126%, batch [486400/756895]\n",
      "loss: 0.014363, accuracy: 93.197%, batch [487680/756895]\n",
      "loss: 0.018337, accuracy: 93.087%, batch [488960/756895]\n",
      "loss: 0.013305, accuracy: 93.323%, batch [490240/756895]\n",
      "loss: 0.018534, accuracy: 93.075%, batch [491520/756895]\n",
      "loss: 0.016790, accuracy: 93.046%, batch [492800/756895]\n",
      "loss: 0.016901, accuracy: 93.180%, batch [494080/756895]\n",
      "loss: 0.015078, accuracy: 93.084%, batch [495360/756895]\n",
      "loss: 0.015048, accuracy: 93.107%, batch [496640/756895]\n",
      "loss: 0.017028, accuracy: 93.224%, batch [497920/756895]\n",
      "loss: 0.018206, accuracy: 93.158%, batch [499200/756895]\n",
      "loss: 0.015430, accuracy: 93.277%, batch [500480/756895]\n",
      "loss: 0.016528, accuracy: 93.090%, batch [501760/756895]\n",
      "loss: 0.014156, accuracy: 93.351%, batch [503040/756895]\n",
      "loss: 0.018863, accuracy: 93.243%, batch [504320/756895]\n",
      "loss: 0.020407, accuracy: 93.119%, batch [505600/756895]\n",
      "loss: 0.019576, accuracy: 93.093%, batch [506880/756895]\n",
      "loss: 0.018161, accuracy: 93.142%, batch [508160/756895]\n",
      "loss: 0.013729, accuracy: 93.224%, batch [509440/756895]\n",
      "loss: 0.018889, accuracy: 92.933%, batch [510720/756895]\n",
      "loss: 0.017574, accuracy: 93.064%, batch [512000/756895]\n",
      "loss: 0.016869, accuracy: 93.041%, batch [513280/756895]\n",
      "loss: 0.019079, accuracy: 93.091%, batch [514560/756895]\n",
      "loss: 0.016433, accuracy: 93.074%, batch [515840/756895]\n",
      "loss: 0.023155, accuracy: 92.858%, batch [517120/756895]\n",
      "loss: 0.018656, accuracy: 93.133%, batch [518400/756895]\n",
      "loss: 0.017458, accuracy: 92.937%, batch [519680/756895]\n",
      "loss: 0.022366, accuracy: 92.710%, batch [520960/756895]\n",
      "loss: 0.017245, accuracy: 92.911%, batch [522240/756895]\n",
      "loss: 0.020535, accuracy: 93.163%, batch [523520/756895]\n",
      "loss: 0.019450, accuracy: 92.917%, batch [524800/756895]\n",
      "loss: 0.016540, accuracy: 93.203%, batch [526080/756895]\n",
      "loss: 0.026263, accuracy: 92.613%, batch [527360/756895]\n",
      "loss: 0.018214, accuracy: 93.020%, batch [528640/756895]\n",
      "loss: 0.017555, accuracy: 93.094%, batch [529920/756895]\n",
      "loss: 0.017509, accuracy: 93.184%, batch [531200/756895]\n",
      "loss: 0.017925, accuracy: 93.113%, batch [532480/756895]\n",
      "loss: 0.017355, accuracy: 93.197%, batch [533760/756895]\n",
      "loss: 0.015048, accuracy: 93.211%, batch [535040/756895]\n",
      "loss: 0.016234, accuracy: 93.228%, batch [536320/756895]\n",
      "loss: 0.018237, accuracy: 93.088%, batch [537600/756895]\n",
      "loss: 0.018808, accuracy: 92.972%, batch [538880/756895]\n",
      "loss: 0.018415, accuracy: 92.968%, batch [540160/756895]\n",
      "loss: 0.016013, accuracy: 93.306%, batch [541440/756895]\n",
      "loss: 0.016479, accuracy: 93.199%, batch [542720/756895]\n",
      "loss: 0.025953, accuracy: 92.850%, batch [544000/756895]\n",
      "loss: 0.019723, accuracy: 93.025%, batch [545280/756895]\n",
      "loss: 0.018329, accuracy: 92.925%, batch [546560/756895]\n",
      "loss: 0.020691, accuracy: 93.094%, batch [547840/756895]\n",
      "loss: 0.020593, accuracy: 93.133%, batch [549120/756895]\n",
      "loss: 0.019554, accuracy: 93.131%, batch [550400/756895]\n",
      "loss: 0.014808, accuracy: 93.192%, batch [551680/756895]\n",
      "loss: 0.017301, accuracy: 93.239%, batch [552960/756895]\n",
      "loss: 0.014963, accuracy: 93.258%, batch [554240/756895]\n",
      "loss: 0.019167, accuracy: 93.100%, batch [555520/756895]\n",
      "loss: 0.018653, accuracy: 93.161%, batch [556800/756895]\n",
      "loss: 0.017094, accuracy: 93.106%, batch [558080/756895]\n",
      "loss: 0.019046, accuracy: 93.072%, batch [559360/756895]\n",
      "loss: 0.019409, accuracy: 92.961%, batch [560640/756895]\n",
      "loss: 0.016377, accuracy: 93.206%, batch [561920/756895]\n",
      "loss: 0.019355, accuracy: 93.062%, batch [563200/756895]\n",
      "loss: 0.016009, accuracy: 93.180%, batch [564480/756895]\n",
      "loss: 0.018229, accuracy: 93.161%, batch [565760/756895]\n",
      "loss: 0.016442, accuracy: 93.212%, batch [567040/756895]\n",
      "loss: 0.016377, accuracy: 93.037%, batch [568320/756895]\n",
      "loss: 0.016519, accuracy: 93.084%, batch [569600/756895]\n",
      "loss: 0.017192, accuracy: 93.162%, batch [570880/756895]\n",
      "loss: 0.023272, accuracy: 92.988%, batch [572160/756895]\n",
      "loss: 0.015764, accuracy: 93.237%, batch [573440/756895]\n",
      "loss: 0.017368, accuracy: 93.101%, batch [574720/756895]\n",
      "loss: 0.021334, accuracy: 93.022%, batch [576000/756895]\n",
      "loss: 0.017144, accuracy: 93.152%, batch [577280/756895]\n",
      "loss: 0.018516, accuracy: 93.053%, batch [578560/756895]\n",
      "loss: 0.019343, accuracy: 93.211%, batch [579840/756895]\n",
      "loss: 0.017643, accuracy: 92.925%, batch [581120/756895]\n",
      "loss: 0.015575, accuracy: 93.212%, batch [582400/756895]\n",
      "loss: 0.015920, accuracy: 93.184%, batch [583680/756895]\n",
      "loss: 0.015778, accuracy: 93.232%, batch [584960/756895]\n",
      "loss: 0.019961, accuracy: 93.002%, batch [586240/756895]\n",
      "loss: 0.018971, accuracy: 93.224%, batch [587520/756895]\n",
      "loss: 0.014932, accuracy: 93.318%, batch [588800/756895]\n",
      "loss: 0.020702, accuracy: 92.959%, batch [590080/756895]\n",
      "loss: 0.015016, accuracy: 93.236%, batch [591360/756895]\n",
      "loss: 0.016562, accuracy: 93.008%, batch [592640/756895]\n",
      "loss: 0.017181, accuracy: 93.067%, batch [593920/756895]\n",
      "loss: 0.017033, accuracy: 93.201%, batch [595200/756895]\n",
      "loss: 0.015358, accuracy: 93.259%, batch [596480/756895]\n",
      "loss: 0.016822, accuracy: 93.090%, batch [597760/756895]\n",
      "loss: 0.017772, accuracy: 93.207%, batch [599040/756895]\n",
      "loss: 0.015867, accuracy: 93.117%, batch [600320/756895]\n",
      "loss: 0.026100, accuracy: 92.870%, batch [601600/756895]\n",
      "loss: 0.015695, accuracy: 93.133%, batch [602880/756895]\n",
      "loss: 0.016519, accuracy: 93.169%, batch [604160/756895]\n",
      "loss: 0.028609, accuracy: 92.708%, batch [605440/756895]\n",
      "loss: 0.023163, accuracy: 93.130%, batch [606720/756895]\n",
      "loss: 0.015682, accuracy: 93.189%, batch [608000/756895]\n",
      "loss: 0.014682, accuracy: 93.140%, batch [609280/756895]\n",
      "loss: 0.019734, accuracy: 92.965%, batch [610560/756895]\n",
      "loss: 0.015846, accuracy: 93.386%, batch [611840/756895]\n",
      "loss: 0.014423, accuracy: 93.173%, batch [613120/756895]\n",
      "loss: 0.017396, accuracy: 93.169%, batch [614400/756895]\n",
      "loss: 0.016659, accuracy: 92.931%, batch [615680/756895]\n",
      "loss: 0.014058, accuracy: 93.276%, batch [616960/756895]\n",
      "loss: 0.019272, accuracy: 93.108%, batch [618240/756895]\n",
      "loss: 0.015535, accuracy: 93.155%, batch [619520/756895]\n",
      "loss: 0.015337, accuracy: 93.358%, batch [620800/756895]\n",
      "loss: 0.017984, accuracy: 93.049%, batch [622080/756895]\n",
      "loss: 0.019905, accuracy: 93.139%, batch [623360/756895]\n",
      "loss: 0.017140, accuracy: 93.215%, batch [624640/756895]\n",
      "loss: 0.014467, accuracy: 93.318%, batch [625920/756895]\n",
      "loss: 0.021142, accuracy: 92.884%, batch [627200/756895]\n",
      "loss: 0.019379, accuracy: 93.025%, batch [628480/756895]\n",
      "loss: 0.015138, accuracy: 93.194%, batch [629760/756895]\n",
      "loss: 0.019953, accuracy: 93.091%, batch [631040/756895]\n",
      "loss: 0.026349, accuracy: 92.667%, batch [632320/756895]\n",
      "loss: 0.021289, accuracy: 92.854%, batch [633600/756895]\n",
      "loss: 0.016827, accuracy: 93.261%, batch [634880/756895]\n",
      "loss: 0.017465, accuracy: 93.247%, batch [636160/756895]\n",
      "loss: 0.017955, accuracy: 93.229%, batch [637440/756895]\n",
      "loss: 0.021692, accuracy: 92.900%, batch [638720/756895]\n",
      "loss: 0.024364, accuracy: 92.777%, batch [640000/756895]\n",
      "loss: 0.020963, accuracy: 93.046%, batch [641280/756895]\n",
      "loss: 0.018260, accuracy: 93.173%, batch [642560/756895]\n",
      "loss: 0.018641, accuracy: 92.937%, batch [643840/756895]\n",
      "loss: 0.016661, accuracy: 93.121%, batch [645120/756895]\n",
      "loss: 0.022168, accuracy: 92.934%, batch [646400/756895]\n",
      "loss: 0.019214, accuracy: 92.938%, batch [647680/756895]\n",
      "loss: 0.019332, accuracy: 93.045%, batch [648960/756895]\n",
      "loss: 0.016763, accuracy: 93.020%, batch [650240/756895]\n",
      "loss: 0.018531, accuracy: 92.801%, batch [651520/756895]\n",
      "loss: 0.015710, accuracy: 93.249%, batch [652800/756895]\n",
      "loss: 0.020034, accuracy: 93.195%, batch [654080/756895]\n",
      "loss: 0.018634, accuracy: 93.124%, batch [655360/756895]\n",
      "loss: 0.016684, accuracy: 93.151%, batch [656640/756895]\n",
      "loss: 0.017091, accuracy: 93.183%, batch [657920/756895]\n",
      "loss: 0.014829, accuracy: 93.264%, batch [659200/756895]\n",
      "loss: 0.021124, accuracy: 92.971%, batch [660480/756895]\n",
      "loss: 0.019110, accuracy: 93.043%, batch [661760/756895]\n",
      "loss: 0.014504, accuracy: 93.251%, batch [663040/756895]\n",
      "loss: 0.020885, accuracy: 93.152%, batch [664320/756895]\n",
      "loss: 0.024783, accuracy: 92.880%, batch [665600/756895]\n",
      "loss: 0.019517, accuracy: 93.074%, batch [666880/756895]\n",
      "loss: 0.020751, accuracy: 93.051%, batch [668160/756895]\n",
      "loss: 0.016946, accuracy: 93.325%, batch [669440/756895]\n",
      "loss: 0.015687, accuracy: 93.208%, batch [670720/756895]\n",
      "loss: 0.015059, accuracy: 93.173%, batch [672000/756895]\n",
      "loss: 0.017509, accuracy: 93.199%, batch [673280/756895]\n",
      "loss: 0.016460, accuracy: 93.290%, batch [674560/756895]\n",
      "loss: 0.015980, accuracy: 93.099%, batch [675840/756895]\n",
      "loss: 0.016598, accuracy: 93.091%, batch [677120/756895]\n",
      "loss: 0.023487, accuracy: 92.970%, batch [678400/756895]\n",
      "loss: 0.014341, accuracy: 93.331%, batch [679680/756895]\n",
      "loss: 0.024286, accuracy: 92.755%, batch [680960/756895]\n",
      "loss: 0.022237, accuracy: 92.891%, batch [682240/756895]\n",
      "loss: 0.017274, accuracy: 93.079%, batch [683520/756895]\n",
      "loss: 0.018420, accuracy: 92.982%, batch [684800/756895]\n",
      "loss: 0.020941, accuracy: 93.038%, batch [686080/756895]\n",
      "loss: 0.019913, accuracy: 93.142%, batch [687360/756895]\n",
      "loss: 0.019224, accuracy: 92.833%, batch [688640/756895]\n",
      "loss: 0.015257, accuracy: 93.196%, batch [689920/756895]\n",
      "loss: 0.016135, accuracy: 93.111%, batch [691200/756895]\n",
      "loss: 0.016052, accuracy: 93.053%, batch [692480/756895]\n",
      "loss: 0.016933, accuracy: 93.248%, batch [693760/756895]\n",
      "loss: 0.019289, accuracy: 92.906%, batch [695040/756895]\n",
      "loss: 0.015581, accuracy: 93.070%, batch [696320/756895]\n",
      "loss: 0.020788, accuracy: 93.146%, batch [697600/756895]\n",
      "loss: 0.016321, accuracy: 93.182%, batch [698880/756895]\n",
      "loss: 0.015061, accuracy: 93.267%, batch [700160/756895]\n",
      "loss: 0.023366, accuracy: 92.700%, batch [701440/756895]\n",
      "loss: 0.021792, accuracy: 93.015%, batch [702720/756895]\n",
      "loss: 0.017928, accuracy: 93.166%, batch [704000/756895]\n",
      "loss: 0.016736, accuracy: 93.031%, batch [705280/756895]\n",
      "loss: 0.018722, accuracy: 92.870%, batch [706560/756895]\n",
      "loss: 0.017704, accuracy: 93.047%, batch [707840/756895]\n",
      "loss: 0.020191, accuracy: 92.891%, batch [709120/756895]\n",
      "loss: 0.018765, accuracy: 92.988%, batch [710400/756895]\n",
      "loss: 0.020980, accuracy: 93.013%, batch [711680/756895]\n",
      "loss: 0.018807, accuracy: 93.123%, batch [712960/756895]\n",
      "loss: 0.016075, accuracy: 93.105%, batch [714240/756895]\n",
      "loss: 0.016536, accuracy: 93.084%, batch [715520/756895]\n",
      "loss: 0.028385, accuracy: 92.752%, batch [716800/756895]\n",
      "loss: 0.018901, accuracy: 92.999%, batch [718080/756895]\n",
      "loss: 0.023885, accuracy: 93.050%, batch [719360/756895]\n",
      "loss: 0.016842, accuracy: 93.089%, batch [720640/756895]\n",
      "loss: 0.016494, accuracy: 93.161%, batch [721920/756895]\n",
      "loss: 0.014886, accuracy: 93.171%, batch [723200/756895]\n",
      "loss: 0.021860, accuracy: 92.983%, batch [724480/756895]\n",
      "loss: 0.016992, accuracy: 93.095%, batch [725760/756895]\n",
      "loss: 0.024244, accuracy: 92.930%, batch [727040/756895]\n",
      "loss: 0.018335, accuracy: 93.127%, batch [728320/756895]\n",
      "loss: 0.021394, accuracy: 93.119%, batch [729600/756895]\n",
      "loss: 0.015851, accuracy: 93.188%, batch [730880/756895]\n",
      "loss: 0.017995, accuracy: 93.088%, batch [732160/756895]\n",
      "loss: 0.020572, accuracy: 92.903%, batch [733440/756895]\n",
      "loss: 0.019236, accuracy: 93.152%, batch [734720/756895]\n",
      "loss: 0.021855, accuracy: 93.147%, batch [736000/756895]\n",
      "loss: 0.017377, accuracy: 93.061%, batch [737280/756895]\n",
      "loss: 0.019114, accuracy: 93.049%, batch [738560/756895]\n",
      "loss: 0.021489, accuracy: 92.970%, batch [739840/756895]\n",
      "loss: 0.019065, accuracy: 93.112%, batch [741120/756895]\n",
      "loss: 0.021874, accuracy: 92.692%, batch [742400/756895]\n",
      "loss: 0.017734, accuracy: 93.346%, batch [743680/756895]\n",
      "loss: 0.015769, accuracy: 93.094%, batch [744960/756895]\n",
      "loss: 0.014016, accuracy: 93.458%, batch [746240/756895]\n",
      "loss: 0.015746, accuracy: 93.139%, batch [747520/756895]\n",
      "loss: 0.020759, accuracy: 93.126%, batch [748800/756895]\n",
      "loss: 0.015599, accuracy: 93.110%, batch [750080/756895]\n",
      "loss: 0.020196, accuracy: 92.911%, batch [751360/756895]\n",
      "loss: 0.016678, accuracy: 93.184%, batch [752640/756895]\n",
      "loss: 0.017650, accuracy: 93.084%, batch [753920/756895]\n",
      "loss: 0.022903, accuracy: 92.868%, batch [755200/756895]\n",
      "loss: 0.024360, accuracy: 92.963%, batch [756480/756895]\n",
      "Test avg loss: 0.019975, test avg accuracy: 93.010% \n",
      "\n",
      "Test avg loss: 0.019615, test avg accuracy: 93.032% \n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openpyxl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run:\n\u001b[1;32m      3\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwc_weights.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m     exec_train(thr_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, loss_weights\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.9\u001b[39m,\u001b[38;5;241m0.1\u001b[39m], train_loader\u001b[38;5;241m=\u001b[39mtrain_loader, dev_loader\u001b[38;5;241m=\u001b[39mdev_loader, \n\u001b[1;32m      5\u001b[0m             test_loader\u001b[38;5;241m=\u001b[39mtest_loader, path_weights\u001b[38;5;241m=\u001b[39mpath, model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[1;32m      6\u001b[0m             path_stats\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstats.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, last_layers \u001b[38;5;241m=\u001b[39m last_layer)\n",
      "Cell \u001b[0;32mIn[22], line 91\u001b[0m, in \u001b[0;36mexec_train\u001b[0;34m(thr_epochs, loss_weights, train_loader, dev_loader, test_loader, path_weights, model, path_stats, last_layers)\u001b[0m\n\u001b[1;32m     89\u001b[0m df_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest validation loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m best_val_loss[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     90\u001b[0m df_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrained last layers\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m last_layers\n\u001b[0;32m---> 91\u001b[0m df_info\u001b[38;5;241m.\u001b[39mto_excel(path_weights\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxlsx\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m epoch_ls, train_loss_ls, train_acc_ls, val_loss_ls, val_acc_ls, test_loss_ls, test_acc_ls\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/generic.py:2417\u001b[0m, in \u001b[0;36mNDFrame.to_excel\u001b[0;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   2404\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexcel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[1;32m   2406\u001b[0m formatter \u001b[38;5;241m=\u001b[39m ExcelFormatter(\n\u001b[1;32m   2407\u001b[0m     df,\n\u001b[1;32m   2408\u001b[0m     na_rep\u001b[38;5;241m=\u001b[39mna_rep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2415\u001b[0m     inf_rep\u001b[38;5;241m=\u001b[39minf_rep,\n\u001b[1;32m   2416\u001b[0m )\n\u001b[0;32m-> 2417\u001b[0m formatter\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[1;32m   2418\u001b[0m     excel_writer,\n\u001b[1;32m   2419\u001b[0m     sheet_name\u001b[38;5;241m=\u001b[39msheet_name,\n\u001b[1;32m   2420\u001b[0m     startrow\u001b[38;5;241m=\u001b[39mstartrow,\n\u001b[1;32m   2421\u001b[0m     startcol\u001b[38;5;241m=\u001b[39mstartcol,\n\u001b[1;32m   2422\u001b[0m     freeze_panes\u001b[38;5;241m=\u001b[39mfreeze_panes,\n\u001b[1;32m   2423\u001b[0m     engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[1;32m   2424\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m   2425\u001b[0m     engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[1;32m   2426\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/formats/excel.py:943\u001b[0m, in \u001b[0;36mExcelFormatter.write\u001b[0;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m    941\u001b[0m     need_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     writer \u001b[38;5;241m=\u001b[39m ExcelWriter(\n\u001b[1;32m    944\u001b[0m         writer,\n\u001b[1;32m    945\u001b[0m         engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[1;32m    946\u001b[0m         storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    947\u001b[0m         engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[1;32m    948\u001b[0m     )\n\u001b[1;32m    949\u001b[0m     need_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/excel/_openpyxl.py:57\u001b[0m, in \u001b[0;36mOpenpyxlWriter.__init__\u001b[0;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     46\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m WriteExcelBuffer \u001b[38;5;241m|\u001b[39m ExcelWriter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Use the openpyxl module as the Excel writer.\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenpyxl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworkbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Workbook\n\u001b[1;32m     59\u001b[0m     engine_kwargs \u001b[38;5;241m=\u001b[39m combine_kwargs(engine_kwargs, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     62\u001b[0m         path,\n\u001b[1;32m     63\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m         engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[1;32m     67\u001b[0m     )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openpyxl'"
     ]
    }
   ],
   "source": [
    "run = True\n",
    "if run:\n",
    "    path = 'wc_weights.pt'\n",
    "    exec_train(thr_epochs=12, loss_weights=[0.9,0.1], train_loader=train_loader, dev_loader=dev_loader, \n",
    "            test_loader=test_loader, path_weights=path, model=model, \n",
    "            path_stats='stats.pkl', last_layers = last_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b717ab68-37aa-4a12-9817-c603f01f12c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd3cd02-ebeb-488b-81fb-481c8f3079b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
